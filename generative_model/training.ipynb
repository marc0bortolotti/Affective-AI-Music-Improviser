{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6405,"status":"ok","timestamp":1720350493956,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"em9mWQxF-fCK","outputId":"b6f25bad-6d9a-45d9-aa52-7468830a096d"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n","c:\\Users\\Gianni\\Desktop\\MARCO\\UNI\\Magistrale\\TESI\\Code\\AI-Affective Music Improviser\\generative_model\n"]}],"source":["import glob\n","import numpy as np\n","import pandas as pd\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n","import matplotlib.pyplot as plt\n","import yaml\n","import re\n","from tokenization import PrettyMidiTokenizer, BCI_TOKENS, SILENCE_TOKEN\n","from model import TCN\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","pd.set_option('display.max_rows',500)\n","pd.set_option('display.max_columns',504)\n","pd.set_option('display.width',1000)\n","\n","\n","# MODEL PARAMETERS\n","EPOCHS = 500 # 500\n","LEARNING_RATE = 1 # 4\n","BATCH_SIZE = 4 # 16\n","TRAIN_MODEL = True\n","FEEDBACK = False\n","EMPHASIZE_EEG = True\n","EARLY_STOP = True\n","\n","pwd = os.getcwd()\n","print(pwd)\n","\n","DIRECTORY_PATH = ''\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6673,"status":"ok","timestamp":1720350500624,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"q-xf9JjP-fCM","outputId":"106f73f2-f24a-4b1b-e086-67277f52054b"},"outputs":[{"name":"stdout","output_type":"stream","text":["dataset\n","Number of input files: 6\n","Number of output files: 6 \n","\n","1: 0_Drum_HardRock_EXCITED.mid -> 0_Bass_HardRock_EXCITED.mid\n","Input sequence length: 26\n","Emotion token: C\n","\n","2: 1_Drum_HardRock_EXCITED.mid -> 1_Bass_HardRock_EXCITED.mid\n","Input sequence length: 32\n","Emotion token: C\n","\n","3: 2_Drum_Blues_EXCITED.mid -> 2_Bass_Blues_EXCITED.mid\n","Input sequence length: 20\n","Emotion token: C\n","\n","4: 3_Drum_Blues_EXCITED.mid -> 3_Bass_Blues_EXCITED.mid\n","Input sequence length: 20\n","Emotion token: C\n","\n","5: 4_Drum_PopRock_RELAX.mid -> 4_Bass_PopRock_RELAX.mid\n","Input sequence length: 35\n","Emotion token: R\n","\n","6: 5_Drum_PopRock_RELAX.mid -> 5_Bass_PopRock_RELAX.mid\n","Input sequence length: 23\n","Emotion token: R\n","\n","\n","Number of input sequences: 156\n","Input sequence length: 192\n","Input vocabulars size: 39\n","\n","Number of output sequences: 156\n","Output sequence length: 192\n","Output vocabulars size: 87\n","\n","Input vocab: {'O': 0, 'R': 1, 'C': 2, '36ffS': 3, '42ppS': 4, '38ffS': 5, '42pS': 6, '36ffS_42ffS': 7, '38ffS_42ffS': 8, '42fS': 9, '36ffS_42pS': 10, '36ffS_42ppS': 11, '42ffS': 12, '36ffS_42fS': 13, '36ppS_38ffS': 14, '36ppS_42fS': 15, '36ppS': 16, '38ppS': 17, '38pS_42ffS': 18, '36ppS_38ffS_42ffS': 19, '36fS': 20, '38ffS_42pS': 21, '36pS': 22, '36pS_42ppS': 23, '38fS_42pS': 24, '36pS_42pS': 25, '38fS_42fS': 26, '36fS_42pS': 27, '38ffS_42fS': 28, '36fS_42ppS': 29, '38fS': 30, '36fS_36ppS_42ppS': 31, '38fS_42ffS': 32, '36fS_42fS': 33, '38pS': 34, '36ppS_42ppS': 35, '38pS_42fS': 36, '38ppS_42pS': 37, '36ppS_38ppS': 38}\n","Output vocab: {'O': 0, '52ffS': 1, '52ff': 2, '55ffS': 3, '55ff': 4, '45ffS': 5, '45ff': 6, '48ffS': 7, '48ff': 8, '47ffS': 9, '47ff': 10, '43ffS': 11, '43ff': 12, '57ffS': 13, '57ff': 14, '50ffS': 15, '50ff': 16, '40ffS': 17, '40ff': 18, '52pS': 19, '52p': 20, '48pS': 21, '48p': 22, '43pS': 23, '43p': 24, '47pS': 25, '47p': 26, '67pS': 27, '67p': 28, '50pS': 29, '50p': 30, '40pS': 31, '40p': 32, '55pS': 33, '55p': 34, '59pS': 35, '59p': 36, '62pS': 37, '62p': 38, '60pS': 39, '60p': 40, '57pS': 41, '57p': 42, '53pS': 43, '53p': 44, '64pS': 45, '64p': 46, '45pS': 47, '45p': 48, '56ffS': 49, '56ff': 50, '59ffS': 51, '59ff': 52, '49ffS': 53, '49ff': 54, '51ffS': 55, '51ff': 56, '54ffS': 57, '54ff': 58, '58ffS': 59, '58ff': 60, '63ffS': 61, '63ff': 62, '64ffS': 63, '64ff': 64, '66ffS': 65, '66ff': 66, '61ffS': 67, '61ff': 68, '62ffS': 69, '62ff': 70, '56pS': 71, '56p': 72, '49pS': 73, '49p': 74, '51pS': 75, '51p': 76, '54pS': 77, '54p': 78, '58pS': 79, '58p': 80, '63pS': 81, '63p': 82, '66pS': 83, '66p': 84, '61pS': 85, '61p': 86}\n"]}],"source":["'''\n","Assumptions:\n","Sequences described as input_#.mid and output_#.mid in the corresponding folders\n","'''\n","DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n","\n","print(DATASET_PATH)\n","\n","input_filenames = sorted(glob.glob(os.path.join(DATASET_PATH, 'input/*.mid')))\n","print('Number of input files:', len(input_filenames))\n","\n","output_filenames = sorted(glob.glob(os.path.join(DATASET_PATH, 'output/*.mid')))\n","print('Number of output files:', len(output_filenames), '\\n')\n","\n","\n","INPUT_TOK = PrettyMidiTokenizer()\n","OUTPUT_TOK = PrettyMidiTokenizer()\n","\n","for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n","\n","    in_file_name = os.path.basename(in_file)\n","    out_file_name = os.path.basename(out_file)\n","    print(f'{i + 1}: {in_file_name} -> {out_file_name}')\n","\n","    if 'RELAX' in in_file_name:\n","        emotion_token = BCI_TOKENS[0]\n","    elif 'EXCITED' in in_file_name:\n","        emotion_token = BCI_TOKENS[1]\n","    else:\n","        raise Exception('Emotion not found in file name. Please add the emotion to the file name.')\n","\n","    in_seq, in_df = INPUT_TOK.midi_to_tokens(in_file, update_vocab=True, update_sequences=True, emotion_token = emotion_token, instrument='drum')\n","    out_seq, out_df = OUTPUT_TOK.midi_to_tokens(out_file, update_vocab=True, update_sequences=True)\n","\n","    print(f'Input sequence length: {len(in_seq)}')\n","    print(f'Emotion token: {emotion_token}\\n')\n","\n","print(f'\\nNumber of input sequences: {len(INPUT_TOK.sequences)}')\n","print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n","print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n","print(f'\\nNumber of output sequences: {len(OUTPUT_TOK.sequences)}')\n","print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n","print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n","\n","print('\\nInput vocab:', INPUT_TOK.VOCAB.word2idx)\n","print('Output vocab:', OUTPUT_TOK.VOCAB.word2idx)\n","\n","# with open('training_seq.txt', 'w') as f:    \n","#     for seq in INPUT_TOK.sequences:\n","#         for tok in seq[:48]:\n","#             f.write('\\\"' + INPUT_TOK.VOCAB.idx2word[tok] + '\\\", ')\n","#         f.write('\\n')\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_2029157/4093092713.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/torch/csrc/utils/tensor_new.cpp:274.)\n","  dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n"]}],"source":["# Create the dataset\n","dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n","                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n","\n","# Split the dataset into training, evaluation and test sets\n","train_set, eval_set, test_set = random_split(dataset, [0.8, 0.1, 0.1])"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":539,"status":"ok","timestamp":1720350501148,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"TZ_LJL_F-fCN","outputId":"8031d1db-eddd-4add-9a47-b05fd05b2d2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size before augmentation: 125\n","Training set size after augmentation: 1750\n"]}],"source":["# Augment the training set\n","def data_augmentation_shift(dataset, shifts):\n","    '''\n","    Shifts the sequences by a number of ticks to create new sequences.\n","    '''\n","    augmented_input_sequences = []\n","    output_sequences = []\n","\n","    for ticks in shifts:\n","        for input_sequence, ouput_sequence in dataset:\n","            input_sequence = input_sequence.cpu().numpy().copy()\n","\n","            # remove the first token since it is the emotion token\n","            emotion_token = input_sequence[0]\n","            input_sequence = input_sequence[1:]\n","\n","            # shift the sequence\n","            new_input_sequence = np.roll(input_sequence, ticks)\n","\n","            # add the emotion token back to the sequence\n","            new_input_sequence = np.concatenate(([emotion_token], new_input_sequence))\n","\n","            # add the new sequence to the augmented sequences\n","            augmented_input_sequences.append(new_input_sequence)\n","            output_sequences.append(ouput_sequence.cpu().numpy().copy())\n","    \n","    augmented_dataset = TensorDataset(torch.LongTensor(augmented_input_sequences).to(device), \n","                                      torch.LongTensor(output_sequences).to(device))\n","    \n","    # Concatenate the original and the augmented dataset\n","    concatenated_dataset = torch.utils.data.ConcatDataset([dataset, augmented_dataset])\n","\n","    return concatenated_dataset\n","\n","\n","def data_augmentation_transposition(dataset, transpositions, probability=0.5):\n","    '''\n","    Transpose the sequences by a number of semitones to create new sequences.\n","\n","    Parameters:\n","    - transpositions: a list of integers representing the number of semitones to transpose the sequences.\n","\n","    NB: The transposition is done by adding the number of semitones to the pitch of each note in the sequence.\n","    '''\n","\n","    input_sequences = []\n","    augmented_output_sequences = []\n","\n","    for transposition in transpositions:\n","        for input_sequence, ouput_sequence in dataset:\n","\n","            input_sequence = input_sequence.cpu().numpy().copy()\n","            new_ouput_sequence = ouput_sequence.cpu().numpy().copy()\n","\n","            for i in range(len(new_ouput_sequence)):\n","\n","                token = ouput_sequence[i]\n","                word = OUTPUT_TOK.VOCAB.idx2word[token]\n","\n","                # check if the token is a note\n","                if word != SILENCE_TOKEN and word != BCI_TOKENS['relaxed'] and word != BCI_TOKENS['concentrated']:\n","\n","                    # extract all the pitches from the token \n","                    pitches = re.findall(r'\\d+', word) # NB: pitches is a string list\n","\n","                    # transpose pitch in the token with a probability\n","                    for pitch in pitches:\n","                        if np.random.rand() < probability:\n","                            new_pitch = str(int(pitch) + transposition)\n","                            word = word.replace(pitch, new_pitch)\n","\n","                    # add the new token to the vocabulary\n","                    OUTPUT_TOK.VOCAB.add_word(word) \n","\n","                    # update the sequence with the new token\n","                    new_ouput_sequence[i] = OUTPUT_TOK.VOCAB.word2idx[word]\n","            \n","            # update sequence with the new tokens\n","            input_sequences.append(input_sequence)\n","            augmented_output_sequences.append(new_ouput_sequence)\n","\n","    augmented_dataset = TensorDataset(torch.LongTensor(input_sequences).to(device), \n","                                      torch.LongTensor(augmented_output_sequences).to(device))\n","    \n","    # Concatenate the original and the augmented dataset\n","    concatenated_dataset = torch.utils.data.ConcatDataset([dataset, augmented_dataset])\n","\n","    return concatenated_dataset\n","\n","train_set_augmented = data_augmentation_shift(train_set, [-3, -2, -1, 1, 2, 3])\n","train_set_augmented = data_augmentation_transposition(train_set_augmented, [8], probability=0.2)\n","\n","print(f'Training set size before augmentation: {len(train_set)}')\n","print(f'Training set size after augmentation: {len(train_set_augmented)}')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train set size: 1750\n","Evaluation set size: 16\n","Test set size: 15\n"]}],"source":["def initialize_dataset():\n","\n","  # Create the dataloaders\n","  train_sampler = RandomSampler(train_set_augmented)\n","  train_dataloader = DataLoader(train_set_augmented, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","  eval_sampler = RandomSampler(eval_set)\n","  eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n","\n","  test_sampler = RandomSampler(test_set)\n","  test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n","\n","  return train_dataloader, eval_dataloader, test_dataloader\n","\n","train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","\n","print(f'Train set size: {len(train_set_augmented)}')\n","print(f'Evaluation set size: {len(eval_set)}')\n","print(f'Test set size: {len(test_set)}')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3117,"status":"ok","timestamp":1720350504263,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"BIzI5qkM-fCO","outputId":"7de9c3de-a426-4484-8487-c62ae3855b99"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Input size: 16\n","\n","Model created: TCN(\n","  (encoder): Embedding(16, 20, padding_idx=0)\n","  (tcn): TemporalConvNet(\n","    (network): Sequential(\n","      (0): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          20, 192, kernel_size=(3,), stride=(1,), padding=(2,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(2,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            20, 192, kernel_size=(3,), stride=(1,), padding=(2,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(2,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (downsample): Conv1d(20, 192, kernel_size=(1,), stride=(1,))\n","        (relu): ReLU()\n","      )\n","      (1): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (2): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (3): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (4): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (5): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (6): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (downsample): Conv1d(192, 20, kernel_size=(1,), stride=(1,))\n","        (relu): ReLU()\n","      )\n","    )\n","  )\n","  (decoder): Linear(in_features=20, out_features=121, bias=True)\n","  (drop): Dropout(p=0.25, inplace=False)\n",")\n","tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","        0.1000, 0.1000], device='cuda:0', grad_fn=<SelectBackward0>)\n"]}],"source":["# Set the hyperparameters\n","SEED = 1111\n","torch.manual_seed(SEED)\n","\n","'''\n","IMPORTANT:\n","to cover all the sequence of tokens k * d must be >= hidden units (see the paper)\n","k = kernel_size\n","d = dilation = 2 ^ (n_levels - 1)\n","'''\n","\n","OUTPUT_SIZE = len(OUTPUT_TOK.VOCAB)\n","\n","if FEEDBACK:\n","    INPUT_SIZE = len(INPUT_TOK.VOCAB) + OUTPUT_SIZE\n","    LEVELS = 8\n","    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH * 2 # 192 * 2 = 384\n","else:\n","    INPUT_SIZE = len(INPUT_TOK.VOCAB)\n","    LEVELS = 7\n","    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH # 192\n","\n","print(f'\\nInput size: {len(INPUT_TOK.VOCAB)}')\n","\n","\n","EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] real value vectors (see model.py)\n","NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE] # [192, 192, 192, 192, 192, 192, 20]\n","GRADIENT_CLIP = 0.35\n","\n","\n","# balance the loss function by assigning a weight to each token related to its frequency\n","LOSS_WEIGTHS = torch.ones([OUTPUT_SIZE], dtype=torch.float, device = device)\n","OUTPUT_TOK.VOCAB.compute_weights()\n","for i, weigth in enumerate(OUTPUT_TOK.VOCAB.weights):\n","    LOSS_WEIGTHS[i] = 1 - weigth\n","    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n","\n","\n","def initialize_model():\n","  # create the model\n","  model = TCN(input_size = INPUT_SIZE,\n","              embedding_size = EMBEDDING_SIZE,\n","              output_size = OUTPUT_SIZE,\n","              num_channels = NUM_CHANNELS,\n","              emphasize_eeg = EMPHASIZE_EEG,\n","              dropout = 0.45,\n","              emb_dropout = 0.25,\n","              kernel_size = 3,\n","              tied_weights = False) # tie encoder and decoder weights (legare)\n","\n","  model.to(device)\n","\n","  # May use adaptive softmax to speed up training\n","  criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n","  optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n","\n","  return model, criterion, optimizer\n","\n","model, criterion, optimizer = initialize_model()\n","\n","print(f'\\nModel created: {model}')\n","print(model.encoder.weight[0])\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tLO23hB8-fCP"},"outputs":[],"source":["def save_parameters():\n","\n","    # plot the losses over the epochs\n","\n","    plt.plot(train_losses, label='train')\n","    plt.plot(eval_losses, label='eval')\n","    plt.legend()\n","    plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))\n","    plt.clf()\n","\n","    # save the vocabularies\n","    INPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'input_vocab.txt'))\n","    OUTPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'output_vocab.txt'))\n","\n","     # save the model hyperparameters in a file txt\n","    with open(os.path.join(RESULTS_PATH, 'model_hyperparameters.txt'), 'w') as f:\n","\n","        f.write(f'DATE: {time.strftime(\"%Y%m%d-%H%M%S\")}\\n\\n')\n","\n","        f.write(f'-----------------DATASET------------------\\n')\n","        f.write(f'DATASET_PATH: {DATASET_PATH}\\n')\n","        f.write(f'TRAIN_SET_SIZE: {len(train_set)}\\n')\n","        f.write(f'EVAL_SET_SIZE: {len(eval_set)}\\n')\n","        f.write(f'TEST_SET_SIZE: {len(test_set)}\\n\\n')\n","\n","\n","        f.write(f'----------OPTIMIZATION PARAMETERS----------\\n')\n","        f.write(f'GRADIENT_CLIP: {GRADIENT_CLIP}\\n')\n","        f.write(f'FEEDBACK: {FEEDBACK}\\n')\n","        f.write(f'EARLY STOPPING: {EARLY_STOP}\\n')\n","        f.write(f'EMPHASIZE_EEG: {EMPHASIZE_EEG}\\n')\n","        f.write(f'LEARNING_RATE: {LEARNING_RATE}\\n')\n","        f.write(f'BATCH_SIZE: {BATCH_SIZE}\\n')\n","        f.write(f'EPOCHS: {EPOCHS}\\n\\n')\n","\n","\n","        f.write(f'------------MODEL PARAMETERS--------------\\n')\n","        f.write(f'SEED: {SEED}\\n')\n","        f.write(f'INPUT_SIZE: {INPUT_SIZE}\\n')\n","        f.write(f'EMBEDDING_SIZE: {EMBEDDING_SIZE}\\n')\n","        f.write(f'LEVELS: {LEVELS}\\n')\n","        f.write(f'HIDDEN_UNITS: {HIDDEN_UNITS}\\n')\n","        f.write(f'NUM_CHANNELS: {NUM_CHANNELS}\\n')\n","        f.write(f'OUTPUT_SIZE: {OUTPUT_SIZE}\\n')\n","        f.write(f'LOSS_WEIGTHS: {LOSS_WEIGTHS}\\n\\n')\n","\n","\n","\n","        f.write(f'-------------------RESULTS----------------\\n')\n","        f.write(f'TRAIN_LOSSES: {best_train_loss}\\n')\n","        f.write(f'BEST_EVAL_LOSS: {best_eval_loss}\\n')\n","        f.write(f'TEST_LOSS: {test_loss}\\n')\n","        f.write(f'BEST_MODEL_EPOCH: {best_model_epoch}\\n')\n","\n","    data = {\n","        'DATE': time.strftime(\"%Y%m%d-%H%M%S\"),\n","        'INPUT_SIZE': INPUT_SIZE,\n","        'EMBEDDING_SIZE': EMBEDDING_SIZE,\n","        'NUM_CHANNELS': NUM_CHANNELS,\n","        'OUTPUT_SIZE': OUTPUT_SIZE,\n","        'KERNEL_SIZE': 3\n","    }\n","\n","    path = os.path.join(RESULTS_PATH, 'config.yaml')\n","    with open(path, 'w') as file:\n","        yaml.safe_dump(data, file)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"8X0pPmwo-fCO"},"outputs":[],"source":["BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n","\n","def epoch_step(dataloader, mode):\n","\n","    if FEEDBACK:\n","        prev_output = torch.zeros([BATCH_SIZE, INPUT_TOK.SEQ_LENGTH], dtype=torch.long, device=device)\n","\n","    if mode == 'train':\n","        model.train()\n","    else:\n","        model.eval() # disable dropout\n","\n","    total_loss = 0\n","\n","    # iterate over the training data\n","    for batch_idx, (data, targets) in enumerate(dataloader):\n","\n","        batch_idx += 1\n","\n","        # mask the last bar of the input data\n","        batch_size = data.size(0)\n","        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long, device = device)), dim = 1)\n","\n","        if FEEDBACK:\n","            input = torch.cat((data_masked, prev_output[:batch_size, :]), dim = 1)\n","        else:\n","            input = data_masked\n","\n","        # reset model gradients to zero\n","        optimizer.zero_grad()\n","\n","        # make the prediction\n","        output = model(input)[:, :INPUT_TOK.SEQ_LENGTH]\n","        prev_output = torch.argmax(output, 2)# batch, seq_len (hidden units), vocab_size\n","\n","        # flatten the output sequence\n","        # NB: the size -1 is inferred from other dimensions\n","        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n","\n","        final_target = targets.contiguous().view(-1)\n","        final_output = output.contiguous().view(-1, OUTPUT_SIZE)\n","\n","        # calculate the loss\n","        loss = criterion(final_output, final_target)\n","\n","        if mode == 'train':\n","            # calculate the gradients\n","            loss.backward()\n","\n","            # clip the gradients to avoid exploding gradients\n","            if GRADIENT_CLIP > 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n","\n","            # update the weights\n","            optimizer.step()\n","\n","        total_loss += loss.data.item()\n","\n","    return total_loss / len(dataloader)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"2yimq9Xg-fCP"},"outputs":[],"source":["def train(results_path = None):\n","\n","    global RESULTS_PATH, MODEL_PATH\n","    global best_eval_loss, best_train_loss, best_model_epoch, train_losses, eval_losses\n","\n","    if results_path is None:\n","        RESULTS_PATH = os.path.join('results', time.strftime(\"%Y%m%d_%H%M%S\"))\n","    else:\n","        RESULTS_PATH = results_path\n","    \n","    if not os.path.exists(RESULTS_PATH):\n","        os.makedirs(RESULTS_PATH)\n","\n","    MODEL_PATH = os.path.join(RESULTS_PATH, 'model_state_dict.pth')\n","\n","    best_eval_loss = 1e8\n","    best_train_loss = 1e8\n","    best_model_epoch = 0\n","    eval_losses = []\n","    train_losses = []\n","    lr = LEARNING_RATE\n","\n","    for epoch in range(1, EPOCHS+1):\n","\n","        start_time = time.time()\n","\n","        train_loss = epoch_step(train_dataloader, 'train')\n","\n","        eval_loss = epoch_step(eval_dataloader, 'eval')\n","\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if eval_loss < best_eval_loss:\n","            # torch.save(model.state_dict(), MODEL_PATH)\n","            best_eval_loss = eval_loss\n","            best_model_epoch = epoch\n","\n","        if train_loss < best_train_loss:\n","            torch.save(model.state_dict(), MODEL_PATH)\n","            best_train_loss = train_loss\n","\n","        # # Anneal the learning rate if the validation loss plateaus\n","        # if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n","        #     lr = lr / 2.\n","        #     if lr < 0.1:\n","        #         lr = 2\n","        #     for param_group in optimizer.param_groups:\n","        #         param_group['lr'] = lr\n","\n","\n","        eval_losses.append(eval_loss)\n","        train_losses.append(train_loss)\n","\n","        # Early stopping\n","        if EARLY_STOP:\n","          if epoch > 15:\n","              if min(train_losses[-15:]) > best_train_loss:\n","                  break\n","\n","        # print the loss and the progress\n","        elapsed = time.time() - start_time\n","        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n","                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n","\n","\n","    print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d} \\n\\n' \\\n","            .format(best_eval_loss, best_model_epoch))\n","\n","\n","    # test the model\n","    global test_loss\n","    test_loss = epoch_step(test_dataloader, 'eval')\n","    print(f'\\n\\nTEST LOSS: {test_loss}')\n","\n","    save_parameters()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2641510,"status":"ok","timestamp":1720355202428,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"EN4N3Ob3a4Gh","outputId":"e089bc58-d5ba-4738-b140-2a9897a5001a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/marco_bortolotti/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv1d(input, weight, bias, self.stride,\n"]},{"name":"stdout","output_type":"stream","text":["| epoch   1/500 | lr 2.00000 | ms/epoch 6449.82195 | train_loss  3.47 | eval_loss  3.47\n","| epoch   2/500 | lr 2.00000 | ms/epoch 4152.54283 | train_loss  3.37 | eval_loss  3.41\n","| epoch   3/500 | lr 2.00000 | ms/epoch 4021.02041 | train_loss  3.36 | eval_loss  3.42\n","| epoch   4/500 | lr 2.00000 | ms/epoch 4274.34158 | train_loss  3.34 | eval_loss  3.43\n","| epoch   5/500 | lr 2.00000 | ms/epoch 4126.55830 | train_loss  3.28 | eval_loss  3.35\n","| epoch   6/500 | lr 2.00000 | ms/epoch 4105.21173 | train_loss  3.25 | eval_loss  3.34\n","| epoch   7/500 | lr 2.00000 | ms/epoch 4125.22030 | train_loss  3.20 | eval_loss  3.19\n","| epoch   8/500 | lr 2.00000 | ms/epoch 4137.72249 | train_loss  2.96 | eval_loss  3.04\n","| epoch   9/500 | lr 2.00000 | ms/epoch 4159.85751 | train_loss  2.83 | eval_loss  2.82\n","| epoch  10/500 | lr 2.00000 | ms/epoch 4267.06314 | train_loss  2.77 | eval_loss  2.77\n","| epoch  11/500 | lr 2.00000 | ms/epoch 4196.30241 | train_loss  2.74 | eval_loss  2.81\n","| epoch  12/500 | lr 2.00000 | ms/epoch 4222.98336 | train_loss  2.72 | eval_loss  2.83\n","| epoch  13/500 | lr 2.00000 | ms/epoch 4282.13620 | train_loss  2.70 | eval_loss  2.88\n","| epoch  14/500 | lr 2.00000 | ms/epoch 4355.63135 | train_loss  2.69 | eval_loss  2.85\n","| epoch  15/500 | lr 2.00000 | ms/epoch 4450.66810 | train_loss  2.68 | eval_loss  2.83\n","| epoch  16/500 | lr 2.00000 | ms/epoch 2203.62401 | train_loss  2.67 | eval_loss  2.76\n","| epoch  17/500 | lr 2.00000 | ms/epoch 2044.12627 | train_loss  2.67 | eval_loss  2.77\n","| epoch  18/500 | lr 2.00000 | ms/epoch 2028.13792 | train_loss  2.66 | eval_loss  2.77\n","| epoch  19/500 | lr 2.00000 | ms/epoch 2076.24483 | train_loss  2.65 | eval_loss  2.77\n","| epoch  20/500 | lr 2.00000 | ms/epoch 2037.37330 | train_loss  2.65 | eval_loss  2.82\n","| epoch  21/500 | lr 2.00000 | ms/epoch 2055.67575 | train_loss  2.65 | eval_loss  2.78\n","| epoch  22/500 | lr 2.00000 | ms/epoch 2048.36941 | train_loss  2.64 | eval_loss  2.78\n","| epoch  23/500 | lr 2.00000 | ms/epoch 2039.46757 | train_loss  2.64 | eval_loss  2.81\n","| epoch  24/500 | lr 2.00000 | ms/epoch 2014.33158 | train_loss  2.64 | eval_loss  2.75\n","| epoch  25/500 | lr 2.00000 | ms/epoch 2009.82475 | train_loss  2.63 | eval_loss  2.82\n","| epoch  26/500 | lr 2.00000 | ms/epoch 1992.81740 | train_loss  2.63 | eval_loss  2.77\n","| epoch  27/500 | lr 2.00000 | ms/epoch 1984.82513 | train_loss  2.63 | eval_loss  2.77\n","| epoch  28/500 | lr 2.00000 | ms/epoch 1978.44505 | train_loss  2.63 | eval_loss  2.79\n","| epoch  29/500 | lr 2.00000 | ms/epoch 2018.67008 | train_loss  2.62 | eval_loss  2.77\n","| epoch  30/500 | lr 2.00000 | ms/epoch 1984.41577 | train_loss  2.62 | eval_loss  2.78\n","| epoch  31/500 | lr 2.00000 | ms/epoch 1976.51196 | train_loss  2.62 | eval_loss  2.80\n","| epoch  32/500 | lr 2.00000 | ms/epoch 3876.28055 | train_loss  2.61 | eval_loss  2.82\n","| epoch  33/500 | lr 2.00000 | ms/epoch 4095.42227 | train_loss  2.61 | eval_loss  2.82\n","| epoch  34/500 | lr 2.00000 | ms/epoch 4059.48377 | train_loss  2.60 | eval_loss  2.77\n","| epoch  35/500 | lr 2.00000 | ms/epoch 4020.78390 | train_loss  2.56 | eval_loss  2.78\n","| epoch  36/500 | lr 2.00000 | ms/epoch 4389.71972 | train_loss  2.53 | eval_loss  2.73\n","| epoch  37/500 | lr 2.00000 | ms/epoch 4258.58164 | train_loss  2.50 | eval_loss  2.72\n","| epoch  38/500 | lr 2.00000 | ms/epoch 4189.36253 | train_loss  2.47 | eval_loss  2.67\n","| epoch  39/500 | lr 2.00000 | ms/epoch 4226.73273 | train_loss  2.44 | eval_loss  2.76\n","| epoch  40/500 | lr 2.00000 | ms/epoch 4289.17885 | train_loss  2.41 | eval_loss  2.71\n","| epoch  41/500 | lr 2.00000 | ms/epoch 4147.33434 | train_loss  2.39 | eval_loss  2.72\n","| epoch  42/500 | lr 2.00000 | ms/epoch 4230.99208 | train_loss  2.37 | eval_loss  2.80\n","| epoch  43/500 | lr 2.00000 | ms/epoch 4160.89773 | train_loss  2.36 | eval_loss  2.71\n","| epoch  44/500 | lr 2.00000 | ms/epoch 4295.31956 | train_loss  2.32 | eval_loss  2.75\n","| epoch  45/500 | lr 2.00000 | ms/epoch 4267.26747 | train_loss  2.31 | eval_loss  2.66\n","| epoch  46/500 | lr 2.00000 | ms/epoch 4231.15039 | train_loss  2.29 | eval_loss  2.79\n","| epoch  47/500 | lr 2.00000 | ms/epoch 4357.66101 | train_loss  2.26 | eval_loss  2.84\n","| epoch  48/500 | lr 2.00000 | ms/epoch 2262.66456 | train_loss  2.25 | eval_loss  2.75\n","| epoch  49/500 | lr 2.00000 | ms/epoch 2014.79220 | train_loss  2.24 | eval_loss  2.77\n","| epoch  50/500 | lr 2.00000 | ms/epoch 2059.25727 | train_loss  2.21 | eval_loss  2.78\n","| epoch  51/500 | lr 2.00000 | ms/epoch 2064.96334 | train_loss  2.20 | eval_loss  2.83\n","| epoch  52/500 | lr 2.00000 | ms/epoch 2077.03805 | train_loss  2.19 | eval_loss  2.85\n","| epoch  53/500 | lr 2.00000 | ms/epoch 2064.70323 | train_loss  2.16 | eval_loss  2.85\n","| epoch  54/500 | lr 2.00000 | ms/epoch 2047.64628 | train_loss  2.14 | eval_loss  2.79\n","| epoch  55/500 | lr 2.00000 | ms/epoch 2047.98007 | train_loss  2.10 | eval_loss  2.80\n","| epoch  56/500 | lr 2.00000 | ms/epoch 2011.77597 | train_loss  2.06 | eval_loss  2.80\n","| epoch  57/500 | lr 2.00000 | ms/epoch 2023.53382 | train_loss  2.03 | eval_loss  2.71\n","| epoch  58/500 | lr 2.00000 | ms/epoch 2025.02227 | train_loss  2.01 | eval_loss  2.79\n","| epoch  59/500 | lr 2.00000 | ms/epoch 1997.89381 | train_loss  1.98 | eval_loss  2.80\n","| epoch  60/500 | lr 2.00000 | ms/epoch 2003.49212 | train_loss  1.96 | eval_loss  2.79\n","| epoch  61/500 | lr 2.00000 | ms/epoch 2051.07164 | train_loss  1.93 | eval_loss  2.83\n","| epoch  62/500 | lr 2.00000 | ms/epoch 2042.71078 | train_loss  1.92 | eval_loss  2.85\n","| epoch  63/500 | lr 2.00000 | ms/epoch 2010.21624 | train_loss  1.91 | eval_loss  2.71\n","| epoch  64/500 | lr 2.00000 | ms/epoch 3934.15141 | train_loss  1.87 | eval_loss  2.82\n","| epoch  65/500 | lr 2.00000 | ms/epoch 4045.08853 | train_loss  1.86 | eval_loss  2.83\n","| epoch  66/500 | lr 2.00000 | ms/epoch 4039.21819 | train_loss  1.84 | eval_loss  2.89\n","| epoch  67/500 | lr 2.00000 | ms/epoch 4013.84497 | train_loss  1.83 | eval_loss  2.77\n","| epoch  68/500 | lr 2.00000 | ms/epoch 4203.39799 | train_loss  1.81 | eval_loss  2.74\n","| epoch  69/500 | lr 2.00000 | ms/epoch 4205.04284 | train_loss  1.80 | eval_loss  2.80\n","| epoch  70/500 | lr 2.00000 | ms/epoch 4339.64682 | train_loss  1.80 | eval_loss  2.76\n","| epoch  71/500 | lr 2.00000 | ms/epoch 4230.11708 | train_loss  1.76 | eval_loss  2.78\n","| epoch  72/500 | lr 2.00000 | ms/epoch 4229.59304 | train_loss  1.76 | eval_loss  2.78\n","| epoch  73/500 | lr 2.00000 | ms/epoch 4320.52708 | train_loss  1.71 | eval_loss  2.75\n","| epoch  74/500 | lr 2.00000 | ms/epoch 4290.03406 | train_loss  1.69 | eval_loss  2.71\n","| epoch  75/500 | lr 2.00000 | ms/epoch 4266.22748 | train_loss  1.68 | eval_loss  2.74\n","| epoch  76/500 | lr 2.00000 | ms/epoch 4291.42070 | train_loss  1.66 | eval_loss  2.74\n","| epoch  77/500 | lr 2.00000 | ms/epoch 4285.44641 | train_loss  1.66 | eval_loss  2.71\n","| epoch  78/500 | lr 2.00000 | ms/epoch 4243.36815 | train_loss  1.63 | eval_loss  2.80\n","| epoch  79/500 | lr 2.00000 | ms/epoch 3088.65047 | train_loss  1.62 | eval_loss  2.83\n","| epoch  80/500 | lr 2.00000 | ms/epoch 2035.52389 | train_loss  1.60 | eval_loss  2.77\n","| epoch  81/500 | lr 2.00000 | ms/epoch 2021.27194 | train_loss  1.58 | eval_loss  2.74\n","| epoch  82/500 | lr 2.00000 | ms/epoch 2026.69954 | train_loss  1.58 | eval_loss  2.75\n","| epoch  83/500 | lr 2.00000 | ms/epoch 2026.91269 | train_loss  1.56 | eval_loss  2.78\n","| epoch  84/500 | lr 2.00000 | ms/epoch 2059.69143 | train_loss  1.55 | eval_loss  2.71\n","| epoch  85/500 | lr 2.00000 | ms/epoch 2071.16985 | train_loss  1.54 | eval_loss  2.64\n","| epoch  86/500 | lr 2.00000 | ms/epoch 2023.28634 | train_loss  1.58 | eval_loss  2.79\n","| epoch  87/500 | lr 2.00000 | ms/epoch 2010.62250 | train_loss  1.52 | eval_loss  2.70\n","| epoch  88/500 | lr 2.00000 | ms/epoch 2031.00562 | train_loss  1.50 | eval_loss  2.75\n","| epoch  89/500 | lr 2.00000 | ms/epoch 2045.08924 | train_loss  1.49 | eval_loss  2.70\n","| epoch  90/500 | lr 2.00000 | ms/epoch 2053.09367 | train_loss  1.48 | eval_loss  2.72\n","| epoch  91/500 | lr 2.00000 | ms/epoch 2036.07178 | train_loss  1.54 | eval_loss  2.76\n","| epoch  92/500 | lr 2.00000 | ms/epoch 2039.55674 | train_loss  1.47 | eval_loss  2.71\n","| epoch  93/500 | lr 2.00000 | ms/epoch 1990.83257 | train_loss  1.45 | eval_loss  2.73\n","| epoch  94/500 | lr 2.00000 | ms/epoch 2015.37871 | train_loss  1.44 | eval_loss  2.66\n","| epoch  95/500 | lr 2.00000 | ms/epoch 3198.60220 | train_loss  1.44 | eval_loss  2.73\n","| epoch  96/500 | lr 2.00000 | ms/epoch 4029.62089 | train_loss  1.43 | eval_loss  2.83\n","| epoch  97/500 | lr 2.00000 | ms/epoch 4026.38721 | train_loss  1.42 | eval_loss  2.73\n","| epoch  98/500 | lr 2.00000 | ms/epoch 4007.37667 | train_loss  1.41 | eval_loss  2.65\n","| epoch  99/500 | lr 2.00000 | ms/epoch 4244.92240 | train_loss  1.40 | eval_loss  2.74\n","| epoch 100/500 | lr 2.00000 | ms/epoch 4282.13525 | train_loss  1.39 | eval_loss  2.68\n","| epoch 101/500 | lr 2.00000 | ms/epoch 4178.52426 | train_loss  1.38 | eval_loss  2.66\n","| epoch 102/500 | lr 2.00000 | ms/epoch 4433.03609 | train_loss  1.37 | eval_loss  2.71\n","| epoch 103/500 | lr 2.00000 | ms/epoch 4165.96985 | train_loss  1.37 | eval_loss  2.68\n","| epoch 104/500 | lr 2.00000 | ms/epoch 4182.37710 | train_loss  1.36 | eval_loss  2.73\n","| epoch 105/500 | lr 2.00000 | ms/epoch 4316.21027 | train_loss  1.36 | eval_loss  2.67\n","| epoch 106/500 | lr 2.00000 | ms/epoch 4386.35278 | train_loss  1.35 | eval_loss  2.66\n","| epoch 107/500 | lr 2.00000 | ms/epoch 4290.75265 | train_loss  1.35 | eval_loss  2.64\n","| epoch 108/500 | lr 2.00000 | ms/epoch 4221.70210 | train_loss  1.34 | eval_loss  2.63\n","| epoch 109/500 | lr 2.00000 | ms/epoch 4342.88454 | train_loss  1.33 | eval_loss  2.52\n","| epoch 110/500 | lr 2.00000 | ms/epoch 3664.45684 | train_loss  1.34 | eval_loss  2.54\n","| epoch 111/500 | lr 2.00000 | ms/epoch 2043.37764 | train_loss  1.32 | eval_loss  2.59\n","| epoch 112/500 | lr 2.00000 | ms/epoch 2037.85086 | train_loss  1.31 | eval_loss  2.58\n","| epoch 113/500 | lr 2.00000 | ms/epoch 2037.95385 | train_loss  1.32 | eval_loss  2.61\n","| epoch 114/500 | lr 2.00000 | ms/epoch 2085.33311 | train_loss  1.30 | eval_loss  2.49\n","| epoch 115/500 | lr 2.00000 | ms/epoch 2051.11313 | train_loss  1.29 | eval_loss  2.64\n","| epoch 116/500 | lr 2.00000 | ms/epoch 2063.82394 | train_loss  1.29 | eval_loss  2.61\n","| epoch 117/500 | lr 2.00000 | ms/epoch 2059.30328 | train_loss  1.28 | eval_loss  2.47\n","| epoch 118/500 | lr 2.00000 | ms/epoch 2111.65500 | train_loss  1.29 | eval_loss  2.52\n","| epoch 119/500 | lr 2.00000 | ms/epoch 2091.66479 | train_loss  1.27 | eval_loss  2.59\n","| epoch 120/500 | lr 2.00000 | ms/epoch 2083.23145 | train_loss  1.26 | eval_loss  2.55\n","| epoch 121/500 | lr 2.00000 | ms/epoch 2028.15390 | train_loss  1.26 | eval_loss  2.48\n","| epoch 122/500 | lr 2.00000 | ms/epoch 2013.38530 | train_loss  1.27 | eval_loss  2.53\n","| epoch 123/500 | lr 2.00000 | ms/epoch 2058.85196 | train_loss  1.25 | eval_loss  2.59\n","| epoch 124/500 | lr 2.00000 | ms/epoch 2065.35220 | train_loss  1.25 | eval_loss  2.41\n","| epoch 125/500 | lr 2.00000 | ms/epoch 2020.10393 | train_loss  1.24 | eval_loss  2.48\n","| epoch 126/500 | lr 2.00000 | ms/epoch 3343.60075 | train_loss  1.25 | eval_loss  2.59\n","| epoch 127/500 | lr 2.00000 | ms/epoch 4137.97617 | train_loss  1.23 | eval_loss  2.53\n","| epoch 128/500 | lr 2.00000 | ms/epoch 4010.57911 | train_loss  1.23 | eval_loss  2.52\n","| epoch 129/500 | lr 2.00000 | ms/epoch 4098.12856 | train_loss  1.23 | eval_loss  2.50\n","| epoch 130/500 | lr 2.00000 | ms/epoch 4170.33505 | train_loss  1.22 | eval_loss  2.52\n","| epoch 131/500 | lr 2.00000 | ms/epoch 4261.85203 | train_loss  1.24 | eval_loss  2.54\n","| epoch 132/500 | lr 2.00000 | ms/epoch 4319.14830 | train_loss  1.21 | eval_loss  2.50\n","| epoch 133/500 | lr 2.00000 | ms/epoch 4258.08859 | train_loss  1.21 | eval_loss  2.48\n","| epoch 134/500 | lr 2.00000 | ms/epoch 4497.42842 | train_loss  1.22 | eval_loss  2.46\n","| epoch 135/500 | lr 2.00000 | ms/epoch 4393.66913 | train_loss  1.21 | eval_loss  2.44\n","| epoch 136/500 | lr 2.00000 | ms/epoch 4389.11414 | train_loss  1.19 | eval_loss  2.47\n","| epoch 137/500 | lr 2.00000 | ms/epoch 4123.02470 | train_loss  1.18 | eval_loss  2.33\n","| epoch 138/500 | lr 2.00000 | ms/epoch 4106.10199 | train_loss  1.18 | eval_loss  2.43\n","| epoch 139/500 | lr 2.00000 | ms/epoch 4233.88720 | train_loss  1.18 | eval_loss  2.42\n","| epoch 140/500 | lr 2.00000 | ms/epoch 4280.94292 | train_loss  1.17 | eval_loss  2.50\n","| epoch 141/500 | lr 2.00000 | ms/epoch 4291.55111 | train_loss  1.17 | eval_loss  2.42\n","| epoch 142/500 | lr 2.00000 | ms/epoch 2695.37640 | train_loss  1.16 | eval_loss  2.40\n","| epoch 143/500 | lr 2.00000 | ms/epoch 2042.21511 | train_loss  1.15 | eval_loss  2.42\n","| epoch 144/500 | lr 2.00000 | ms/epoch 2086.82799 | train_loss  1.15 | eval_loss  2.45\n","| epoch 145/500 | lr 2.00000 | ms/epoch 2067.20209 | train_loss  1.15 | eval_loss  2.32\n","| epoch 146/500 | lr 2.00000 | ms/epoch 2088.19342 | train_loss  1.14 | eval_loss  2.41\n","| epoch 147/500 | lr 2.00000 | ms/epoch 2097.55754 | train_loss  1.14 | eval_loss  2.52\n","| epoch 148/500 | lr 2.00000 | ms/epoch 2031.71134 | train_loss  1.13 | eval_loss  2.33\n","| epoch 149/500 | lr 2.00000 | ms/epoch 2056.06055 | train_loss  1.14 | eval_loss  2.43\n","| epoch 150/500 | lr 2.00000 | ms/epoch 2078.86195 | train_loss  1.12 | eval_loss  2.45\n","| epoch 151/500 | lr 2.00000 | ms/epoch 2040.86804 | train_loss  1.13 | eval_loss  2.42\n","| epoch 152/500 | lr 2.00000 | ms/epoch 2083.53186 | train_loss  1.12 | eval_loss  2.42\n","| epoch 153/500 | lr 2.00000 | ms/epoch 2022.28975 | train_loss  1.12 | eval_loss  2.40\n","| epoch 154/500 | lr 2.00000 | ms/epoch 2043.73288 | train_loss  1.12 | eval_loss  2.45\n","| epoch 155/500 | lr 2.00000 | ms/epoch 2007.94983 | train_loss  1.10 | eval_loss  2.42\n","| epoch 156/500 | lr 2.00000 | ms/epoch 2021.86036 | train_loss  1.10 | eval_loss  2.40\n","| epoch 157/500 | lr 2.00000 | ms/epoch 2036.69405 | train_loss  1.10 | eval_loss  2.38\n","| epoch 158/500 | lr 2.00000 | ms/epoch 3947.75605 | train_loss  1.11 | eval_loss  2.40\n","| epoch 159/500 | lr 2.00000 | ms/epoch 4070.29629 | train_loss  1.09 | eval_loss  2.42\n","| epoch 160/500 | lr 2.00000 | ms/epoch 4077.40712 | train_loss  1.09 | eval_loss  2.43\n","| epoch 161/500 | lr 2.00000 | ms/epoch 3926.88751 | train_loss  1.12 | eval_loss  2.39\n","| epoch 162/500 | lr 2.00000 | ms/epoch 4090.39426 | train_loss  1.08 | eval_loss  2.39\n","| epoch 163/500 | lr 2.00000 | ms/epoch 4317.60812 | train_loss  1.09 | eval_loss  2.37\n","| epoch 164/500 | lr 2.00000 | ms/epoch 4239.86864 | train_loss  1.08 | eval_loss  2.34\n","| epoch 165/500 | lr 2.00000 | ms/epoch 4326.20263 | train_loss  1.10 | eval_loss  2.41\n","| epoch 166/500 | lr 2.00000 | ms/epoch 4283.00834 | train_loss  1.10 | eval_loss  2.43\n","| epoch 167/500 | lr 2.00000 | ms/epoch 4302.82879 | train_loss  1.08 | eval_loss  2.40\n","| epoch 168/500 | lr 2.00000 | ms/epoch 4363.04617 | train_loss  1.06 | eval_loss  2.37\n","| epoch 169/500 | lr 2.00000 | ms/epoch 4322.86000 | train_loss  1.07 | eval_loss  2.36\n","| epoch 170/500 | lr 2.00000 | ms/epoch 4303.16854 | train_loss  1.05 | eval_loss  2.38\n","| epoch 171/500 | lr 2.00000 | ms/epoch 4186.18250 | train_loss  1.06 | eval_loss  2.38\n","| epoch 172/500 | lr 2.00000 | ms/epoch 4276.27420 | train_loss  1.05 | eval_loss  2.43\n","| epoch 173/500 | lr 2.00000 | ms/epoch 3291.20660 | train_loss  1.04 | eval_loss  2.40\n","| epoch 174/500 | lr 2.00000 | ms/epoch 2006.00553 | train_loss  1.05 | eval_loss  2.36\n","| epoch 175/500 | lr 2.00000 | ms/epoch 2010.41675 | train_loss  1.04 | eval_loss  2.37\n","| epoch 176/500 | lr 2.00000 | ms/epoch 2004.77719 | train_loss  1.05 | eval_loss  2.46\n","| epoch 177/500 | lr 2.00000 | ms/epoch 2017.72404 | train_loss  1.04 | eval_loss  2.41\n","| epoch 178/500 | lr 2.00000 | ms/epoch 1991.11104 | train_loss  1.05 | eval_loss  2.42\n","| epoch 179/500 | lr 2.00000 | ms/epoch 2006.13976 | train_loss  1.03 | eval_loss  2.38\n","| epoch 180/500 | lr 2.00000 | ms/epoch 2005.33414 | train_loss  1.03 | eval_loss  2.41\n","| epoch 181/500 | lr 2.00000 | ms/epoch 1988.47198 | train_loss  1.04 | eval_loss  2.42\n","| epoch 182/500 | lr 2.00000 | ms/epoch 2065.93132 | train_loss  1.02 | eval_loss  2.31\n","| epoch 183/500 | lr 2.00000 | ms/epoch 2044.09313 | train_loss  1.02 | eval_loss  2.41\n","| epoch 184/500 | lr 2.00000 | ms/epoch 2034.39426 | train_loss  1.02 | eval_loss  2.27\n","| epoch 185/500 | lr 2.00000 | ms/epoch 1977.15306 | train_loss  1.03 | eval_loss  2.38\n","| epoch 186/500 | lr 2.00000 | ms/epoch 1966.08782 | train_loss  1.02 | eval_loss  2.31\n","| epoch 187/500 | lr 2.00000 | ms/epoch 1995.12267 | train_loss  1.02 | eval_loss  2.46\n","| epoch 188/500 | lr 2.00000 | ms/epoch 1992.39016 | train_loss  1.00 | eval_loss  2.35\n","| epoch 189/500 | lr 2.00000 | ms/epoch 2492.16032 | train_loss  1.00 | eval_loss  2.38\n","| epoch 190/500 | lr 2.00000 | ms/epoch 3947.67165 | train_loss  1.00 | eval_loss  2.42\n","| epoch 191/500 | lr 2.00000 | ms/epoch 4058.96449 | train_loss  1.00 | eval_loss  2.33\n","| epoch 192/500 | lr 2.00000 | ms/epoch 3967.78107 | train_loss  1.02 | eval_loss  2.39\n","| epoch 193/500 | lr 2.00000 | ms/epoch 4000.20051 | train_loss  1.00 | eval_loss  2.31\n","| epoch 194/500 | lr 2.00000 | ms/epoch 4314.45599 | train_loss  1.02 | eval_loss  2.26\n","| epoch 195/500 | lr 2.00000 | ms/epoch 4317.63887 | train_loss  1.00 | eval_loss  2.39\n","| epoch 196/500 | lr 2.00000 | ms/epoch 4275.44379 | train_loss  1.01 | eval_loss  2.38\n","| epoch 197/500 | lr 2.00000 | ms/epoch 4459.36131 | train_loss  0.99 | eval_loss  2.41\n","| epoch 198/500 | lr 2.00000 | ms/epoch 4297.48154 | train_loss  0.98 | eval_loss  2.30\n","| epoch 199/500 | lr 2.00000 | ms/epoch 4277.29607 | train_loss  1.00 | eval_loss  2.39\n","| epoch 200/500 | lr 2.00000 | ms/epoch 4190.35292 | train_loss  0.98 | eval_loss  2.32\n","| epoch 201/500 | lr 2.00000 | ms/epoch 4089.91885 | train_loss  0.99 | eval_loss  2.29\n","| epoch 202/500 | lr 2.00000 | ms/epoch 4089.98990 | train_loss  0.99 | eval_loss  2.31\n","| epoch 203/500 | lr 2.00000 | ms/epoch 4341.19797 | train_loss  0.99 | eval_loss  2.39\n","| epoch 204/500 | lr 2.00000 | ms/epoch 4157.02009 | train_loss  0.98 | eval_loss  2.33\n","| epoch 205/500 | lr 2.00000 | ms/epoch 2609.20858 | train_loss  0.97 | eval_loss  2.37\n","| epoch 206/500 | lr 2.00000 | ms/epoch 2060.52446 | train_loss  0.97 | eval_loss  2.38\n","| epoch 207/500 | lr 2.00000 | ms/epoch 2057.73783 | train_loss  0.98 | eval_loss  2.40\n","| epoch 208/500 | lr 2.00000 | ms/epoch 2004.44674 | train_loss  0.97 | eval_loss  2.36\n","| epoch 209/500 | lr 2.00000 | ms/epoch 1996.33455 | train_loss  1.00 | eval_loss  2.40\n","| epoch 210/500 | lr 2.00000 | ms/epoch 2000.62728 | train_loss  0.96 | eval_loss  2.39\n","| epoch 211/500 | lr 2.00000 | ms/epoch 1992.32864 | train_loss  0.97 | eval_loss  2.30\n","| epoch 212/500 | lr 2.00000 | ms/epoch 2029.79088 | train_loss  0.99 | eval_loss  2.39\n","| epoch 213/500 | lr 2.00000 | ms/epoch 2030.60293 | train_loss  0.97 | eval_loss  2.39\n","| epoch 214/500 | lr 2.00000 | ms/epoch 2048.15793 | train_loss  0.96 | eval_loss  2.30\n","| epoch 215/500 | lr 2.00000 | ms/epoch 2013.63277 | train_loss  0.98 | eval_loss  2.34\n","| epoch 216/500 | lr 2.00000 | ms/epoch 1988.37090 | train_loss  0.97 | eval_loss  2.39\n","| epoch 217/500 | lr 2.00000 | ms/epoch 2021.39401 | train_loss  0.95 | eval_loss  2.42\n","| epoch 218/500 | lr 2.00000 | ms/epoch 1991.71782 | train_loss  0.96 | eval_loss  2.46\n","| epoch 219/500 | lr 2.00000 | ms/epoch 2012.70604 | train_loss  0.95 | eval_loss  2.35\n","| epoch 220/500 | lr 2.00000 | ms/epoch 2018.31579 | train_loss  0.95 | eval_loss  2.30\n","| epoch 221/500 | lr 2.00000 | ms/epoch 3343.17064 | train_loss  0.97 | eval_loss  2.29\n","| epoch 222/500 | lr 2.00000 | ms/epoch 4031.66366 | train_loss  0.95 | eval_loss  2.38\n","| epoch 223/500 | lr 2.00000 | ms/epoch 4064.10980 | train_loss  0.95 | eval_loss  2.33\n","| epoch 224/500 | lr 2.00000 | ms/epoch 4036.31210 | train_loss  0.95 | eval_loss  2.35\n","| epoch 225/500 | lr 2.00000 | ms/epoch 4070.68872 | train_loss  0.94 | eval_loss  2.36\n","| epoch 226/500 | lr 2.00000 | ms/epoch 4122.11180 | train_loss  0.96 | eval_loss  2.32\n","| epoch 227/500 | lr 2.00000 | ms/epoch 4264.79173 | train_loss  0.95 | eval_loss  2.39\n","| epoch 228/500 | lr 2.00000 | ms/epoch 4199.91446 | train_loss  0.94 | eval_loss  2.35\n","| epoch 229/500 | lr 2.00000 | ms/epoch 4182.79457 | train_loss  0.94 | eval_loss  2.30\n","| epoch 230/500 | lr 2.00000 | ms/epoch 4182.46794 | train_loss  0.93 | eval_loss  2.41\n","| epoch 231/500 | lr 2.00000 | ms/epoch 4170.36653 | train_loss  0.95 | eval_loss  2.34\n","| epoch 232/500 | lr 2.00000 | ms/epoch 4158.62727 | train_loss  1.05 | eval_loss  2.41\n","| epoch 233/500 | lr 2.00000 | ms/epoch 4336.87854 | train_loss  0.95 | eval_loss  2.28\n","| epoch 234/500 | lr 2.00000 | ms/epoch 4358.20723 | train_loss  0.93 | eval_loss  2.34\n","| epoch 235/500 | lr 2.00000 | ms/epoch 4221.05169 | train_loss  0.96 | eval_loss  2.28\n","| epoch 236/500 | lr 2.00000 | ms/epoch 4338.23133 | train_loss  0.92 | eval_loss  2.36\n","| epoch 237/500 | lr 2.00000 | ms/epoch 3676.70107 | train_loss  0.94 | eval_loss  2.33\n","| epoch 238/500 | lr 2.00000 | ms/epoch 2009.26447 | train_loss  0.93 | eval_loss  2.35\n","| epoch 239/500 | lr 2.00000 | ms/epoch 2051.71037 | train_loss  0.92 | eval_loss  2.39\n","| epoch 240/500 | lr 2.00000 | ms/epoch 1991.45889 | train_loss  0.93 | eval_loss  2.34\n","| epoch 241/500 | lr 2.00000 | ms/epoch 1996.05536 | train_loss  0.93 | eval_loss  2.29\n","| epoch 242/500 | lr 2.00000 | ms/epoch 2040.77530 | train_loss  0.93 | eval_loss  2.38\n","| epoch 243/500 | lr 2.00000 | ms/epoch 1996.87004 | train_loss  0.92 | eval_loss  2.35\n","| epoch 244/500 | lr 2.00000 | ms/epoch 2038.76257 | train_loss  0.92 | eval_loss  2.33\n","| epoch 245/500 | lr 2.00000 | ms/epoch 2023.24700 | train_loss  0.92 | eval_loss  2.34\n","| epoch 246/500 | lr 2.00000 | ms/epoch 1995.71395 | train_loss  0.92 | eval_loss  2.48\n","| epoch 247/500 | lr 2.00000 | ms/epoch 2039.01458 | train_loss  0.91 | eval_loss  2.32\n","| epoch 248/500 | lr 2.00000 | ms/epoch 2014.93597 | train_loss  0.96 | eval_loss  2.39\n","| epoch 249/500 | lr 2.00000 | ms/epoch 2052.60944 | train_loss  0.91 | eval_loss  2.40\n","| epoch 250/500 | lr 2.00000 | ms/epoch 2002.13552 | train_loss  0.93 | eval_loss  2.30\n","| epoch 251/500 | lr 2.00000 | ms/epoch 2033.44822 | train_loss  0.91 | eval_loss  2.32\n","| epoch 252/500 | lr 2.00000 | ms/epoch 2067.80314 | train_loss  0.92 | eval_loss  2.39\n","| epoch 253/500 | lr 2.00000 | ms/epoch 2818.48145 | train_loss  0.91 | eval_loss  2.43\n","| epoch 254/500 | lr 2.00000 | ms/epoch 4172.24002 | train_loss  0.90 | eval_loss  2.30\n","| epoch 255/500 | lr 2.00000 | ms/epoch 4177.52743 | train_loss  0.92 | eval_loss  2.41\n","| epoch 256/500 | lr 2.00000 | ms/epoch 4282.96041 | train_loss  0.91 | eval_loss  2.54\n","| epoch 257/500 | lr 2.00000 | ms/epoch 4054.80218 | train_loss  0.92 | eval_loss  2.28\n","| epoch 258/500 | lr 2.00000 | ms/epoch 4251.33896 | train_loss  0.91 | eval_loss  2.30\n","| epoch 259/500 | lr 2.00000 | ms/epoch 4193.76874 | train_loss  0.91 | eval_loss  2.35\n","| epoch 260/500 | lr 2.00000 | ms/epoch 4261.26528 | train_loss  0.90 | eval_loss  2.41\n","| epoch 261/500 | lr 2.00000 | ms/epoch 4193.15243 | train_loss  0.91 | eval_loss  2.25\n","| epoch 262/500 | lr 2.00000 | ms/epoch 4214.47158 | train_loss  0.92 | eval_loss  2.39\n","| epoch 263/500 | lr 2.00000 | ms/epoch 4247.42723 | train_loss  0.92 | eval_loss  2.32\n","| epoch 264/500 | lr 2.00000 | ms/epoch 4429.23379 | train_loss  0.90 | eval_loss  2.54\n","| epoch 265/500 | lr 2.00000 | ms/epoch 4101.90988 | train_loss  0.90 | eval_loss  2.35\n","| epoch 266/500 | lr 2.00000 | ms/epoch 4336.59935 | train_loss  0.91 | eval_loss  2.35\n","| epoch 267/500 | lr 2.00000 | ms/epoch 4405.62272 | train_loss  0.89 | eval_loss  2.32\n","| epoch 268/500 | lr 2.00000 | ms/epoch 4194.23437 | train_loss  0.90 | eval_loss  2.32\n","| epoch 269/500 | lr 2.00000 | ms/epoch 2347.39852 | train_loss  0.89 | eval_loss  2.37\n","| epoch 270/500 | lr 2.00000 | ms/epoch 2005.47266 | train_loss  0.90 | eval_loss  2.28\n","| epoch 271/500 | lr 2.00000 | ms/epoch 2002.47860 | train_loss  0.91 | eval_loss  2.41\n","| epoch 272/500 | lr 2.00000 | ms/epoch 2019.51957 | train_loss  0.90 | eval_loss  2.31\n","| epoch 273/500 | lr 2.00000 | ms/epoch 2018.75019 | train_loss  0.89 | eval_loss  2.48\n","| epoch 274/500 | lr 2.00000 | ms/epoch 2003.40199 | train_loss  0.89 | eval_loss  2.28\n","| epoch 275/500 | lr 2.00000 | ms/epoch 1995.11194 | train_loss  0.90 | eval_loss  2.30\n","| epoch 276/500 | lr 2.00000 | ms/epoch 1979.88939 | train_loss  0.88 | eval_loss  2.38\n","| epoch 277/500 | lr 2.00000 | ms/epoch 1987.18071 | train_loss  0.89 | eval_loss  2.42\n","| epoch 278/500 | lr 2.00000 | ms/epoch 1971.54784 | train_loss  0.90 | eval_loss  2.35\n","| epoch 279/500 | lr 2.00000 | ms/epoch 2011.15799 | train_loss  0.88 | eval_loss  2.36\n","| epoch 280/500 | lr 2.00000 | ms/epoch 1958.76718 | train_loss  0.89 | eval_loss  2.31\n","| epoch 281/500 | lr 2.00000 | ms/epoch 1987.60891 | train_loss  0.88 | eval_loss  2.36\n","| epoch 282/500 | lr 2.00000 | ms/epoch 1990.50450 | train_loss  0.89 | eval_loss  2.41\n","| epoch 283/500 | lr 2.00000 | ms/epoch 1965.18517 | train_loss  0.88 | eval_loss  2.64\n","| epoch 284/500 | lr 2.00000 | ms/epoch 1980.41654 | train_loss  0.88 | eval_loss  2.43\n","| epoch 285/500 | lr 2.00000 | ms/epoch 3321.32053 | train_loss  0.88 | eval_loss  2.41\n","| epoch 286/500 | lr 2.00000 | ms/epoch 3920.65263 | train_loss  0.89 | eval_loss  2.54\n","| epoch 287/500 | lr 2.00000 | ms/epoch 3959.05399 | train_loss  0.88 | eval_loss  2.27\n","| epoch 288/500 | lr 2.00000 | ms/epoch 4055.88317 | train_loss  0.88 | eval_loss  2.57\n","| epoch 289/500 | lr 2.00000 | ms/epoch 4039.04033 | train_loss  0.92 | eval_loss  2.33\n","| epoch 290/500 | lr 2.00000 | ms/epoch 4222.57805 | train_loss  0.88 | eval_loss  2.42\n","| epoch 291/500 | lr 2.00000 | ms/epoch 4131.52337 | train_loss  0.88 | eval_loss  2.62\n","| epoch 292/500 | lr 2.00000 | ms/epoch 4399.69659 | train_loss  0.87 | eval_loss  2.45\n","| epoch 293/500 | lr 2.00000 | ms/epoch 5615.59987 | train_loss  0.89 | eval_loss  2.55\n","| epoch 294/500 | lr 2.00000 | ms/epoch 5730.41177 | train_loss  0.87 | eval_loss  2.56\n","| epoch 295/500 | lr 2.00000 | ms/epoch 5569.45395 | train_loss  0.88 | eval_loss  2.46\n","| epoch 296/500 | lr 2.00000 | ms/epoch 5536.97252 | train_loss  0.87 | eval_loss  2.35\n","| epoch 297/500 | lr 2.00000 | ms/epoch 5691.86401 | train_loss  0.87 | eval_loss  2.42\n","| epoch 298/500 | lr 2.00000 | ms/epoch 5758.02779 | train_loss  0.87 | eval_loss  2.32\n","| epoch 299/500 | lr 2.00000 | ms/epoch 6087.01992 | train_loss  0.90 | eval_loss  2.28\n","| epoch 300/500 | lr 2.00000 | ms/epoch 6079.45037 | train_loss  0.87 | eval_loss  2.34\n","| epoch 301/500 | lr 2.00000 | ms/epoch 5955.87397 | train_loss  0.86 | eval_loss  2.35\n","| epoch 302/500 | lr 2.00000 | ms/epoch 4769.28353 | train_loss  0.88 | eval_loss  2.46\n","| epoch 303/500 | lr 2.00000 | ms/epoch 4169.23952 | train_loss  0.86 | eval_loss  2.59\n","| epoch 304/500 | lr 2.00000 | ms/epoch 4161.19289 | train_loss  0.86 | eval_loss  2.46\n","| epoch 305/500 | lr 2.00000 | ms/epoch 4142.94839 | train_loss  0.88 | eval_loss  2.46\n","| epoch 306/500 | lr 2.00000 | ms/epoch 4125.80943 | train_loss  0.89 | eval_loss  2.34\n","| epoch 307/500 | lr 2.00000 | ms/epoch 4251.83582 | train_loss  0.86 | eval_loss  2.45\n","| epoch 308/500 | lr 2.00000 | ms/epoch 4208.81581 | train_loss  0.87 | eval_loss  2.38\n","| epoch 309/500 | lr 2.00000 | ms/epoch 2600.95954 | train_loss  0.89 | eval_loss  2.71\n","| epoch 310/500 | lr 2.00000 | ms/epoch 2724.64252 | train_loss  0.86 | eval_loss  2.37\n","| epoch 311/500 | lr 2.00000 | ms/epoch 4225.54994 | train_loss  0.85 | eval_loss  2.65\n","| epoch 312/500 | lr 2.00000 | ms/epoch 4120.21518 | train_loss  0.86 | eval_loss  2.46\n","| epoch 313/500 | lr 2.00000 | ms/epoch 3967.88502 | train_loss  0.89 | eval_loss  2.52\n","| epoch 314/500 | lr 2.00000 | ms/epoch 4075.72627 | train_loss  0.86 | eval_loss  2.51\n","| epoch 315/500 | lr 2.00000 | ms/epoch 4276.88909 | train_loss  0.86 | eval_loss  2.40\n","| epoch 316/500 | lr 2.00000 | ms/epoch 4198.04478 | train_loss  0.86 | eval_loss  2.31\n","| epoch 317/500 | lr 2.00000 | ms/epoch 4191.08558 | train_loss  0.86 | eval_loss  2.52\n","| epoch 318/500 | lr 2.00000 | ms/epoch 4213.38606 | train_loss  0.85 | eval_loss  2.78\n","| epoch 319/500 | lr 2.00000 | ms/epoch 4284.55830 | train_loss  0.85 | eval_loss  2.49\n","| epoch 320/500 | lr 2.00000 | ms/epoch 4224.26414 | train_loss  0.86 | eval_loss  2.32\n","| epoch 321/500 | lr 2.00000 | ms/epoch 4140.87009 | train_loss  0.85 | eval_loss  2.39\n","| epoch 322/500 | lr 2.00000 | ms/epoch 4058.75397 | train_loss  0.86 | eval_loss  2.63\n","| epoch 323/500 | lr 2.00000 | ms/epoch 4184.69882 | train_loss  0.85 | eval_loss  2.34\n","| epoch 324/500 | lr 2.00000 | ms/epoch 4165.62510 | train_loss  0.85 | eval_loss  2.53\n","| epoch 325/500 | lr 2.00000 | ms/epoch 4353.46270 | train_loss  0.85 | eval_loss  2.40\n","| epoch 326/500 | lr 2.00000 | ms/epoch 4218.20498 | train_loss  0.85 | eval_loss  2.56\n","| epoch 327/500 | lr 2.00000 | ms/epoch 2877.08640 | train_loss  0.85 | eval_loss  2.70\n","| epoch 328/500 | lr 2.00000 | ms/epoch 2045.30120 | train_loss  0.85 | eval_loss  2.75\n","| epoch 329/500 | lr 2.00000 | ms/epoch 2000.09084 | train_loss  0.99 | eval_loss  2.44\n","| epoch 330/500 | lr 2.00000 | ms/epoch 2022.42088 | train_loss  0.86 | eval_loss  2.68\n","| epoch 331/500 | lr 2.00000 | ms/epoch 2019.28020 | train_loss  0.86 | eval_loss  2.70\n","| epoch 332/500 | lr 2.00000 | ms/epoch 2011.50513 | train_loss  0.87 | eval_loss  2.43\n","| epoch 333/500 | lr 2.00000 | ms/epoch 2009.77755 | train_loss  0.84 | eval_loss  2.53\n","| epoch 334/500 | lr 2.00000 | ms/epoch 1997.74361 | train_loss  0.85 | eval_loss  2.29\n","| epoch 335/500 | lr 2.00000 | ms/epoch 1993.64424 | train_loss  0.84 | eval_loss  2.28\n","| epoch 336/500 | lr 2.00000 | ms/epoch 2022.86100 | train_loss  0.85 | eval_loss  2.70\n","| epoch 337/500 | lr 2.00000 | ms/epoch 2010.02860 | train_loss  0.84 | eval_loss  2.64\n","| epoch 338/500 | lr 2.00000 | ms/epoch 1986.50169 | train_loss  0.86 | eval_loss  2.79\n","| epoch 339/500 | lr 2.00000 | ms/epoch 2010.08105 | train_loss  0.84 | eval_loss  2.42\n","| epoch 340/500 | lr 2.00000 | ms/epoch 1988.82508 | train_loss  0.86 | eval_loss  2.37\n","| epoch 341/500 | lr 2.00000 | ms/epoch 2023.69475 | train_loss  0.84 | eval_loss  2.54\n","| epoch 342/500 | lr 2.00000 | ms/epoch 2027.77767 | train_loss  0.85 | eval_loss  2.29\n","| epoch 343/500 | lr 2.00000 | ms/epoch 2950.46115 | train_loss  0.84 | eval_loss  2.38\n","| epoch 344/500 | lr 2.00000 | ms/epoch 4041.93544 | train_loss  0.88 | eval_loss  2.43\n","| epoch 345/500 | lr 2.00000 | ms/epoch 4032.30524 | train_loss  0.84 | eval_loss  2.50\n","| epoch 346/500 | lr 2.00000 | ms/epoch 3889.18042 | train_loss  0.84 | eval_loss  2.60\n","| epoch 347/500 | lr 2.00000 | ms/epoch 3995.00322 | train_loss  0.84 | eval_loss  2.46\n","| epoch 348/500 | lr 2.00000 | ms/epoch 4169.99626 | train_loss  0.84 | eval_loss  2.40\n","| epoch 349/500 | lr 2.00000 | ms/epoch 4160.03489 | train_loss  0.86 | eval_loss  2.83\n","| epoch 350/500 | lr 2.00000 | ms/epoch 4404.57463 | train_loss  0.84 | eval_loss  2.70\n","| epoch 351/500 | lr 2.00000 | ms/epoch 4332.09896 | train_loss  0.85 | eval_loss  2.67\n","| epoch 352/500 | lr 2.00000 | ms/epoch 4423.24924 | train_loss  0.84 | eval_loss  2.49\n","| epoch 353/500 | lr 2.00000 | ms/epoch 4315.97948 | train_loss  0.83 | eval_loss  2.57\n","| epoch 354/500 | lr 2.00000 | ms/epoch 4214.73718 | train_loss  0.83 | eval_loss  2.67\n","| epoch 355/500 | lr 2.00000 | ms/epoch 4229.31647 | train_loss  0.84 | eval_loss  2.34\n","| epoch 356/500 | lr 2.00000 | ms/epoch 4388.73863 | train_loss  0.84 | eval_loss  2.73\n","| epoch 357/500 | lr 2.00000 | ms/epoch 4427.91939 | train_loss  0.89 | eval_loss  2.88\n","| epoch 358/500 | lr 2.00000 | ms/epoch 4197.75176 | train_loss  0.84 | eval_loss  2.74\n","| epoch 359/500 | lr 2.00000 | ms/epoch 2141.07633 | train_loss  0.83 | eval_loss  2.70\n","| epoch 360/500 | lr 2.00000 | ms/epoch 2007.72119 | train_loss  0.84 | eval_loss  2.51\n","| epoch 361/500 | lr 2.00000 | ms/epoch 2001.86372 | train_loss  0.83 | eval_loss  2.46\n","| epoch 362/500 | lr 2.00000 | ms/epoch 1999.43733 | train_loss  0.83 | eval_loss  2.75\n","| epoch 363/500 | lr 2.00000 | ms/epoch 2023.13161 | train_loss  0.83 | eval_loss  2.86\n","| epoch 364/500 | lr 2.00000 | ms/epoch 2000.31352 | train_loss  0.83 | eval_loss  2.63\n","| epoch 365/500 | lr 2.00000 | ms/epoch 2025.07615 | train_loss  0.86 | eval_loss  2.48\n","| epoch 366/500 | lr 2.00000 | ms/epoch 1993.74747 | train_loss  0.87 | eval_loss  2.64\n","| epoch 367/500 | lr 2.00000 | ms/epoch 2008.02684 | train_loss  0.83 | eval_loss  2.49\n","| epoch 368/500 | lr 2.00000 | ms/epoch 2021.55828 | train_loss  0.83 | eval_loss  2.58\n","| epoch 369/500 | lr 2.00000 | ms/epoch 2011.95264 | train_loss  0.84 | eval_loss  2.54\n","| epoch 370/500 | lr 2.00000 | ms/epoch 2031.46410 | train_loss  0.84 | eval_loss  2.70\n","| epoch 371/500 | lr 2.00000 | ms/epoch 2054.78382 | train_loss  0.83 | eval_loss  2.58\n","| epoch 372/500 | lr 2.00000 | ms/epoch 2067.00420 | train_loss  0.83 | eval_loss  2.58\n","| epoch 373/500 | lr 2.00000 | ms/epoch 2055.59254 | train_loss  0.83 | eval_loss  2.57\n","| epoch 374/500 | lr 2.00000 | ms/epoch 2001.47152 | train_loss  0.83 | eval_loss  2.53\n","| epoch 375/500 | lr 2.00000 | ms/epoch 4041.37421 | train_loss  0.82 | eval_loss  2.47\n","| epoch 376/500 | lr 2.00000 | ms/epoch 3978.05691 | train_loss  0.83 | eval_loss  2.75\n","| epoch 377/500 | lr 2.00000 | ms/epoch 4037.78768 | train_loss  0.85 | eval_loss  2.83\n","| epoch 378/500 | lr 2.00000 | ms/epoch 3931.81348 | train_loss  0.84 | eval_loss  2.36\n","| epoch 379/500 | lr 2.00000 | ms/epoch 4058.14219 | train_loss  0.82 | eval_loss  2.88\n","| epoch 380/500 | lr 2.00000 | ms/epoch 4266.14380 | train_loss  0.83 | eval_loss  2.94\n","| epoch 381/500 | lr 2.00000 | ms/epoch 4210.44612 | train_loss  0.83 | eval_loss  2.38\n","| epoch 382/500 | lr 2.00000 | ms/epoch 4307.93834 | train_loss  0.82 | eval_loss  2.79\n","| epoch 383/500 | lr 2.00000 | ms/epoch 4158.24461 | train_loss  0.84 | eval_loss  2.62\n","| epoch 384/500 | lr 2.00000 | ms/epoch 4149.55449 | train_loss  0.83 | eval_loss  2.70\n","| epoch 385/500 | lr 2.00000 | ms/epoch 4101.61400 | train_loss  0.82 | eval_loss  2.85\n","| epoch 386/500 | lr 2.00000 | ms/epoch 4056.87475 | train_loss  0.82 | eval_loss  2.72\n","| epoch 387/500 | lr 2.00000 | ms/epoch 4108.94179 | train_loss  0.82 | eval_loss  2.42\n","| epoch 388/500 | lr 2.00000 | ms/epoch 4174.57247 | train_loss  0.82 | eval_loss  2.64\n","| epoch 389/500 | lr 2.00000 | ms/epoch 4190.09066 | train_loss  0.83 | eval_loss  2.70\n","| epoch 390/500 | lr 2.00000 | ms/epoch 3891.48116 | train_loss  0.84 | eval_loss  2.55\n","| epoch 391/500 | lr 2.00000 | ms/epoch 2003.80278 | train_loss  0.82 | eval_loss  2.67\n","| epoch 392/500 | lr 2.00000 | ms/epoch 1980.88288 | train_loss  0.82 | eval_loss  2.59\n","| epoch 393/500 | lr 2.00000 | ms/epoch 1991.57739 | train_loss  0.82 | eval_loss  2.58\n","| epoch 394/500 | lr 2.00000 | ms/epoch 2022.64500 | train_loss  0.83 | eval_loss  2.78\n","| epoch 395/500 | lr 2.00000 | ms/epoch 1988.97004 | train_loss  0.82 | eval_loss  2.40\n","| epoch 396/500 | lr 2.00000 | ms/epoch 2018.92734 | train_loss  0.82 | eval_loss  2.56\n","| epoch 397/500 | lr 2.00000 | ms/epoch 1972.91493 | train_loss  0.82 | eval_loss  2.49\n","| epoch 398/500 | lr 2.00000 | ms/epoch 1995.29171 | train_loss  0.82 | eval_loss  2.72\n","| epoch 399/500 | lr 2.00000 | ms/epoch 1994.19379 | train_loss  0.82 | eval_loss  2.60\n","| epoch 400/500 | lr 2.00000 | ms/epoch 2014.07027 | train_loss  0.84 | eval_loss  2.57\n","| epoch 401/500 | lr 2.00000 | ms/epoch 2018.46099 | train_loss  0.82 | eval_loss  2.57\n","| epoch 402/500 | lr 2.00000 | ms/epoch 1981.03309 | train_loss  0.83 | eval_loss  2.62\n","| epoch 403/500 | lr 2.00000 | ms/epoch 2007.94005 | train_loss  0.84 | eval_loss  2.70\n","| epoch 404/500 | lr 2.00000 | ms/epoch 2005.44500 | train_loss  0.83 | eval_loss  2.24\n","| epoch 405/500 | lr 2.00000 | ms/epoch 1995.61453 | train_loss  0.83 | eval_loss  2.44\n","\n","\n"," TRAINING FINISHED:\n","\n","\tBest Loss:  2.24\tBest Model saved at epoch: 404 \n","\n","\n","\n","\n","TEST LOSS: 1.6471701264381409\n","| epoch   1/500 | lr 2.00000 | ms/epoch 4054.98195 | train_loss  3.46 | eval_loss  3.41\n","| epoch   2/500 | lr 2.00000 | ms/epoch 4051.77879 | train_loss  3.36 | eval_loss  3.48\n","| epoch   3/500 | lr 2.00000 | ms/epoch 4270.43390 | train_loss  3.36 | eval_loss  3.41\n","| epoch   4/500 | lr 2.00000 | ms/epoch 4303.68471 | train_loss  3.35 | eval_loss  3.42\n","| epoch   5/500 | lr 2.00000 | ms/epoch 4444.43274 | train_loss  3.33 | eval_loss  3.40\n","| epoch   6/500 | lr 2.00000 | ms/epoch 4228.76358 | train_loss  3.27 | eval_loss  3.35\n","| epoch   7/500 | lr 2.00000 | ms/epoch 4274.06645 | train_loss  3.23 | eval_loss  3.26\n","| epoch   8/500 | lr 2.00000 | ms/epoch 4351.87697 | train_loss  3.01 | eval_loss  2.91\n","| epoch   9/500 | lr 2.00000 | ms/epoch 4274.37973 | train_loss  2.84 | eval_loss  2.83\n","| epoch  10/500 | lr 2.00000 | ms/epoch 4185.11009 | train_loss  2.78 | eval_loss  2.77\n","| epoch  11/500 | lr 2.00000 | ms/epoch 4166.65840 | train_loss  2.74 | eval_loss  2.84\n","| epoch  12/500 | lr 2.00000 | ms/epoch 4079.28729 | train_loss  2.73 | eval_loss  2.77\n","| epoch  13/500 | lr 2.00000 | ms/epoch 4284.51467 | train_loss  2.70 | eval_loss  2.78\n","| epoch  14/500 | lr 2.00000 | ms/epoch 4414.60967 | train_loss  2.70 | eval_loss  2.82\n","| epoch  15/500 | lr 2.00000 | ms/epoch 4124.97258 | train_loss  2.68 | eval_loss  2.75\n","| epoch  16/500 | lr 2.00000 | ms/epoch 3956.72059 | train_loss  2.67 | eval_loss  2.78\n","| epoch  17/500 | lr 2.00000 | ms/epoch 2069.34237 | train_loss  2.67 | eval_loss  2.78\n","| epoch  18/500 | lr 2.00000 | ms/epoch 2035.32028 | train_loss  2.66 | eval_loss  2.73\n","| epoch  19/500 | lr 2.00000 | ms/epoch 2030.92837 | train_loss  2.66 | eval_loss  2.76\n","| epoch  20/500 | lr 2.00000 | ms/epoch 2043.71595 | train_loss  2.65 | eval_loss  2.73\n","| epoch  21/500 | lr 2.00000 | ms/epoch 2025.07591 | train_loss  2.65 | eval_loss  2.77\n","| epoch  22/500 | lr 2.00000 | ms/epoch 2062.61134 | train_loss  2.65 | eval_loss  2.77\n","| epoch  23/500 | lr 2.00000 | ms/epoch 2017.98224 | train_loss  2.64 | eval_loss  2.78\n","| epoch  24/500 | lr 2.00000 | ms/epoch 2026.42035 | train_loss  2.64 | eval_loss  2.80\n","| epoch  25/500 | lr 2.00000 | ms/epoch 2043.12229 | train_loss  2.64 | eval_loss  2.83\n","| epoch  26/500 | lr 2.00000 | ms/epoch 2022.86315 | train_loss  2.64 | eval_loss  2.77\n","| epoch  27/500 | lr 2.00000 | ms/epoch 2054.13556 | train_loss  2.64 | eval_loss  2.77\n","| epoch  28/500 | lr 2.00000 | ms/epoch 2004.55904 | train_loss  2.63 | eval_loss  2.73\n","| epoch  29/500 | lr 2.00000 | ms/epoch 2012.38441 | train_loss  2.63 | eval_loss  2.75\n","| epoch  30/500 | lr 2.00000 | ms/epoch 2047.21761 | train_loss  2.59 | eval_loss  2.65\n","| epoch  31/500 | lr 2.00000 | ms/epoch 2017.97247 | train_loss  2.55 | eval_loss  2.62\n","| epoch  32/500 | lr 2.00000 | ms/epoch 2756.52504 | train_loss  2.52 | eval_loss  2.64\n","| epoch  33/500 | lr 2.00000 | ms/epoch 4164.01124 | train_loss  2.46 | eval_loss  2.75\n","| epoch  34/500 | lr 2.00000 | ms/epoch 4258.04138 | train_loss  2.38 | eval_loss  2.54\n","| epoch  35/500 | lr 2.00000 | ms/epoch 4076.41816 | train_loss  2.32 | eval_loss  2.62\n","| epoch  36/500 | lr 2.00000 | ms/epoch 4067.95001 | train_loss  2.29 | eval_loss  2.63\n","| epoch  37/500 | lr 2.00000 | ms/epoch 4217.46683 | train_loss  2.25 | eval_loss  2.49\n","| epoch  38/500 | lr 2.00000 | ms/epoch 4166.98050 | train_loss  2.21 | eval_loss  2.57\n","| epoch  39/500 | lr 2.00000 | ms/epoch 4189.61811 | train_loss  2.18 | eval_loss  2.52\n","| epoch  40/500 | lr 2.00000 | ms/epoch 4169.55137 | train_loss  2.14 | eval_loss  2.44\n","| epoch  41/500 | lr 2.00000 | ms/epoch 4241.75596 | train_loss  2.12 | eval_loss  2.53\n","| epoch  42/500 | lr 2.00000 | ms/epoch 4193.66693 | train_loss  2.09 | eval_loss  2.63\n","| epoch  43/500 | lr 2.00000 | ms/epoch 4077.83771 | train_loss  2.06 | eval_loss  2.60\n","| epoch  44/500 | lr 2.00000 | ms/epoch 4225.07858 | train_loss  2.02 | eval_loss  2.57\n","| epoch  45/500 | lr 2.00000 | ms/epoch 4263.11707 | train_loss  2.00 | eval_loss  2.65\n","| epoch  46/500 | lr 2.00000 | ms/epoch 4294.85106 | train_loss  1.96 | eval_loss  2.66\n","| epoch  47/500 | lr 2.00000 | ms/epoch 3889.59599 | train_loss  1.94 | eval_loss  2.62\n","| epoch  48/500 | lr 2.00000 | ms/epoch 1971.62843 | train_loss  1.91 | eval_loss  2.73\n","| epoch  49/500 | lr 2.00000 | ms/epoch 1975.26312 | train_loss  1.89 | eval_loss  2.74\n","| epoch  50/500 | lr 2.00000 | ms/epoch 1964.28013 | train_loss  1.86 | eval_loss  2.74\n","| epoch  51/500 | lr 2.00000 | ms/epoch 1956.87747 | train_loss  1.84 | eval_loss  2.75\n","| epoch  52/500 | lr 2.00000 | ms/epoch 1955.03354 | train_loss  1.81 | eval_loss  2.75\n","| epoch  53/500 | lr 2.00000 | ms/epoch 1959.50437 | train_loss  1.79 | eval_loss  2.67\n","| epoch  54/500 | lr 2.00000 | ms/epoch 1965.90066 | train_loss  1.76 | eval_loss  2.70\n","| epoch  55/500 | lr 2.00000 | ms/epoch 1960.89554 | train_loss  1.76 | eval_loss  2.82\n","| epoch  56/500 | lr 2.00000 | ms/epoch 1958.92739 | train_loss  1.72 | eval_loss  2.70\n","| epoch  57/500 | lr 2.00000 | ms/epoch 1958.75621 | train_loss  1.70 | eval_loss  2.68\n","| epoch  58/500 | lr 2.00000 | ms/epoch 1954.74410 | train_loss  1.69 | eval_loss  2.66\n","| epoch  59/500 | lr 2.00000 | ms/epoch 1969.51962 | train_loss  1.66 | eval_loss  2.73\n","| epoch  60/500 | lr 2.00000 | ms/epoch 1972.44716 | train_loss  1.63 | eval_loss  2.78\n","| epoch  61/500 | lr 2.00000 | ms/epoch 1965.77930 | train_loss  1.62 | eval_loss  2.65\n","| epoch  62/500 | lr 2.00000 | ms/epoch 1956.56371 | train_loss  1.60 | eval_loss  2.65\n","| epoch  63/500 | lr 2.00000 | ms/epoch 1957.84044 | train_loss  1.57 | eval_loss  2.64\n","| epoch  64/500 | lr 2.00000 | ms/epoch 3631.85501 | train_loss  1.55 | eval_loss  2.73\n","| epoch  65/500 | lr 2.00000 | ms/epoch 4039.78419 | train_loss  1.55 | eval_loss  2.65\n","| epoch  66/500 | lr 2.00000 | ms/epoch 3983.59847 | train_loss  1.53 | eval_loss  2.65\n","| epoch  67/500 | lr 2.00000 | ms/epoch 3975.38877 | train_loss  1.52 | eval_loss  2.56\n","| epoch  68/500 | lr 2.00000 | ms/epoch 4166.74328 | train_loss  1.48 | eval_loss  2.54\n","| epoch  69/500 | lr 2.00000 | ms/epoch 4150.82502 | train_loss  1.47 | eval_loss  2.56\n","| epoch  70/500 | lr 2.00000 | ms/epoch 4181.97703 | train_loss  1.44 | eval_loss  2.61\n","| epoch  71/500 | lr 2.00000 | ms/epoch 4201.11895 | train_loss  1.43 | eval_loss  2.53\n","| epoch  72/500 | lr 2.00000 | ms/epoch 4243.32118 | train_loss  1.42 | eval_loss  2.46\n","| epoch  73/500 | lr 2.00000 | ms/epoch 4128.87931 | train_loss  1.41 | eval_loss  2.45\n","| epoch  74/500 | lr 2.00000 | ms/epoch 4119.90142 | train_loss  1.41 | eval_loss  2.48\n","| epoch  75/500 | lr 2.00000 | ms/epoch 4221.37690 | train_loss  1.37 | eval_loss  2.44\n","| epoch  76/500 | lr 2.00000 | ms/epoch 4273.74744 | train_loss  1.38 | eval_loss  2.44\n","| epoch  77/500 | lr 2.00000 | ms/epoch 4226.86982 | train_loss  1.36 | eval_loss  2.51\n","| epoch  78/500 | lr 2.00000 | ms/epoch 4186.24568 | train_loss  1.36 | eval_loss  2.44\n","| epoch  79/500 | lr 2.00000 | ms/epoch 2247.77126 | train_loss  1.33 | eval_loss  2.42\n","| epoch  80/500 | lr 2.00000 | ms/epoch 1951.45416 | train_loss  1.32 | eval_loss  2.45\n","| epoch  81/500 | lr 2.00000 | ms/epoch 1960.17957 | train_loss  1.32 | eval_loss  2.47\n","| epoch  82/500 | lr 2.00000 | ms/epoch 1961.42030 | train_loss  1.31 | eval_loss  2.40\n","| epoch  83/500 | lr 2.00000 | ms/epoch 1946.26355 | train_loss  1.31 | eval_loss  2.40\n","| epoch  84/500 | lr 2.00000 | ms/epoch 1979.38418 | train_loss  1.29 | eval_loss  2.43\n","| epoch  85/500 | lr 2.00000 | ms/epoch 1959.62977 | train_loss  1.29 | eval_loss  2.35\n","| epoch  86/500 | lr 2.00000 | ms/epoch 1956.60567 | train_loss  1.28 | eval_loss  2.35\n","| epoch  87/500 | lr 2.00000 | ms/epoch 1955.16062 | train_loss  1.27 | eval_loss  2.31\n","| epoch  88/500 | lr 2.00000 | ms/epoch 1967.82589 | train_loss  1.26 | eval_loss  2.27\n","| epoch  89/500 | lr 2.00000 | ms/epoch 1964.41936 | train_loss  1.26 | eval_loss  2.25\n","| epoch  90/500 | lr 2.00000 | ms/epoch 1955.27649 | train_loss  1.27 | eval_loss  2.19\n","| epoch  91/500 | lr 2.00000 | ms/epoch 1954.84042 | train_loss  1.24 | eval_loss  2.40\n","| epoch  92/500 | lr 2.00000 | ms/epoch 1963.38940 | train_loss  1.23 | eval_loss  2.27\n","| epoch  93/500 | lr 2.00000 | ms/epoch 1960.65140 | train_loss  1.23 | eval_loss  2.20\n","| epoch  94/500 | lr 2.00000 | ms/epoch 1964.80632 | train_loss  1.22 | eval_loss  2.20\n","| epoch  95/500 | lr 2.00000 | ms/epoch 2783.63490 | train_loss  1.21 | eval_loss  2.17\n","| epoch  96/500 | lr 2.00000 | ms/epoch 4114.47811 | train_loss  1.21 | eval_loss  2.33\n","| epoch  97/500 | lr 2.00000 | ms/epoch 4047.07003 | train_loss  1.21 | eval_loss  2.21\n","| epoch  98/500 | lr 2.00000 | ms/epoch 3984.97105 | train_loss  1.20 | eval_loss  2.36\n","| epoch  99/500 | lr 2.00000 | ms/epoch 4144.13714 | train_loss  1.21 | eval_loss  2.33\n","| epoch 100/500 | lr 2.00000 | ms/epoch 4205.13964 | train_loss  1.20 | eval_loss  2.30\n","| epoch 101/500 | lr 2.00000 | ms/epoch 4214.70714 | train_loss  1.18 | eval_loss  2.28\n","| epoch 102/500 | lr 2.00000 | ms/epoch 4205.04785 | train_loss  1.18 | eval_loss  2.29\n","| epoch 103/500 | lr 2.00000 | ms/epoch 4193.03346 | train_loss  1.17 | eval_loss  2.18\n","| epoch 104/500 | lr 2.00000 | ms/epoch 4181.56624 | train_loss  1.18 | eval_loss  2.14\n","| epoch 105/500 | lr 2.00000 | ms/epoch 4064.67485 | train_loss  1.18 | eval_loss  2.32\n","| epoch 106/500 | lr 2.00000 | ms/epoch 4035.78067 | train_loss  1.16 | eval_loss  2.09\n","| epoch 107/500 | lr 2.00000 | ms/epoch 4196.43188 | train_loss  1.16 | eval_loss  2.19\n","| epoch 108/500 | lr 2.00000 | ms/epoch 4277.89116 | train_loss  1.16 | eval_loss  2.12\n","| epoch 109/500 | lr 2.00000 | ms/epoch 4255.54299 | train_loss  1.15 | eval_loss  2.22\n","| epoch 110/500 | lr 2.00000 | ms/epoch 1984.56049 | train_loss  1.14 | eval_loss  2.33\n","| epoch 111/500 | lr 2.00000 | ms/epoch 1966.88819 | train_loss  1.14 | eval_loss  2.26\n","| epoch 112/500 | lr 2.00000 | ms/epoch 1974.58267 | train_loss  1.14 | eval_loss  2.21\n","| epoch 113/500 | lr 2.00000 | ms/epoch 2004.35305 | train_loss  1.14 | eval_loss  2.27\n","| epoch 114/500 | lr 2.00000 | ms/epoch 2002.73204 | train_loss  1.13 | eval_loss  2.30\n","| epoch 115/500 | lr 2.00000 | ms/epoch 1968.41836 | train_loss  1.13 | eval_loss  2.16\n","| epoch 116/500 | lr 2.00000 | ms/epoch 1948.42339 | train_loss  1.13 | eval_loss  2.25\n","| epoch 117/500 | lr 2.00000 | ms/epoch 1969.57731 | train_loss  1.12 | eval_loss  2.15\n","| epoch 118/500 | lr 2.00000 | ms/epoch 1963.95707 | train_loss  1.12 | eval_loss  2.21\n","| epoch 119/500 | lr 2.00000 | ms/epoch 1995.63789 | train_loss  1.12 | eval_loss  2.13\n","| epoch 120/500 | lr 2.00000 | ms/epoch 1986.85074 | train_loss  1.12 | eval_loss  2.18\n","| epoch 121/500 | lr 2.00000 | ms/epoch 1985.09169 | train_loss  1.11 | eval_loss  2.21\n","| epoch 122/500 | lr 2.00000 | ms/epoch 1962.77785 | train_loss  1.12 | eval_loss  2.28\n","| epoch 123/500 | lr 2.00000 | ms/epoch 1982.09286 | train_loss  1.10 | eval_loss  2.19\n","| epoch 124/500 | lr 2.00000 | ms/epoch 1994.09699 | train_loss  1.10 | eval_loss  2.23\n","| epoch 125/500 | lr 2.00000 | ms/epoch 2005.59592 | train_loss  1.10 | eval_loss  2.12\n","| epoch 126/500 | lr 2.00000 | ms/epoch 3278.78976 | train_loss  1.10 | eval_loss  2.19\n","| epoch 127/500 | lr 2.00000 | ms/epoch 4150.32578 | train_loss  1.10 | eval_loss  2.21\n","| epoch 128/500 | lr 2.00000 | ms/epoch 4126.66178 | train_loss  1.09 | eval_loss  2.20\n","| epoch 129/500 | lr 2.00000 | ms/epoch 4023.57554 | train_loss  1.09 | eval_loss  2.21\n","| epoch 130/500 | lr 2.00000 | ms/epoch 4243.66951 | train_loss  1.11 | eval_loss  2.26\n","| epoch 131/500 | lr 2.00000 | ms/epoch 4220.00957 | train_loss  1.10 | eval_loss  2.13\n","| epoch 132/500 | lr 2.00000 | ms/epoch 4209.27167 | train_loss  1.09 | eval_loss  2.25\n","| epoch 133/500 | lr 2.00000 | ms/epoch 4198.24576 | train_loss  1.10 | eval_loss  2.17\n","| epoch 134/500 | lr 2.00000 | ms/epoch 4242.24544 | train_loss  1.08 | eval_loss  2.24\n","| epoch 135/500 | lr 2.00000 | ms/epoch 4155.21193 | train_loss  1.09 | eval_loss  2.21\n","| epoch 136/500 | lr 2.00000 | ms/epoch 4123.63982 | train_loss  1.07 | eval_loss  2.28\n","| epoch 137/500 | lr 2.00000 | ms/epoch 4092.05532 | train_loss  1.09 | eval_loss  2.23\n","| epoch 138/500 | lr 2.00000 | ms/epoch 4261.77096 | train_loss  1.07 | eval_loss  2.20\n","| epoch 139/500 | lr 2.00000 | ms/epoch 4253.06177 | train_loss  1.07 | eval_loss  2.09\n","| epoch 140/500 | lr 2.00000 | ms/epoch 3285.73132 | train_loss  1.07 | eval_loss  2.12\n","| epoch 141/500 | lr 2.00000 | ms/epoch 1954.62704 | train_loss  1.07 | eval_loss  2.04\n","| epoch 142/500 | lr 2.00000 | ms/epoch 1922.22428 | train_loss  1.08 | eval_loss  2.14\n","| epoch 143/500 | lr 2.00000 | ms/epoch 1941.83993 | train_loss  1.06 | eval_loss  2.23\n","| epoch 144/500 | lr 2.00000 | ms/epoch 1964.74648 | train_loss  1.07 | eval_loss  2.15\n","| epoch 145/500 | lr 2.00000 | ms/epoch 1977.26464 | train_loss  1.06 | eval_loss  2.17\n","| epoch 146/500 | lr 2.00000 | ms/epoch 1972.80121 | train_loss  1.07 | eval_loss  2.16\n","| epoch 147/500 | lr 2.00000 | ms/epoch 1963.82427 | train_loss  1.06 | eval_loss  2.17\n","| epoch 148/500 | lr 2.00000 | ms/epoch 1964.85233 | train_loss  1.06 | eval_loss  2.10\n","| epoch 149/500 | lr 2.00000 | ms/epoch 1993.24489 | train_loss  1.05 | eval_loss  2.12\n","| epoch 150/500 | lr 2.00000 | ms/epoch 1975.87013 | train_loss  1.05 | eval_loss  2.17\n","| epoch 151/500 | lr 2.00000 | ms/epoch 1962.83317 | train_loss  1.08 | eval_loss  2.08\n","| epoch 152/500 | lr 2.00000 | ms/epoch 1970.22986 | train_loss  1.06 | eval_loss  2.17\n","| epoch 153/500 | lr 2.00000 | ms/epoch 1974.33138 | train_loss  1.04 | eval_loss  2.10\n","| epoch 154/500 | lr 2.00000 | ms/epoch 1961.79533 | train_loss  1.04 | eval_loss  2.09\n","| epoch 155/500 | lr 2.00000 | ms/epoch 1987.15806 | train_loss  1.04 | eval_loss  2.17\n","| epoch 156/500 | lr 2.00000 | ms/epoch 1991.88924 | train_loss  1.04 | eval_loss  2.14\n","| epoch 157/500 | lr 2.00000 | ms/epoch 3961.88617 | train_loss  1.05 | eval_loss  2.11\n","| epoch 158/500 | lr 2.00000 | ms/epoch 3970.02530 | train_loss  1.04 | eval_loss  2.14\n","| epoch 159/500 | lr 2.00000 | ms/epoch 3962.03136 | train_loss  1.06 | eval_loss  2.08\n","| epoch 160/500 | lr 2.00000 | ms/epoch 3969.18631 | train_loss  1.04 | eval_loss  2.12\n","| epoch 161/500 | lr 2.00000 | ms/epoch 4154.94561 | train_loss  1.05 | eval_loss  2.13\n","| epoch 162/500 | lr 2.00000 | ms/epoch 4104.34031 | train_loss  1.04 | eval_loss  2.06\n","| epoch 163/500 | lr 2.00000 | ms/epoch 4174.40009 | train_loss  1.05 | eval_loss  2.13\n","| epoch 164/500 | lr 2.00000 | ms/epoch 4180.37820 | train_loss  1.03 | eval_loss  2.13\n","| epoch 165/500 | lr 2.00000 | ms/epoch 4328.49550 | train_loss  1.02 | eval_loss  2.14\n","| epoch 166/500 | lr 2.00000 | ms/epoch 4071.41304 | train_loss  1.02 | eval_loss  2.15\n","| epoch 167/500 | lr 2.00000 | ms/epoch 4018.15987 | train_loss  1.02 | eval_loss  2.06\n","| epoch 168/500 | lr 2.00000 | ms/epoch 4079.39649 | train_loss  1.02 | eval_loss  2.20\n","| epoch 169/500 | lr 2.00000 | ms/epoch 4164.97731 | train_loss  1.03 | eval_loss  2.06\n","| epoch 170/500 | lr 2.00000 | ms/epoch 4213.24515 | train_loss  1.02 | eval_loss  2.16\n","| epoch 171/500 | lr 2.00000 | ms/epoch 2092.62204 | train_loss  1.02 | eval_loss  2.07\n","| epoch 172/500 | lr 2.00000 | ms/epoch 1920.66216 | train_loss  1.02 | eval_loss  2.10\n","| epoch 173/500 | lr 2.00000 | ms/epoch 1943.30716 | train_loss  1.02 | eval_loss  2.13\n","| epoch 174/500 | lr 2.00000 | ms/epoch 1926.06473 | train_loss  1.02 | eval_loss  2.12\n","| epoch 175/500 | lr 2.00000 | ms/epoch 1937.00171 | train_loss  1.04 | eval_loss  2.14\n","| epoch 176/500 | lr 2.00000 | ms/epoch 1936.19847 | train_loss  1.01 | eval_loss  2.08\n","| epoch 177/500 | lr 2.00000 | ms/epoch 1938.21144 | train_loss  1.01 | eval_loss  2.15\n","| epoch 178/500 | lr 2.00000 | ms/epoch 1935.17756 | train_loss  1.03 | eval_loss  2.17\n","| epoch 179/500 | lr 2.00000 | ms/epoch 1932.77884 | train_loss  1.01 | eval_loss  2.00\n","| epoch 180/500 | lr 2.00000 | ms/epoch 1961.26223 | train_loss  1.01 | eval_loss  2.06\n","| epoch 181/500 | lr 2.00000 | ms/epoch 1952.70920 | train_loss  1.02 | eval_loss  2.18\n","| epoch 182/500 | lr 2.00000 | ms/epoch 1922.42289 | train_loss  1.01 | eval_loss  2.26\n","| epoch 183/500 | lr 2.00000 | ms/epoch 1942.43550 | train_loss  1.00 | eval_loss  2.16\n","| epoch 184/500 | lr 2.00000 | ms/epoch 1922.97578 | train_loss  1.00 | eval_loss  2.03\n","| epoch 185/500 | lr 2.00000 | ms/epoch 1934.49593 | train_loss  1.01 | eval_loss  2.12\n","| epoch 186/500 | lr 2.00000 | ms/epoch 1957.57556 | train_loss  1.00 | eval_loss  2.17\n","| epoch 187/500 | lr 2.00000 | ms/epoch 2770.36953 | train_loss  1.00 | eval_loss  2.13\n","| epoch 188/500 | lr 2.00000 | ms/epoch 4003.82733 | train_loss  1.00 | eval_loss  2.21\n","| epoch 189/500 | lr 2.00000 | ms/epoch 4123.57450 | train_loss  1.00 | eval_loss  2.07\n","| epoch 190/500 | lr 2.00000 | ms/epoch 4033.53000 | train_loss  1.00 | eval_loss  2.05\n","| epoch 191/500 | lr 2.00000 | ms/epoch 4136.61218 | train_loss  1.01 | eval_loss  2.14\n","| epoch 192/500 | lr 2.00000 | ms/epoch 4185.17590 | train_loss  1.02 | eval_loss  2.21\n","| epoch 193/500 | lr 2.00000 | ms/epoch 4208.98890 | train_loss  1.02 | eval_loss  2.11\n","| epoch 194/500 | lr 2.00000 | ms/epoch 4230.12543 | train_loss  1.00 | eval_loss  2.15\n","| epoch 195/500 | lr 2.00000 | ms/epoch 4259.88936 | train_loss  0.99 | eval_loss  2.12\n","| epoch 196/500 | lr 2.00000 | ms/epoch 4202.37112 | train_loss  0.99 | eval_loss  2.10\n","| epoch 197/500 | lr 2.00000 | ms/epoch 4198.91548 | train_loss  0.99 | eval_loss  2.08\n","| epoch 198/500 | lr 2.00000 | ms/epoch 4102.74029 | train_loss  0.99 | eval_loss  2.22\n","| epoch 199/500 | lr 2.00000 | ms/epoch 4210.60610 | train_loss  1.02 | eval_loss  2.18\n","| epoch 200/500 | lr 2.00000 | ms/epoch 4168.42794 | train_loss  1.01 | eval_loss  2.08\n","| epoch 201/500 | lr 2.00000 | ms/epoch 2359.40504 | train_loss  0.99 | eval_loss  2.13\n","| epoch 202/500 | lr 2.00000 | ms/epoch 1972.35346 | train_loss  0.98 | eval_loss  2.16\n","| epoch 203/500 | lr 2.00000 | ms/epoch 1983.75940 | train_loss  0.98 | eval_loss  2.13\n","| epoch 204/500 | lr 2.00000 | ms/epoch 1972.43428 | train_loss  0.99 | eval_loss  2.06\n","| epoch 205/500 | lr 2.00000 | ms/epoch 1973.61207 | train_loss  0.99 | eval_loss  2.06\n","| epoch 206/500 | lr 2.00000 | ms/epoch 1967.00406 | train_loss  1.00 | eval_loss  2.12\n","| epoch 207/500 | lr 2.00000 | ms/epoch 2058.84743 | train_loss  0.98 | eval_loss  2.06\n","| epoch 208/500 | lr 2.00000 | ms/epoch 2035.84623 | train_loss  0.98 | eval_loss  2.17\n","| epoch 209/500 | lr 2.00000 | ms/epoch 2027.65584 | train_loss  1.00 | eval_loss  2.11\n","| epoch 210/500 | lr 2.00000 | ms/epoch 2034.26981 | train_loss  0.99 | eval_loss  2.15\n","| epoch 211/500 | lr 2.00000 | ms/epoch 2037.92143 | train_loss  0.98 | eval_loss  2.09\n","| epoch 212/500 | lr 2.00000 | ms/epoch 2072.95871 | train_loss  0.97 | eval_loss  2.13\n","| epoch 213/500 | lr 2.00000 | ms/epoch 2055.83096 | train_loss  0.97 | eval_loss  2.09\n","| epoch 214/500 | lr 2.00000 | ms/epoch 2023.65232 | train_loss  0.97 | eval_loss  2.11\n","| epoch 215/500 | lr 2.00000 | ms/epoch 2013.94701 | train_loss  0.97 | eval_loss  2.10\n","| epoch 216/500 | lr 2.00000 | ms/epoch 2006.96135 | train_loss  0.97 | eval_loss  2.19\n","| epoch 217/500 | lr 2.00000 | ms/epoch 4000.29945 | train_loss  0.97 | eval_loss  2.16\n","| epoch 218/500 | lr 2.00000 | ms/epoch 4064.26358 | train_loss  0.97 | eval_loss  2.09\n","| epoch 219/500 | lr 2.00000 | ms/epoch 4082.46803 | train_loss  0.97 | eval_loss  2.17\n","| epoch 220/500 | lr 2.00000 | ms/epoch 3996.83738 | train_loss  0.97 | eval_loss  2.18\n","| epoch 221/500 | lr 2.00000 | ms/epoch 4342.81158 | train_loss  0.97 | eval_loss  2.23\n","| epoch 222/500 | lr 2.00000 | ms/epoch 4145.26868 | train_loss  0.97 | eval_loss  2.13\n","| epoch 223/500 | lr 2.00000 | ms/epoch 4150.25282 | train_loss  0.97 | eval_loss  2.14\n","| epoch 224/500 | lr 2.00000 | ms/epoch 4168.86830 | train_loss  0.98 | eval_loss  2.08\n","| epoch 225/500 | lr 2.00000 | ms/epoch 4156.78716 | train_loss  0.97 | eval_loss  2.09\n","| epoch 226/500 | lr 2.00000 | ms/epoch 4235.75020 | train_loss  0.96 | eval_loss  2.12\n","| epoch 227/500 | lr 2.00000 | ms/epoch 4165.50922 | train_loss  0.97 | eval_loss  2.10\n","| epoch 228/500 | lr 2.00000 | ms/epoch 4065.11021 | train_loss  0.96 | eval_loss  2.03\n","| epoch 229/500 | lr 2.00000 | ms/epoch 4459.67436 | train_loss  0.96 | eval_loss  2.16\n","| epoch 230/500 | lr 2.00000 | ms/epoch 4259.56750 | train_loss  0.96 | eval_loss  2.10\n","| epoch 231/500 | lr 2.00000 | ms/epoch 4311.49578 | train_loss  0.99 | eval_loss  2.15\n","| epoch 232/500 | lr 2.00000 | ms/epoch 2520.23029 | train_loss  0.96 | eval_loss  2.03\n","| epoch 233/500 | lr 2.00000 | ms/epoch 2050.86470 | train_loss  0.96 | eval_loss  2.26\n","| epoch 234/500 | lr 2.00000 | ms/epoch 2017.77887 | train_loss  0.97 | eval_loss  2.13\n","| epoch 235/500 | lr 2.00000 | ms/epoch 2061.10549 | train_loss  0.96 | eval_loss  2.15\n","| epoch 236/500 | lr 2.00000 | ms/epoch 2035.12120 | train_loss  0.96 | eval_loss  2.09\n","| epoch 237/500 | lr 2.00000 | ms/epoch 1995.86153 | train_loss  0.96 | eval_loss  2.04\n","| epoch 238/500 | lr 2.00000 | ms/epoch 2003.64089 | train_loss  0.98 | eval_loss  2.17\n","| epoch 239/500 | lr 2.00000 | ms/epoch 2067.91496 | train_loss  0.95 | eval_loss  2.21\n","| epoch 240/500 | lr 2.00000 | ms/epoch 2046.89431 | train_loss  0.95 | eval_loss  2.20\n","| epoch 241/500 | lr 2.00000 | ms/epoch 2015.51723 | train_loss  0.95 | eval_loss  2.13\n","| epoch 242/500 | lr 2.00000 | ms/epoch 2017.43555 | train_loss  0.96 | eval_loss  2.22\n","| epoch 243/500 | lr 2.00000 | ms/epoch 2003.56102 | train_loss  0.95 | eval_loss  2.09\n","| epoch 244/500 | lr 2.00000 | ms/epoch 2022.79496 | train_loss  0.95 | eval_loss  2.11\n","| epoch 245/500 | lr 2.00000 | ms/epoch 2035.04014 | train_loss  0.95 | eval_loss  2.16\n","| epoch 246/500 | lr 2.00000 | ms/epoch 2056.00095 | train_loss  0.95 | eval_loss  2.23\n","| epoch 247/500 | lr 2.00000 | ms/epoch 2011.80792 | train_loss  0.95 | eval_loss  2.10\n","| epoch 248/500 | lr 2.00000 | ms/epoch 3576.97725 | train_loss  0.95 | eval_loss  2.12\n","| epoch 249/500 | lr 2.00000 | ms/epoch 4059.04412 | train_loss  0.95 | eval_loss  2.17\n","| epoch 250/500 | lr 2.00000 | ms/epoch 3949.06950 | train_loss  0.95 | eval_loss  2.26\n","| epoch 251/500 | lr 2.00000 | ms/epoch 3893.57162 | train_loss  0.95 | eval_loss  2.12\n","| epoch 252/500 | lr 2.00000 | ms/epoch 4042.21034 | train_loss  0.95 | eval_loss  2.12\n","| epoch 253/500 | lr 2.00000 | ms/epoch 4152.94957 | train_loss  0.94 | eval_loss  2.15\n","| epoch 254/500 | lr 2.00000 | ms/epoch 4198.09723 | train_loss  0.94 | eval_loss  2.11\n","| epoch 255/500 | lr 2.00000 | ms/epoch 4209.16200 | train_loss  0.94 | eval_loss  2.14\n","| epoch 256/500 | lr 2.00000 | ms/epoch 4160.18558 | train_loss  0.94 | eval_loss  2.10\n","| epoch 257/500 | lr 2.00000 | ms/epoch 4228.79529 | train_loss  0.94 | eval_loss  2.19\n","| epoch 258/500 | lr 2.00000 | ms/epoch 4106.24957 | train_loss  0.95 | eval_loss  2.21\n","| epoch 259/500 | lr 2.00000 | ms/epoch 4028.61691 | train_loss  0.94 | eval_loss  2.14\n","| epoch 260/500 | lr 2.00000 | ms/epoch 4080.11198 | train_loss  0.94 | eval_loss  2.07\n","| epoch 261/500 | lr 2.00000 | ms/epoch 4252.58708 | train_loss  0.97 | eval_loss  2.27\n","| epoch 262/500 | lr 2.00000 | ms/epoch 4172.50609 | train_loss  0.94 | eval_loss  2.14\n","| epoch 263/500 | lr 2.00000 | ms/epoch 3594.02704 | train_loss  0.94 | eval_loss  2.09\n","| epoch 264/500 | lr 2.00000 | ms/epoch 2042.93013 | train_loss  0.94 | eval_loss  2.16\n","| epoch 265/500 | lr 2.00000 | ms/epoch 2027.15993 | train_loss  0.93 | eval_loss  2.17\n","| epoch 266/500 | lr 2.00000 | ms/epoch 1994.97437 | train_loss  0.95 | eval_loss  2.25\n","| epoch 267/500 | lr 2.00000 | ms/epoch 2015.13958 | train_loss  0.94 | eval_loss  2.10\n","| epoch 268/500 | lr 2.00000 | ms/epoch 2018.42976 | train_loss  0.93 | eval_loss  2.06\n","| epoch 269/500 | lr 2.00000 | ms/epoch 2050.24171 | train_loss  0.95 | eval_loss  2.15\n","| epoch 270/500 | lr 2.00000 | ms/epoch 1998.46220 | train_loss  0.94 | eval_loss  2.14\n","| epoch 271/500 | lr 2.00000 | ms/epoch 2031.93760 | train_loss  0.93 | eval_loss  2.19\n","| epoch 272/500 | lr 2.00000 | ms/epoch 2067.42573 | train_loss  0.93 | eval_loss  2.14\n","| epoch 273/500 | lr 2.00000 | ms/epoch 2001.46604 | train_loss  0.93 | eval_loss  2.27\n","| epoch 274/500 | lr 2.00000 | ms/epoch 2085.11090 | train_loss  0.93 | eval_loss  2.14\n","| epoch 275/500 | lr 2.00000 | ms/epoch 2035.40397 | train_loss  0.93 | eval_loss  2.22\n","| epoch 276/500 | lr 2.00000 | ms/epoch 2009.78279 | train_loss  0.94 | eval_loss  2.17\n","| epoch 277/500 | lr 2.00000 | ms/epoch 2034.41715 | train_loss  0.93 | eval_loss  2.19\n","| epoch 278/500 | lr 2.00000 | ms/epoch 2026.52884 | train_loss  0.93 | eval_loss  2.24\n","| epoch 279/500 | lr 2.00000 | ms/epoch 2571.74063 | train_loss  0.93 | eval_loss  2.13\n","| epoch 280/500 | lr 2.00000 | ms/epoch 4096.00210 | train_loss  0.93 | eval_loss  2.12\n","| epoch 281/500 | lr 2.00000 | ms/epoch 4026.63708 | train_loss  0.93 | eval_loss  2.28\n","| epoch 282/500 | lr 2.00000 | ms/epoch 3935.50229 | train_loss  0.93 | eval_loss  2.23\n","| epoch 283/500 | lr 2.00000 | ms/epoch 3962.59260 | train_loss  0.93 | eval_loss  2.10\n","| epoch 284/500 | lr 2.00000 | ms/epoch 4421.63634 | train_loss  0.94 | eval_loss  2.19\n","| epoch 285/500 | lr 2.00000 | ms/epoch 4248.04235 | train_loss  0.93 | eval_loss  2.10\n","| epoch 286/500 | lr 2.00000 | ms/epoch 4271.56043 | train_loss  0.93 | eval_loss  2.25\n","| epoch 287/500 | lr 2.00000 | ms/epoch 4301.55087 | train_loss  0.92 | eval_loss  2.38\n","| epoch 288/500 | lr 2.00000 | ms/epoch 4356.17304 | train_loss  0.93 | eval_loss  2.16\n","| epoch 289/500 | lr 2.00000 | ms/epoch 4413.04255 | train_loss  0.92 | eval_loss  2.07\n","| epoch 290/500 | lr 2.00000 | ms/epoch 4160.12549 | train_loss  0.92 | eval_loss  2.19\n","| epoch 291/500 | lr 2.00000 | ms/epoch 4131.03366 | train_loss  0.93 | eval_loss  2.15\n","| epoch 292/500 | lr 2.00000 | ms/epoch 4141.52932 | train_loss  0.92 | eval_loss  2.25\n","| epoch 293/500 | lr 2.00000 | ms/epoch 4296.58699 | train_loss  0.92 | eval_loss  2.14\n","| epoch 294/500 | lr 2.00000 | ms/epoch 4298.64359 | train_loss  0.92 | eval_loss  2.31\n","| epoch 295/500 | lr 2.00000 | ms/epoch 2780.55477 | train_loss  0.93 | eval_loss  2.32\n","| epoch 296/500 | lr 2.00000 | ms/epoch 2016.96944 | train_loss  0.92 | eval_loss  2.26\n","| epoch 297/500 | lr 2.00000 | ms/epoch 2028.43404 | train_loss  0.92 | eval_loss  2.43\n","| epoch 298/500 | lr 2.00000 | ms/epoch 2026.27778 | train_loss  0.92 | eval_loss  2.17\n","| epoch 299/500 | lr 2.00000 | ms/epoch 2020.32351 | train_loss  0.92 | eval_loss  2.13\n","| epoch 300/500 | lr 2.00000 | ms/epoch 2021.05021 | train_loss  0.92 | eval_loss  2.09\n","| epoch 301/500 | lr 2.00000 | ms/epoch 2033.76913 | train_loss  0.92 | eval_loss  2.19\n","| epoch 302/500 | lr 2.00000 | ms/epoch 2073.00282 | train_loss  0.92 | eval_loss  2.26\n","| epoch 303/500 | lr 2.00000 | ms/epoch 1994.09175 | train_loss  0.92 | eval_loss  2.10\n","| epoch 304/500 | lr 2.00000 | ms/epoch 2037.03165 | train_loss  0.92 | eval_loss  2.16\n","| epoch 305/500 | lr 2.00000 | ms/epoch 2077.92878 | train_loss  0.92 | eval_loss  2.29\n","| epoch 306/500 | lr 2.00000 | ms/epoch 2009.62830 | train_loss  0.94 | eval_loss  2.11\n","| epoch 307/500 | lr 2.00000 | ms/epoch 2010.89597 | train_loss  0.92 | eval_loss  2.22\n","| epoch 308/500 | lr 2.00000 | ms/epoch 1989.42995 | train_loss  0.92 | eval_loss  2.39\n","| epoch 309/500 | lr 2.00000 | ms/epoch 2027.40407 | train_loss  0.92 | eval_loss  2.18\n","| epoch 310/500 | lr 2.00000 | ms/epoch 2020.83492 | train_loss  0.92 | eval_loss  2.47\n","| epoch 311/500 | lr 2.00000 | ms/epoch 3446.54512 | train_loss  0.92 | eval_loss  2.34\n","| epoch 312/500 | lr 2.00000 | ms/epoch 4116.67323 | train_loss  0.92 | eval_loss  2.28\n","| epoch 313/500 | lr 2.00000 | ms/epoch 3978.01018 | train_loss  0.92 | eval_loss  2.31\n","| epoch 314/500 | lr 2.00000 | ms/epoch 3969.60115 | train_loss  0.91 | eval_loss  2.20\n","| epoch 315/500 | lr 2.00000 | ms/epoch 4139.04476 | train_loss  0.92 | eval_loss  2.15\n","| epoch 316/500 | lr 2.00000 | ms/epoch 4299.98231 | train_loss  0.94 | eval_loss  2.10\n","| epoch 317/500 | lr 2.00000 | ms/epoch 4078.53913 | train_loss  0.93 | eval_loss  2.25\n","| epoch 318/500 | lr 2.00000 | ms/epoch 4324.56517 | train_loss  0.91 | eval_loss  2.22\n","| epoch 319/500 | lr 2.00000 | ms/epoch 4278.20635 | train_loss  0.91 | eval_loss  2.19\n","| epoch 320/500 | lr 2.00000 | ms/epoch 4351.27997 | train_loss  0.92 | eval_loss  2.22\n","| epoch 321/500 | lr 2.00000 | ms/epoch 4311.56754 | train_loss  0.91 | eval_loss  2.33\n","| epoch 322/500 | lr 2.00000 | ms/epoch 4173.38181 | train_loss  0.91 | eval_loss  2.16\n","| epoch 323/500 | lr 2.00000 | ms/epoch 4348.53530 | train_loss  0.91 | eval_loss  2.19\n","| epoch 324/500 | lr 2.00000 | ms/epoch 4365.98778 | train_loss  0.91 | eval_loss  2.22\n","| epoch 325/500 | lr 2.00000 | ms/epoch 4325.82831 | train_loss  0.91 | eval_loss  2.25\n","| epoch 326/500 | lr 2.00000 | ms/epoch 4141.28494 | train_loss  0.91 | eval_loss  2.17\n","| epoch 327/500 | lr 2.00000 | ms/epoch 2053.98011 | train_loss  0.91 | eval_loss  2.22\n","| epoch 328/500 | lr 2.00000 | ms/epoch 2090.02757 | train_loss  0.91 | eval_loss  2.18\n","| epoch 329/500 | lr 2.00000 | ms/epoch 2067.66772 | train_loss  0.93 | eval_loss  2.27\n","| epoch 330/500 | lr 2.00000 | ms/epoch 2070.93024 | train_loss  0.91 | eval_loss  2.13\n","| epoch 331/500 | lr 2.00000 | ms/epoch 2066.88261 | train_loss  0.91 | eval_loss  2.45\n","| epoch 332/500 | lr 2.00000 | ms/epoch 2082.61991 | train_loss  0.91 | eval_loss  2.14\n","| epoch 333/500 | lr 2.00000 | ms/epoch 2065.20200 | train_loss  0.91 | eval_loss  2.18\n","| epoch 334/500 | lr 2.00000 | ms/epoch 2059.52597 | train_loss  0.92 | eval_loss  2.21\n","| epoch 335/500 | lr 2.00000 | ms/epoch 2027.52376 | train_loss  0.92 | eval_loss  2.17\n","| epoch 336/500 | lr 2.00000 | ms/epoch 2051.76544 | train_loss  0.92 | eval_loss  2.16\n","| epoch 337/500 | lr 2.00000 | ms/epoch 2038.80715 | train_loss  0.92 | eval_loss  2.13\n","| epoch 338/500 | lr 2.00000 | ms/epoch 1999.07112 | train_loss  0.92 | eval_loss  2.14\n","| epoch 339/500 | lr 2.00000 | ms/epoch 2016.70551 | train_loss  0.91 | eval_loss  2.27\n","| epoch 340/500 | lr 2.00000 | ms/epoch 2028.41163 | train_loss  0.90 | eval_loss  2.22\n","| epoch 341/500 | lr 2.00000 | ms/epoch 2040.12084 | train_loss  0.91 | eval_loss  2.35\n","| epoch 342/500 | lr 2.00000 | ms/epoch 2705.55973 | train_loss  0.93 | eval_loss  2.23\n","| epoch 343/500 | lr 2.00000 | ms/epoch 4214.49614 | train_loss  0.92 | eval_loss  2.30\n","| epoch 344/500 | lr 2.00000 | ms/epoch 4126.86920 | train_loss  0.90 | eval_loss  2.21\n","| epoch 345/500 | lr 2.00000 | ms/epoch 4029.15716 | train_loss  0.91 | eval_loss  2.21\n","| epoch 346/500 | lr 2.00000 | ms/epoch 4108.62112 | train_loss  0.90 | eval_loss  2.18\n","| epoch 347/500 | lr 2.00000 | ms/epoch 4266.36672 | train_loss  0.90 | eval_loss  2.18\n","| epoch 348/500 | lr 2.00000 | ms/epoch 4249.86982 | train_loss  0.90 | eval_loss  2.19\n","| epoch 349/500 | lr 2.00000 | ms/epoch 4238.64317 | train_loss  0.91 | eval_loss  2.43\n","| epoch 350/500 | lr 2.00000 | ms/epoch 4269.73033 | train_loss  0.90 | eval_loss  2.30\n","| epoch 351/500 | lr 2.00000 | ms/epoch 4353.71184 | train_loss  0.90 | eval_loss  2.40\n","| epoch 352/500 | lr 2.00000 | ms/epoch 4323.74954 | train_loss  0.90 | eval_loss  2.22\n","| epoch 353/500 | lr 2.00000 | ms/epoch 4409.59167 | train_loss  0.90 | eval_loss  2.14\n","| epoch 354/500 | lr 2.00000 | ms/epoch 4132.49588 | train_loss  0.90 | eval_loss  2.32\n","| epoch 355/500 | lr 2.00000 | ms/epoch 4229.54130 | train_loss  0.90 | eval_loss  2.12\n","| epoch 356/500 | lr 2.00000 | ms/epoch 4207.39746 | train_loss  0.90 | eval_loss  2.14\n","| epoch 357/500 | lr 2.00000 | ms/epoch 3529.08158 | train_loss  0.90 | eval_loss  2.18\n","| epoch 358/500 | lr 2.00000 | ms/epoch 1977.01240 | train_loss  0.90 | eval_loss  2.12\n","| epoch 359/500 | lr 2.00000 | ms/epoch 1997.68686 | train_loss  0.90 | eval_loss  2.15\n","| epoch 360/500 | lr 2.00000 | ms/epoch 1993.53886 | train_loss  0.90 | eval_loss  2.28\n","| epoch 361/500 | lr 2.00000 | ms/epoch 1996.26827 | train_loss  0.90 | eval_loss  2.19\n","| epoch 362/500 | lr 2.00000 | ms/epoch 2015.90800 | train_loss  0.90 | eval_loss  2.34\n","| epoch 363/500 | lr 2.00000 | ms/epoch 1991.94956 | train_loss  0.92 | eval_loss  2.31\n","| epoch 364/500 | lr 2.00000 | ms/epoch 2035.40468 | train_loss  0.90 | eval_loss  2.24\n","| epoch 365/500 | lr 2.00000 | ms/epoch 2026.63732 | train_loss  0.90 | eval_loss  2.16\n","| epoch 366/500 | lr 2.00000 | ms/epoch 2053.71952 | train_loss  0.89 | eval_loss  2.12\n","| epoch 367/500 | lr 2.00000 | ms/epoch 2055.73583 | train_loss  0.91 | eval_loss  2.08\n","| epoch 368/500 | lr 2.00000 | ms/epoch 2013.39793 | train_loss  0.90 | eval_loss  2.29\n","| epoch 369/500 | lr 2.00000 | ms/epoch 1982.17797 | train_loss  0.90 | eval_loss  2.19\n","| epoch 370/500 | lr 2.00000 | ms/epoch 2005.20396 | train_loss  0.89 | eval_loss  2.28\n","| epoch 371/500 | lr 2.00000 | ms/epoch 2027.49133 | train_loss  0.90 | eval_loss  2.19\n","| epoch 372/500 | lr 2.00000 | ms/epoch 2025.63787 | train_loss  0.93 | eval_loss  2.35\n","| epoch 373/500 | lr 2.00000 | ms/epoch 2467.61632 | train_loss  0.90 | eval_loss  2.18\n","| epoch 374/500 | lr 2.00000 | ms/epoch 4044.65580 | train_loss  0.90 | eval_loss  2.16\n","| epoch 375/500 | lr 2.00000 | ms/epoch 4045.11547 | train_loss  0.90 | eval_loss  2.12\n","| epoch 376/500 | lr 2.00000 | ms/epoch 4057.42717 | train_loss  0.90 | eval_loss  2.18\n","| epoch 377/500 | lr 2.00000 | ms/epoch 4004.05288 | train_loss  0.90 | eval_loss  2.20\n","| epoch 378/500 | lr 2.00000 | ms/epoch 4202.01778 | train_loss  0.89 | eval_loss  2.22\n","| epoch 379/500 | lr 2.00000 | ms/epoch 4163.97095 | train_loss  0.90 | eval_loss  2.17\n","| epoch 380/500 | lr 2.00000 | ms/epoch 4171.01932 | train_loss  0.90 | eval_loss  2.21\n","| epoch 381/500 | lr 2.00000 | ms/epoch 4143.97454 | train_loss  0.89 | eval_loss  2.16\n","| epoch 382/500 | lr 2.00000 | ms/epoch 4114.76851 | train_loss  0.90 | eval_loss  2.20\n","| epoch 383/500 | lr 2.00000 | ms/epoch 4107.40495 | train_loss  0.89 | eval_loss  2.29\n","| epoch 384/500 | lr 2.00000 | ms/epoch 4061.71918 | train_loss  0.90 | eval_loss  2.16\n","| epoch 385/500 | lr 2.00000 | ms/epoch 3971.30966 | train_loss  0.90 | eval_loss  2.16\n","| epoch 386/500 | lr 2.00000 | ms/epoch 4114.14266 | train_loss  0.89 | eval_loss  2.17\n","| epoch 387/500 | lr 2.00000 | ms/epoch 4180.22943 | train_loss  0.89 | eval_loss  2.17\n","| epoch 388/500 | lr 2.00000 | ms/epoch 4300.72379 | train_loss  0.89 | eval_loss  2.17\n","| epoch 389/500 | lr 2.00000 | ms/epoch 2376.38450 | train_loss  0.89 | eval_loss  2.19\n","| epoch 390/500 | lr 2.00000 | ms/epoch 2008.29434 | train_loss  0.90 | eval_loss  2.32\n","| epoch 391/500 | lr 2.00000 | ms/epoch 2020.19739 | train_loss  0.89 | eval_loss  2.20\n","| epoch 392/500 | lr 2.00000 | ms/epoch 2006.09064 | train_loss  0.89 | eval_loss  2.14\n","| epoch 393/500 | lr 2.00000 | ms/epoch 2010.56862 | train_loss  0.91 | eval_loss  2.21\n","| epoch 394/500 | lr 2.00000 | ms/epoch 2023.50283 | train_loss  0.89 | eval_loss  2.26\n","| epoch 395/500 | lr 2.00000 | ms/epoch 1996.89507 | train_loss  0.89 | eval_loss  2.18\n","| epoch 396/500 | lr 2.00000 | ms/epoch 2001.01304 | train_loss  0.89 | eval_loss  2.32\n","| epoch 397/500 | lr 2.00000 | ms/epoch 1997.87736 | train_loss  0.89 | eval_loss  2.19\n","| epoch 398/500 | lr 2.00000 | ms/epoch 2022.86100 | train_loss  0.89 | eval_loss  2.09\n","| epoch 399/500 | lr 2.00000 | ms/epoch 2003.93772 | train_loss  0.93 | eval_loss  2.19\n","| epoch 400/500 | lr 2.00000 | ms/epoch 2002.66623 | train_loss  0.89 | eval_loss  2.17\n","| epoch 401/500 | lr 2.00000 | ms/epoch 2073.29464 | train_loss  0.89 | eval_loss  2.31\n","| epoch 402/500 | lr 2.00000 | ms/epoch 2062.10899 | train_loss  0.89 | eval_loss  2.36\n","| epoch 403/500 | lr 2.00000 | ms/epoch 2054.64411 | train_loss  0.89 | eval_loss  2.82\n","| epoch 404/500 | lr 2.00000 | ms/epoch 2016.85071 | train_loss  0.88 | eval_loss  2.28\n","| epoch 405/500 | lr 2.00000 | ms/epoch 3425.47989 | train_loss  0.89 | eval_loss  2.21\n","| epoch 406/500 | lr 2.00000 | ms/epoch 4020.70856 | train_loss  0.89 | eval_loss  2.17\n","| epoch 407/500 | lr 2.00000 | ms/epoch 3960.27708 | train_loss  0.89 | eval_loss  2.18\n","| epoch 408/500 | lr 2.00000 | ms/epoch 3967.23366 | train_loss  0.89 | eval_loss  2.15\n","| epoch 409/500 | lr 2.00000 | ms/epoch 4100.82221 | train_loss  0.89 | eval_loss  2.24\n","| epoch 410/500 | lr 2.00000 | ms/epoch 4085.73842 | train_loss  0.89 | eval_loss  2.16\n","| epoch 411/500 | lr 2.00000 | ms/epoch 4195.68729 | train_loss  0.91 | eval_loss  2.27\n","| epoch 412/500 | lr 2.00000 | ms/epoch 4263.96394 | train_loss  0.89 | eval_loss  2.19\n","| epoch 413/500 | lr 2.00000 | ms/epoch 4218.34588 | train_loss  0.89 | eval_loss  2.30\n","| epoch 414/500 | lr 2.00000 | ms/epoch 4308.63166 | train_loss  0.89 | eval_loss  2.35\n","| epoch 415/500 | lr 2.00000 | ms/epoch 4095.82949 | train_loss  0.89 | eval_loss  2.28\n","| epoch 416/500 | lr 2.00000 | ms/epoch 4291.95189 | train_loss  0.89 | eval_loss  2.27\n","| epoch 417/500 | lr 2.00000 | ms/epoch 4167.07182 | train_loss  0.88 | eval_loss  2.26\n","| epoch 418/500 | lr 2.00000 | ms/epoch 4292.81926 | train_loss  0.88 | eval_loss  2.31\n","| epoch 419/500 | lr 2.00000 | ms/epoch 4421.73004 | train_loss  0.88 | eval_loss  2.32\n","| epoch 420/500 | lr 2.00000 | ms/epoch 3666.65554 | train_loss  0.89 | eval_loss  2.29\n","| epoch 421/500 | lr 2.00000 | ms/epoch 1997.25270 | train_loss  0.91 | eval_loss  2.11\n","| epoch 422/500 | lr 2.00000 | ms/epoch 2032.95231 | train_loss  0.88 | eval_loss  2.35\n","| epoch 423/500 | lr 2.00000 | ms/epoch 2046.88144 | train_loss  0.92 | eval_loss  2.33\n","| epoch 424/500 | lr 2.00000 | ms/epoch 1993.67189 | train_loss  0.88 | eval_loss  2.18\n","| epoch 425/500 | lr 2.00000 | ms/epoch 2009.92417 | train_loss  0.88 | eval_loss  2.24\n","| epoch 426/500 | lr 2.00000 | ms/epoch 2025.37870 | train_loss  0.88 | eval_loss  2.52\n","| epoch 427/500 | lr 2.00000 | ms/epoch 2031.56734 | train_loss  0.88 | eval_loss  2.30\n","| epoch 428/500 | lr 2.00000 | ms/epoch 1999.34793 | train_loss  0.91 | eval_loss  2.17\n","| epoch 429/500 | lr 2.00000 | ms/epoch 2032.06515 | train_loss  0.88 | eval_loss  2.18\n","| epoch 430/500 | lr 2.00000 | ms/epoch 1994.83156 | train_loss  0.89 | eval_loss  2.25\n","| epoch 431/500 | lr 2.00000 | ms/epoch 1999.76540 | train_loss  0.91 | eval_loss  2.20\n","| epoch 432/500 | lr 2.00000 | ms/epoch 2040.78269 | train_loss  0.88 | eval_loss  2.22\n","| epoch 433/500 | lr 2.00000 | ms/epoch 2030.66087 | train_loss  0.90 | eval_loss  2.26\n","| epoch 434/500 | lr 2.00000 | ms/epoch 2050.60053 | train_loss  0.89 | eval_loss  2.19\n","| epoch 435/500 | lr 2.00000 | ms/epoch 2034.74498 | train_loss  0.89 | eval_loss  2.20\n","| epoch 436/500 | lr 2.00000 | ms/epoch 2759.04989 | train_loss  0.88 | eval_loss  2.33\n","| epoch 437/500 | lr 2.00000 | ms/epoch 4245.86201 | train_loss  0.90 | eval_loss  2.19\n","| epoch 438/500 | lr 2.00000 | ms/epoch 4233.74128 | train_loss  0.88 | eval_loss  2.69\n","| epoch 439/500 | lr 2.00000 | ms/epoch 4182.32489 | train_loss  0.88 | eval_loss  2.24\n","| epoch 440/500 | lr 2.00000 | ms/epoch 4174.48878 | train_loss  0.88 | eval_loss  2.16\n","| epoch 441/500 | lr 2.00000 | ms/epoch 4324.57256 | train_loss  0.88 | eval_loss  2.20\n","| epoch 442/500 | lr 2.00000 | ms/epoch 4294.57140 | train_loss  0.88 | eval_loss  2.15\n","| epoch 443/500 | lr 2.00000 | ms/epoch 4309.24487 | train_loss  0.88 | eval_loss  2.20\n","| epoch 444/500 | lr 2.00000 | ms/epoch 4240.19194 | train_loss  0.88 | eval_loss  2.34\n","| epoch 445/500 | lr 2.00000 | ms/epoch 4314.41665 | train_loss  0.88 | eval_loss  2.24\n","| epoch 446/500 | lr 2.00000 | ms/epoch 4216.39633 | train_loss  0.89 | eval_loss  2.16\n","| epoch 447/500 | lr 2.00000 | ms/epoch 4130.91636 | train_loss  0.88 | eval_loss  2.27\n","| epoch 448/500 | lr 2.00000 | ms/epoch 4102.74458 | train_loss  0.88 | eval_loss  2.28\n","| epoch 449/500 | lr 2.00000 | ms/epoch 4300.45366 | train_loss  0.87 | eval_loss  2.32\n","| epoch 450/500 | lr 2.00000 | ms/epoch 4176.01466 | train_loss  0.88 | eval_loss  2.26\n","| epoch 451/500 | lr 2.00000 | ms/epoch 2018.96834 | train_loss  0.88 | eval_loss  2.42\n","| epoch 452/500 | lr 2.00000 | ms/epoch 1973.31595 | train_loss  0.88 | eval_loss  2.32\n","| epoch 453/500 | lr 2.00000 | ms/epoch 3642.04001 | train_loss  0.89 | eval_loss  2.35\n","| epoch 454/500 | lr 2.00000 | ms/epoch 4075.52791 | train_loss  0.87 | eval_loss  2.16\n","| epoch 455/500 | lr 2.00000 | ms/epoch 3953.91965 | train_loss  0.88 | eval_loss  2.43\n","| epoch 456/500 | lr 2.00000 | ms/epoch 4066.18404 | train_loss  0.88 | eval_loss  2.41\n","| epoch 457/500 | lr 2.00000 | ms/epoch 4190.88316 | train_loss  0.87 | eval_loss  2.22\n","| epoch 458/500 | lr 2.00000 | ms/epoch 4191.63108 | train_loss  0.90 | eval_loss  2.28\n","| epoch 459/500 | lr 2.00000 | ms/epoch 4324.15867 | train_loss  0.88 | eval_loss  2.46\n","| epoch 460/500 | lr 2.00000 | ms/epoch 5684.83973 | train_loss  0.88 | eval_loss  2.39\n","| epoch 461/500 | lr 2.00000 | ms/epoch 6023.92197 | train_loss  0.87 | eval_loss  2.74\n","| epoch 462/500 | lr 2.00000 | ms/epoch 5853.47414 | train_loss  0.88 | eval_loss  2.45\n","| epoch 463/500 | lr 2.00000 | ms/epoch 5816.79988 | train_loss  0.89 | eval_loss  2.17\n","| epoch 464/500 | lr 2.00000 | ms/epoch 6056.99325 | train_loss  0.87 | eval_loss  2.27\n","| epoch 465/500 | lr 2.00000 | ms/epoch 6050.67348 | train_loss  0.88 | eval_loss  2.32\n","| epoch 466/500 | lr 2.00000 | ms/epoch 6101.23038 | train_loss  0.87 | eval_loss  2.25\n","| epoch 467/500 | lr 2.00000 | ms/epoch 5948.24791 | train_loss  0.88 | eval_loss  2.29\n","| epoch 468/500 | lr 2.00000 | ms/epoch 4977.61011 | train_loss  0.87 | eval_loss  2.16\n","| epoch 469/500 | lr 2.00000 | ms/epoch 4261.18255 | train_loss  0.87 | eval_loss  2.23\n","| epoch 470/500 | lr 2.00000 | ms/epoch 4310.13751 | train_loss  0.87 | eval_loss  2.42\n","| epoch 471/500 | lr 2.00000 | ms/epoch 4210.73389 | train_loss  0.87 | eval_loss  2.13\n","| epoch 472/500 | lr 2.00000 | ms/epoch 4168.21647 | train_loss  0.90 | eval_loss  2.56\n","| epoch 473/500 | lr 2.00000 | ms/epoch 4201.65491 | train_loss  0.88 | eval_loss  2.31\n","| epoch 474/500 | lr 2.00000 | ms/epoch 4297.21522 | train_loss  0.87 | eval_loss  2.22\n","| epoch 475/500 | lr 2.00000 | ms/epoch 3767.34281 | train_loss  0.87 | eval_loss  2.37\n","| epoch 476/500 | lr 2.00000 | ms/epoch 2022.63594 | train_loss  0.87 | eval_loss  2.43\n","| epoch 477/500 | lr 2.00000 | ms/epoch 2017.95602 | train_loss  0.87 | eval_loss  2.60\n","| epoch 478/500 | lr 2.00000 | ms/epoch 2003.22318 | train_loss  0.88 | eval_loss  2.26\n","| epoch 479/500 | lr 2.00000 | ms/epoch 1999.27855 | train_loss  0.90 | eval_loss  2.12\n","| epoch 480/500 | lr 2.00000 | ms/epoch 2006.73389 | train_loss  0.87 | eval_loss  2.31\n","| epoch 481/500 | lr 2.00000 | ms/epoch 1998.70992 | train_loss  0.89 | eval_loss  2.49\n","| epoch 482/500 | lr 2.00000 | ms/epoch 1984.45702 | train_loss  0.87 | eval_loss  2.28\n","| epoch 483/500 | lr 2.00000 | ms/epoch 2005.34105 | train_loss  0.88 | eval_loss  2.30\n","| epoch 484/500 | lr 2.00000 | ms/epoch 2049.40534 | train_loss  0.87 | eval_loss  2.59\n","| epoch 485/500 | lr 2.00000 | ms/epoch 2003.82662 | train_loss  0.87 | eval_loss  2.59\n","| epoch 486/500 | lr 2.00000 | ms/epoch 2005.28216 | train_loss  0.87 | eval_loss  2.32\n","| epoch 487/500 | lr 2.00000 | ms/epoch 2009.83381 | train_loss  0.87 | eval_loss  2.51\n","| epoch 488/500 | lr 2.00000 | ms/epoch 2003.50642 | train_loss  0.87 | eval_loss  2.25\n","| epoch 489/500 | lr 2.00000 | ms/epoch 1989.85863 | train_loss  0.87 | eval_loss  2.66\n","| epoch 490/500 | lr 2.00000 | ms/epoch 2015.31529 | train_loss  0.87 | eval_loss  2.54\n","| epoch 491/500 | lr 2.00000 | ms/epoch 2503.67117 | train_loss  0.87 | eval_loss  2.21\n","| epoch 492/500 | lr 2.00000 | ms/epoch 3952.80123 | train_loss  0.86 | eval_loss  2.50\n","| epoch 493/500 | lr 2.00000 | ms/epoch 4039.68072 | train_loss  0.86 | eval_loss  2.28\n","| epoch 494/500 | lr 2.00000 | ms/epoch 4001.85895 | train_loss  0.87 | eval_loss  2.33\n","| epoch 495/500 | lr 2.00000 | ms/epoch 4077.57425 | train_loss  0.87 | eval_loss  2.28\n","| epoch 496/500 | lr 2.00000 | ms/epoch 4162.43839 | train_loss  0.87 | eval_loss  2.14\n","| epoch 497/500 | lr 2.00000 | ms/epoch 4215.63601 | train_loss  0.87 | eval_loss  2.57\n","| epoch 498/500 | lr 2.00000 | ms/epoch 4239.16006 | train_loss  0.86 | eval_loss  2.38\n","| epoch 499/500 | lr 2.00000 | ms/epoch 4206.28786 | train_loss  0.87 | eval_loss  2.35\n","| epoch 500/500 | lr 2.00000 | ms/epoch 4314.44836 | train_loss  0.86 | eval_loss  2.73\n","\n","\n"," TRAINING FINISHED:\n","\n","\tBest Loss:  2.00\tBest Model saved at epoch: 179 \n","\n","\n","\n","\n","TEST LOSS: 2.1436619758605957\n"]},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# MODEL PARAMETERS\n","TRAIN_MODEL = True\n","\n","EPOCHS = 500 # 500\n","LEARNING_RATE = 2 # 4\n","BATCH_SIZE = 8 # 16\n","EARLY_STOP = True\n","\n","# import gc\n","# gc.collect()\n","\n","train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","\n","FEEDBACK = False\n","EMPHASIZE_EEG = False\n","model, criterion, optimizer = initialize_model()\n","train('results/LR2/model')\n","\n","FEEDBACK = False\n","EMPHASIZE_EEG = True\n","model, criterion, optimizer = initialize_model()\n","train('results/LR2/model_EEG')\n","\n","# FEEDBACK = True\n","# EMPHASIZE_EEG = False\n","# model, criterion, optimizer = initialize_model()\n","# train('results/model_feedback')\n","\n","# FEEDBACK = True\n","# EMPHASIZE_EEG = True\n","# model, criterion, optimizer = initialize_model()\n","# train('results/model_EEG_feedback')\n","\n","# if TRAIN_MODEL:\n","\n","#   for i in range(2):\n","\n","#     if i == 0:\n","#       FEEDBACK = False\n","#     else:\n","#       FEEDBACK = True\n","\n","#     BATCH_SIZE = 4\n","#     LEARNING_RATE = 1.0\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     LEARNING_RATE = 2.0\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     LEARNING_RATE = 4.0\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     LEARNING_RATE = 1.0\n","#     BATCH_SIZE = 8\n","#     train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     BATCH_SIZE = 16\n","#     train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     BATCH_SIZE = 32\n","#     train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","#     model, criterion, optimizer = initialize_model()\n","#     train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/marc0bortolotti/Affective-AI-Music-Improviser/blob/main/TCN/training.ipynb","timestamp":1720018338190}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
