{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6405,"status":"ok","timestamp":1720350493956,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"em9mWQxF-fCK","outputId":"b6f25bad-6d9a-45d9-aa52-7468830a096d"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n","/home/marco_bortolotti/project/Affective-AI-Music-Improviser/TCN\n"]}],"source":["import glob\n","import numpy as np\n","import pandas as pd\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n","import matplotlib.pyplot as plt\n","import yaml\n","import re\n","import sys\n","sys.path.append('..')\n","from APPLICATION.model.tokenization import PrettyMidiTokenizer, BCI_TOKENS, SILENCE_TOKEN\n","from APPLICATION.model.model import TCN\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","pd.set_option('display.max_rows',500)\n","pd.set_option('display.max_columns',504)\n","pd.set_option('display.width',1000)\n","\n","\n","# MODEL PARAMETERS\n","EPOCHS = 500 # 500\n","LEARNING_RATE = 1 # 4\n","BATCH_SIZE = 4 # 16\n","TRAIN_MODEL = True\n","FEEDBACK = False\n","EMPHASIZE_EEG = True\n","EARLY_STOP = True\n","\n","pwd = os.getcwd()\n","print(pwd)\n","\n","DIRECTORY_PATH = ''\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6673,"status":"ok","timestamp":1720350500624,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"q-xf9JjP-fCM","outputId":"106f73f2-f24a-4b1b-e086-67277f52054b"},"outputs":[{"name":"stdout","output_type":"stream","text":["dataset\n","Number of input files: 6\n","Number of output files: 6 \n","\n","1: 0_Drum_HardRock_EXCITED.mid -> 0_Bass_HardRock_EXCITED.mid\n","Input sequence length: 26\n","Emotion token: C\n","\n","2: 1_Drum_HardRock_EXCITED.mid -> 1_Bass_HardRock_EXCITED.mid\n","Input sequence length: 32\n","Emotion token: C\n","\n","3: 2_Drum_Blues_EXCITED.mid -> 2_Bass_Blues_EXCITED.mid\n","Input sequence length: 20\n","Emotion token: C\n","\n","4: 3_Drum_Blues_EXCITED.mid -> 3_Bass_Blues_EXCITED.mid\n","Input sequence length: 20\n","Emotion token: C\n","\n","5: 4_Drum_PopRock_RELAX.mid -> 4_Bass_PopRock_RELAX.mid\n","Input sequence length: 35\n","Emotion token: R\n","\n","6: 5_Drum_PopRock_RELAX.mid -> 5_Bass_PopRock_RELAX.mid\n","Input sequence length: 23\n","Emotion token: R\n","\n","\n","Number of input sequences: 156\n","Input sequence length: 192\n","Input vocabulars size: 16\n","\n","Number of output sequences: 156\n","Output sequence length: 192\n","Output vocabulars size: 87\n","\n","Input vocab: {'O': 0, 'R': 1, 'C': 2, '36fS': 3, '42pS': 4, '38fS': 5, '36fS_42fS': 6, '38fS_42fS': 7, '42fS': 8, '36fS_42pS': 9, '38pS_42fS': 10, '38fS_42pS': 11, '36pS': 12, '36pS_42pS': 13, '38pS': 14, '38pS_42pS': 15}\n","Output vocab: {'O': 0, '52fS': 1, '52f': 2, '55fS': 3, '55f': 4, '45fS': 5, '45f': 6, '48fS': 7, '48f': 8, '47fS': 9, '47f': 10, '43fS': 11, '43f': 12, '57fS': 13, '57f': 14, '50fS': 15, '50f': 16, '40fS': 17, '40f': 18, '52pS': 19, '52p': 20, '48pS': 21, '48p': 22, '43pS': 23, '43p': 24, '47pS': 25, '47p': 26, '67pS': 27, '67p': 28, '50pS': 29, '50p': 30, '40pS': 31, '40p': 32, '55pS': 33, '55p': 34, '59pS': 35, '59p': 36, '62pS': 37, '62p': 38, '60pS': 39, '60p': 40, '57pS': 41, '57p': 42, '53pS': 43, '53p': 44, '64pS': 45, '64p': 46, '45pS': 47, '45p': 48, '56fS': 49, '56f': 50, '59fS': 51, '59f': 52, '49fS': 53, '49f': 54, '51fS': 55, '51f': 56, '54fS': 57, '54f': 58, '58fS': 59, '58f': 60, '63fS': 61, '63f': 62, '64fS': 63, '64f': 64, '66fS': 65, '66f': 66, '61fS': 67, '61f': 68, '62fS': 69, '62f': 70, '56pS': 71, '56p': 72, '49pS': 73, '49p': 74, '51pS': 75, '51p': 76, '54pS': 77, '54p': 78, '58pS': 79, '58p': 80, '63pS': 81, '63p': 82, '66pS': 83, '66p': 84, '61pS': 85, '61p': 86}\n"]}],"source":["'''\n","Assumptions:\n","Sequences described as input_#.mid and output_#.mid in the corresponding folders\n","'''\n","DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n","\n","print(DATASET_PATH)\n","\n","input_filenames = sorted(glob.glob(os.path.join(DATASET_PATH, 'input/*.mid')))\n","print('Number of input files:', len(input_filenames))\n","\n","output_filenames = sorted(glob.glob(os.path.join(DATASET_PATH, 'output/*.mid')))\n","print('Number of output files:', len(output_filenames), '\\n')\n","\n","\n","INPUT_TOK = PrettyMidiTokenizer()\n","OUTPUT_TOK = PrettyMidiTokenizer()\n","\n","for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n","\n","    in_file_name = os.path.basename(in_file)\n","    out_file_name = os.path.basename(out_file)\n","    print(f'{i + 1}: {in_file_name} -> {out_file_name}')\n","\n","    if 'RELAX' in in_file_name:\n","        emotion_token = BCI_TOKENS['relaxed']\n","    elif 'EXCITED' in in_file_name:\n","        emotion_token = BCI_TOKENS['concentrated']\n","    else:\n","        raise Exception('Emotion not found in file name. Please add the emotion to the file name.')\n","\n","    in_seq, in_df = INPUT_TOK.midi_to_tokens(in_file, update_vocab=True, update_sequences=True, emotion_token = emotion_token, instrument='drum')\n","    out_seq, out_df = OUTPUT_TOK.midi_to_tokens(out_file, update_vocab=True, update_sequences=True)\n","\n","    print(f'Input sequence length: {len(in_seq)}')\n","    print(f'Emotion token: {emotion_token}\\n')\n","\n","print(f'\\nNumber of input sequences: {len(INPUT_TOK.sequences)}')\n","print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n","print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n","print(f'\\nNumber of output sequences: {len(OUTPUT_TOK.sequences)}')\n","print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n","print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n","\n","print('\\nInput vocab:', INPUT_TOK.VOCAB.word2idx)\n","print('Output vocab:', OUTPUT_TOK.VOCAB.word2idx)\n","\n","# with open('training_seq.txt', 'w') as f:    \n","#     for seq in INPUT_TOK.sequences:\n","#         for tok in seq[:48]:\n","#             f.write('\\\"' + INPUT_TOK.VOCAB.idx2word[tok] + '\\\", ')\n","#         f.write('\\n')\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Create the dataset\n","dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n","                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n","\n","# Split the dataset into training, evaluation and test sets\n","train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":539,"status":"ok","timestamp":1720350501148,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"TZ_LJL_F-fCN","outputId":"8031d1db-eddd-4add-9a47-b05fd05b2d2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size before augmentation: 94\n","Training set size after augmentation: 1974\n"]}],"source":["# Augment the training set\n","def data_augmentation_shift(dataset, shifts):\n","    '''\n","    Shifts the sequences by a number of ticks to create new sequences.\n","    '''\n","    augmented_input_sequences = []\n","    output_sequences = []\n","\n","    for ticks in shifts:\n","        for input_sequence, ouput_sequence in dataset:\n","            input_sequence = input_sequence.cpu().numpy().copy()\n","\n","            # remove the first token since it is the emotion token\n","            emotion_token = input_sequence[0]\n","            input_sequence = input_sequence[1:]\n","\n","            # shift the sequence\n","            new_input_sequence = np.roll(input_sequence, ticks)\n","\n","            # add the emotion token back to the sequence\n","            new_input_sequence = np.concatenate(([emotion_token], new_input_sequence))\n","\n","            # add the new sequence to the augmented sequences\n","            augmented_input_sequences.append(new_input_sequence)\n","            output_sequences.append(ouput_sequence.cpu().numpy().copy())\n","    \n","    augmented_dataset = TensorDataset(torch.LongTensor(augmented_input_sequences).to(device), \n","                                      torch.LongTensor(output_sequences).to(device))\n","    \n","    # Concatenate the original and the augmented dataset\n","    concatenated_dataset = torch.utils.data.ConcatDataset([dataset, augmented_dataset])\n","\n","    return concatenated_dataset\n","\n","\n","def data_augmentation_transposition(dataset, transpositions):\n","    '''\n","    Transpose the sequences by a number of semitones to create new sequences.\n","\n","    Parameters:\n","    - transpositions: a list of integers representing the number of semitones to transpose the sequences.\n","\n","    NB: The transposition is done by adding the number of semitones to the pitch of each note in the sequence.\n","    '''\n","\n","    input_sequences = []\n","    augmented_output_sequences = []\n","\n","    for transposition in transpositions:\n","        for input_sequence, ouput_sequence in dataset:\n","\n","            input_sequence = input_sequence.cpu().numpy().copy()\n","            new_ouput_sequence = ouput_sequence.cpu().numpy().copy()\n","\n","            for i in range(len(new_ouput_sequence)):\n","\n","                token = ouput_sequence[i]\n","                word = OUTPUT_TOK.VOCAB.idx2word[token]\n","\n","                # check if the token is a note\n","                if word != SILENCE_TOKEN and word != BCI_TOKENS['relaxed'] and word != BCI_TOKENS['concentrated']:\n","\n","                    # extract all the pitches from the token \n","                    pitches = re.findall(r'\\d+', word) # NB: pitches is a string list\n","\n","                    # transpose each pitch in the token \n","                    for pitch in pitches:\n","                        new_pitch = str(int(pitch) + transposition)\n","                        word = word.replace(pitch, new_pitch)\n","\n","                    # add the new token to the vocabulary\n","                    OUTPUT_TOK.VOCAB.add_word(word) \n","\n","                    # update the sequence with the new token\n","                    new_ouput_sequence[i] = OUTPUT_TOK.VOCAB.word2idx[word]\n","            \n","            # update sequence with the new tokens\n","            input_sequences.append(input_sequence)\n","            augmented_output_sequences.append(new_ouput_sequence)\n","\n","    augmented_dataset = TensorDataset(torch.LongTensor(input_sequences).to(device), \n","                                      torch.LongTensor(augmented_output_sequences).to(device))\n","    \n","    # Concatenate the original and the augmented dataset\n","    concatenated_dataset = torch.utils.data.ConcatDataset([dataset, augmented_dataset])\n","\n","    return concatenated_dataset\n","\n","train_set_augmented = data_augmentation_shift(train_set, [-3, -2, -1, 1, 2, 3])\n","train_set_augmented = data_augmentation_transposition(train_set_augmented, [3,5])\n","\n","print(f'Training set size before augmentation: {len(train_set)}')\n","print(f'Training set size after augmentation: {len(train_set_augmented)}')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train set size: 1974\n","Evaluation set size: 31\n","Test set size: 31\n"]}],"source":["def initialize_dataset():\n","\n","  # Create the dataloaders\n","  train_sampler = RandomSampler(train_set_augmented)\n","  train_dataloader = DataLoader(train_set_augmented, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","  eval_sampler = RandomSampler(eval_set)\n","  eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n","\n","  test_sampler = RandomSampler(test_set)\n","  test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n","\n","  return train_dataloader, eval_dataloader, test_dataloader\n","\n","train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","\n","print(f'Train set size: {len(train_set_augmented)}')\n","print(f'Evaluation set size: {len(eval_set)}')\n","print(f'Test set size: {len(test_set)}')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3117,"status":"ok","timestamp":1720350504263,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"BIzI5qkM-fCO","outputId":"7de9c3de-a426-4484-8487-c62ae3855b99"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Input size: 16\n","\n","Model created: TCN(\n","  (encoder): Embedding(16, 20, padding_idx=0)\n","  (tcn): TemporalConvNet(\n","    (network): Sequential(\n","      (0): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          20, 192, kernel_size=(3,), stride=(1,), padding=(2,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(2,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            20, 192, kernel_size=(3,), stride=(1,), padding=(2,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(2,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (downsample): Conv1d(20, 192, kernel_size=(1,), stride=(1,))\n","        (relu): ReLU()\n","      )\n","      (1): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (2): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (3): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (4): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (5): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (6): TemporalBlock(\n","        (conv1): ParametrizedConv1d(\n","          192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.45, inplace=False)\n","        (conv2): ParametrizedConv1d(\n","          20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,)\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.45, inplace=False)\n","        (net): Sequential(\n","          (0): ParametrizedConv1d(\n","            192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.45, inplace=False)\n","          (4): ParametrizedConv1d(\n","            20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,)\n","            (parametrizations): ModuleDict(\n","              (weight): ParametrizationList(\n","                (0): _WeightNorm()\n","              )\n","            )\n","          )\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.45, inplace=False)\n","        )\n","        (downsample): Conv1d(192, 20, kernel_size=(1,), stride=(1,))\n","        (relu): ReLU()\n","      )\n","    )\n","  )\n","  (decoder): Linear(in_features=20, out_features=107, bias=True)\n","  (drop): Dropout(p=0.25, inplace=False)\n",")\n","tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","        0.1000, 0.1000], device='cuda:0', grad_fn=<SelectBackward0>)\n"]}],"source":["# Set the hyperparameters\n","SEED = 1111\n","torch.manual_seed(SEED)\n","\n","'''\n","IMPORTANT:\n","to cover all the sequence of tokens k * d must be >= hidden units (see the paper)\n","k = kernel_size\n","d = dilation = 2 ^ (n_levels - 1)\n","'''\n","\n","OUTPUT_SIZE = len(OUTPUT_TOK.VOCAB)\n","\n","if FEEDBACK:\n","    INPUT_SIZE = len(INPUT_TOK.VOCAB) + OUTPUT_SIZE\n","    LEVELS = 8\n","    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH * 2 # 192 * 2 = 384\n","else:\n","    INPUT_SIZE = len(INPUT_TOK.VOCAB)\n","    LEVELS = 7\n","    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH # 192\n","\n","print(f'\\nInput size: {len(INPUT_TOK.VOCAB)}')\n","\n","\n","EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] real value vectors (see model.py)\n","NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE] # [192, 192, 192, 192, 192, 192, 20]\n","GRADIENT_CLIP = 0.35\n","\n","\n","# balance the loss function by assigning a weight to each token related to its frequency\n","LOSS_WEIGTHS = torch.ones([OUTPUT_SIZE], dtype=torch.float, device = device)\n","OUTPUT_TOK.VOCAB.compute_weights()\n","for i, weigth in enumerate(OUTPUT_TOK.VOCAB.weights):\n","    LOSS_WEIGTHS[i] = 1 - weigth\n","    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n","\n","\n","def initialize_model():\n","  # create the model\n","  model = TCN(input_size = INPUT_SIZE,\n","              embedding_size = EMBEDDING_SIZE,\n","              output_size = OUTPUT_SIZE,\n","              num_channels = NUM_CHANNELS,\n","              emphasize_eeg = EMPHASIZE_EEG,\n","              dropout = 0.45,\n","              emb_dropout = 0.25,\n","              kernel_size = 3,\n","              tied_weights = False) # tie encoder and decoder weights (legare)\n","\n","  model.to(device)\n","\n","  # May use adaptive softmax to speed up training\n","  criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n","  optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n","\n","  return model, criterion, optimizer\n","\n","model, criterion, optimizer = initialize_model()\n","\n","print(f'\\nModel created: {model}')\n","print(model.encoder.weight[0])\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"tLO23hB8-fCP"},"outputs":[],"source":["def save_parameters():\n","\n","    # plot the losses over the epochs\n","\n","    plt.plot(train_losses, label='train')\n","    plt.plot(eval_losses, label='eval')\n","    plt.legend()\n","    plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))\n","    plt.clf()\n","\n","    # save the vocabularies\n","    INPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'input_vocab.txt'))\n","    OUTPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'output_vocab.txt'))\n","\n","     # save the model hyperparameters in a file txt\n","    with open(os.path.join(RESULTS_PATH, 'model_hyperparameters.txt'), 'w') as f:\n","\n","        f.write(f'DATE: {time.strftime(\"%Y%m%d-%H%M%S\")}\\n\\n')\n","\n","        f.write(f'-----------------DATASET------------------\\n')\n","        f.write(f'DATASET_PATH: {DATASET_PATH}\\n')\n","        f.write(f'TRAIN_SET_SIZE: {len(train_set)}\\n')\n","        f.write(f'EVAL_SET_SIZE: {len(eval_set)}\\n')\n","        f.write(f'TEST_SET_SIZE: {len(test_set)}\\n\\n')\n","\n","\n","        f.write(f'----------OPTIMIZATION PARAMETERS----------\\n')\n","        f.write(f'GRADIENT_CLIP: {GRADIENT_CLIP}\\n')\n","        f.write(f'FEEDBACK: {FEEDBACK}\\n')\n","        f.write(f'EARLY STOPPING: {EARLY_STOP}\\n')\n","        f.write(f'EMPHASIZE_EEG: {EMPHASIZE_EEG}\\n')\n","        f.write(f'LEARNING_RATE: {LEARNING_RATE}\\n')\n","        f.write(f'BATCH_SIZE: {BATCH_SIZE}\\n')\n","        f.write(f'EPOCHS: {EPOCHS}\\n\\n')\n","\n","\n","        f.write(f'------------MODEL PARAMETERS--------------\\n')\n","        f.write(f'SEED: {SEED}\\n')\n","        f.write(f'INPUT_SIZE: {INPUT_SIZE}\\n')\n","        f.write(f'EMBEDDING_SIZE: {EMBEDDING_SIZE}\\n')\n","        f.write(f'LEVELS: {LEVELS}\\n')\n","        f.write(f'HIDDEN_UNITS: {HIDDEN_UNITS}\\n')\n","        f.write(f'NUM_CHANNELS: {NUM_CHANNELS}\\n')\n","        f.write(f'OUTPUT_SIZE: {OUTPUT_SIZE}\\n')\n","        f.write(f'LOSS_WEIGTHS: {LOSS_WEIGTHS}\\n\\n')\n","\n","\n","\n","        f.write(f'-------------------RESULTS----------------\\n')\n","        f.write(f'TRAIN_LOSSES: {best_train_loss}\\n')\n","        f.write(f'BEST_EVAL_LOSS: {best_eval_loss}\\n')\n","        f.write(f'TEST_LOSS: {test_loss}\\n')\n","        f.write(f'BEST_MODEL_EPOCH: {best_model_epoch}\\n')\n","\n","    data = {\n","        'DATE': time.strftime(\"%Y%m%d-%H%M%S\"),\n","        'INPUT_SIZE': INPUT_SIZE,\n","        'EMBEDDING_SIZE': EMBEDDING_SIZE,\n","        'NUM_CHANNELS': NUM_CHANNELS,\n","        'OUTPUT_SIZE': OUTPUT_SIZE,\n","        'KERNEL_SIZE': 3\n","    }\n","\n","    path = os.path.join(RESULTS_PATH, 'config.yaml')\n","    with open(path, 'w') as file:\n","        yaml.safe_dump(data, file)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"8X0pPmwo-fCO"},"outputs":[],"source":["BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n","\n","def epoch_step(dataloader, mode):\n","\n","    if FEEDBACK:\n","        prev_output = torch.zeros([BATCH_SIZE, INPUT_TOK.SEQ_LENGTH], dtype=torch.long, device=device)\n","\n","    if mode == 'train':\n","        model.train()\n","    else:\n","        model.eval() # disable dropout\n","\n","    total_loss = 0\n","\n","    # iterate over the training data\n","    for batch_idx, (data, targets) in enumerate(dataloader):\n","\n","        batch_idx += 1\n","\n","        # mask the last bar of the input data\n","        batch_size = data.size(0)\n","        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long, device = device)), dim = 1)\n","\n","        if FEEDBACK:\n","            input = torch.cat((data_masked, prev_output[:batch_size, :]), dim = 1)\n","        else:\n","            input = data_masked\n","\n","        # reset model gradients to zero\n","        optimizer.zero_grad()\n","\n","        # make the prediction\n","        output = model(input)[:, :INPUT_TOK.SEQ_LENGTH]\n","        prev_output = torch.argmax(output, 2)# batch, seq_len (hidden units), vocab_size\n","\n","        # flatten the output sequence\n","        # NB: the size -1 is inferred from other dimensions\n","        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n","\n","        final_target = targets.contiguous().view(-1)\n","        final_output = output.contiguous().view(-1, OUTPUT_SIZE)\n","\n","        # calculate the loss\n","        loss = criterion(final_output, final_target)\n","\n","        if mode == 'train':\n","            # calculate the gradients\n","            loss.backward()\n","\n","            # clip the gradients to avoid exploding gradients\n","            if GRADIENT_CLIP > 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n","\n","            # update the weights\n","            optimizer.step()\n","\n","        total_loss += loss.data.item()\n","\n","    return total_loss / len(dataloader)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"2yimq9Xg-fCP"},"outputs":[],"source":["def train(results_path = None):\n","\n","    global RESULTS_PATH, MODEL_PATH\n","    global best_eval_loss, best_train_loss, best_model_epoch, train_losses, eval_losses\n","\n","    if results_path is None:\n","        RESULTS_PATH = os.path.join('results', time.strftime(\"%Y%m%d_%H%M%S\"))\n","    else:\n","        RESULTS_PATH = results_path\n","    \n","    if not os.path.exists(RESULTS_PATH):\n","        os.makedirs(RESULTS_PATH)\n","\n","    MODEL_PATH = os.path.join(RESULTS_PATH, 'model_state_dict.pth')\n","\n","    best_eval_loss = 1e8\n","    best_train_loss = 1e8\n","    best_model_epoch = 0\n","    eval_losses = []\n","    train_losses = []\n","    lr = LEARNING_RATE\n","\n","    for epoch in range(1, EPOCHS+1):\n","\n","        start_time = time.time()\n","\n","        train_loss = epoch_step(train_dataloader, 'train')\n","\n","        eval_loss = epoch_step(eval_dataloader, 'eval')\n","\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if eval_loss < best_eval_loss:\n","            torch.save(model.state_dict(), MODEL_PATH)\n","            best_eval_loss = eval_loss\n","            best_model_epoch = epoch\n","\n","        if train_loss < best_train_loss:\n","            best_train_loss = train_loss\n","\n","        # # Anneal the learning rate if the validation loss plateaus\n","        # if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n","        #     lr = lr / 2.\n","        #     if lr < 0.1:\n","        #         lr = 2\n","        #     for param_group in optimizer.param_groups:\n","        #         param_group['lr'] = lr\n","\n","\n","        eval_losses.append(eval_loss)\n","        train_losses.append(train_loss)\n","\n","        # Early stopping\n","        if EARLY_STOP:\n","          if epoch > 15:\n","              if min(eval_losses[-15:]) > best_eval_loss:\n","                  break\n","\n","        # print the loss and the progress\n","        elapsed = time.time() - start_time\n","        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n","                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n","\n","\n","    print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d} \\n\\n' \\\n","            .format(best_eval_loss, best_model_epoch))\n","\n","\n","    # test the model\n","    global test_loss\n","    test_loss = epoch_step(test_dataloader, 'eval')\n","    print(f'\\n\\nTEST LOSS: {test_loss}')\n","\n","    save_parameters()"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2641510,"status":"ok","timestamp":1720355202428,"user":{"displayName":"Marco Bortolotti","userId":"16205829590380891051"},"user_tz":-120},"id":"EN4N3Ob3a4Gh","outputId":"e089bc58-d5ba-4738-b140-2a9897a5001a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/marco_bortolotti/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n","  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n","/home/marco_bortolotti/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv1d(input, weight, bias, self.stride,\n"]},{"name":"stdout","output_type":"stream","text":["| epoch   1/500 | lr 2.00000 | ms/epoch 12016.55388 | train_loss  3.40 | eval_loss  3.44\n","| epoch   2/500 | lr 2.00000 | ms/epoch 9182.69706 | train_loss  3.31 | eval_loss  3.46\n","| epoch   3/500 | lr 2.00000 | ms/epoch 9375.64325 | train_loss  3.24 | eval_loss  3.37\n","| epoch   4/500 | lr 2.00000 | ms/epoch 9157.55773 | train_loss  3.16 | eval_loss  3.14\n","| epoch   5/500 | lr 2.00000 | ms/epoch 9452.27933 | train_loss  2.85 | eval_loss  2.89\n","| epoch   6/500 | lr 2.00000 | ms/epoch 9329.99206 | train_loss  2.69 | eval_loss  3.04\n","| epoch   7/500 | lr 2.00000 | ms/epoch 9357.37634 | train_loss  2.64 | eval_loss  2.79\n","| epoch   8/500 | lr 2.00000 | ms/epoch 8954.49948 | train_loss  2.61 | eval_loss  2.83\n","| epoch   9/500 | lr 2.00000 | ms/epoch 8876.15395 | train_loss  2.59 | eval_loss  2.87\n","| epoch  10/500 | lr 2.00000 | ms/epoch 9139.60004 | train_loss  2.59 | eval_loss  2.84\n","| epoch  11/500 | lr 2.00000 | ms/epoch 9495.34225 | train_loss  2.57 | eval_loss  2.78\n","| epoch  12/500 | lr 2.00000 | ms/epoch 9454.62608 | train_loss  2.57 | eval_loss  2.91\n","| epoch  13/500 | lr 2.00000 | ms/epoch 9591.32314 | train_loss  2.56 | eval_loss  2.82\n","| epoch  14/500 | lr 2.00000 | ms/epoch 9404.29568 | train_loss  2.55 | eval_loss  2.78\n","| epoch  15/500 | lr 2.00000 | ms/epoch 9541.54468 | train_loss  2.54 | eval_loss  2.78\n","| epoch  16/500 | lr 2.00000 | ms/epoch 9546.50688 | train_loss  2.54 | eval_loss  2.85\n","| epoch  17/500 | lr 2.00000 | ms/epoch 8951.15495 | train_loss  2.53 | eval_loss  2.78\n","| epoch  18/500 | lr 2.00000 | ms/epoch 8851.72391 | train_loss  2.50 | eval_loss  3.14\n","| epoch  19/500 | lr 2.00000 | ms/epoch 9044.33846 | train_loss  2.51 | eval_loss  2.76\n","| epoch  20/500 | lr 2.00000 | ms/epoch 8720.64614 | train_loss  2.49 | eval_loss  2.79\n","| epoch  21/500 | lr 2.00000 | ms/epoch 8979.50625 | train_loss  2.47 | eval_loss  2.88\n","| epoch  22/500 | lr 2.00000 | ms/epoch 8722.47338 | train_loss  2.47 | eval_loss  2.82\n","| epoch  23/500 | lr 2.00000 | ms/epoch 8837.36777 | train_loss  2.47 | eval_loss  2.76\n","| epoch  24/500 | lr 2.00000 | ms/epoch 8692.80744 | train_loss  2.46 | eval_loss  2.82\n","| epoch  25/500 | lr 2.00000 | ms/epoch 8791.58711 | train_loss  2.45 | eval_loss  2.77\n","| epoch  26/500 | lr 2.00000 | ms/epoch 7500.58007 | train_loss  2.45 | eval_loss  2.76\n","| epoch  27/500 | lr 2.00000 | ms/epoch 4318.59970 | train_loss  2.44 | eval_loss  2.77\n","| epoch  28/500 | lr 2.00000 | ms/epoch 4353.38831 | train_loss  2.44 | eval_loss  2.92\n","| epoch  29/500 | lr 2.00000 | ms/epoch 4316.33639 | train_loss  2.44 | eval_loss  2.88\n","| epoch  30/500 | lr 2.00000 | ms/epoch 4285.28833 | train_loss  2.43 | eval_loss  2.77\n","| epoch  31/500 | lr 2.00000 | ms/epoch 4298.20561 | train_loss  2.43 | eval_loss  2.78\n","| epoch  32/500 | lr 2.00000 | ms/epoch 4276.99542 | train_loss  2.43 | eval_loss  2.77\n","| epoch  33/500 | lr 2.00000 | ms/epoch 4289.25085 | train_loss  2.43 | eval_loss  2.80\n","| epoch  34/500 | lr 2.00000 | ms/epoch 8236.56082 | train_loss  2.42 | eval_loss  2.75\n","| epoch  35/500 | lr 2.00000 | ms/epoch 8914.50286 | train_loss  2.45 | eval_loss  2.80\n","| epoch  36/500 | lr 2.00000 | ms/epoch 9191.40196 | train_loss  2.42 | eval_loss  2.80\n","| epoch  37/500 | lr 2.00000 | ms/epoch 9415.13300 | train_loss  2.42 | eval_loss  2.75\n","| epoch  38/500 | lr 2.00000 | ms/epoch 9151.12782 | train_loss  2.41 | eval_loss  2.83\n","| epoch  39/500 | lr 2.00000 | ms/epoch 9164.93058 | train_loss  2.42 | eval_loss  2.86\n","| epoch  40/500 | lr 2.00000 | ms/epoch 9243.84546 | train_loss  2.41 | eval_loss  2.79\n","| epoch  41/500 | lr 2.00000 | ms/epoch 9489.35008 | train_loss  2.41 | eval_loss  2.80\n","| epoch  42/500 | lr 2.00000 | ms/epoch 9068.37678 | train_loss  2.41 | eval_loss  2.75\n","| epoch  43/500 | lr 2.00000 | ms/epoch 9146.58880 | train_loss  2.41 | eval_loss  2.84\n","| epoch  44/500 | lr 2.00000 | ms/epoch 8981.26101 | train_loss  2.42 | eval_loss  2.79\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m EMPHASIZE_EEG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model, criterion, optimizer \u001b[38;5;241m=\u001b[39m initialize_model()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m FEEDBACK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m EMPHASIZE_EEG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(results_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     25\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 27\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mepoch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m epoch_step(eval_dataloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Save the model if the validation loss is the best we've seen so far.\u001b[39;00m\n","Cell \u001b[0;32mIn[19], line 33\u001b[0m, in \u001b[0;36mepoch_step\u001b[0;34m(dataloader, mode)\u001b[0m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# make the prediction\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m[:, :INPUT_TOK\u001b[38;5;241m.\u001b[39mSEQ_LENGTH]\n\u001b[1;32m     34\u001b[0m prev_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;66;03m# batch, seq_len (hidden units), vocab_size\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# flatten the output sequence\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# NB: the size -1 is inferred from other dimensions\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/project/Affective-AI-Music-Improviser/TCN/../APPLICATION/model/model.py:45\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, input, eeg_class)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Input ought to have dimension (N, C_in, L_in), where L_in is the seq_len; here the input is (N, L, C)\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m---> 45\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     46\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(y)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\u001b[38;5;241m.\u001b[39mcontiguous()\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/project/Affective-AI-Music-Improviser/TCN/../APPLICATION/model/model.py:109\u001b[0m, in \u001b[0;36mTemporalConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/project/Affective-AI-Music-Improviser/TCN/../APPLICATION/model/model.py:89\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 89\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1535\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1535\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# MODEL PARAMETERS\n","TRAIN_MODEL = True\n","\n","EPOCHS = 500 # 500\n","LEARNING_RATE = 2 # 4\n","BATCH_SIZE = 4 # 16\n","EARLY_STOP = True\n","\n","train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","\n","FEEDBACK = False\n","EMPHASIZE_EEG = False\n","model, criterion, optimizer = initialize_model()\n","train('results/model')\n","\n","FEEDBACK = False\n","EMPHASIZE_EEG = True\n","model, criterion, optimizer = initialize_model()\n","train('results/model_EEG')\n","\n","FEEDBACK = True\n","EMPHASIZE_EEG = False\n","model, criterion, optimizer = initialize_model()\n","train('results/model_feedback')\n","\n","FEEDBACK = True\n","EMPHASIZE_EEG = True\n","model, criterion, optimizer = initialize_model()\n","train('results/model_EEG_feedback')\n","\n","# if TRAIN_MODEL:\n","\n","#   for i in range(2):\n","\n","#     if i == 0:\n","#       FEEDBACK = False\n","#     else:\n","#       FEEDBACK = True\n","\n","#     BATCH_SIZE = 4\n","#     LEARNING_RATE = 1.0\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     LEARNING_RATE = 2.0\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     LEARNING_RATE = 4.0\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     LEARNING_RATE = 1.0\n","#     BATCH_SIZE = 8\n","#     train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     BATCH_SIZE = 16\n","#     train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","#     model, criterion, optimizer = initialize_model()\n","#     train()\n","\n","#     BATCH_SIZE = 32\n","#     train_dataloader, eval_dataloader, test_dataloader = initialize_dataset()\n","#     model, criterion, optimizer = initialize_model()\n","#     train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/marc0bortolotti/Affective-AI-Music-Improviser/blob/main/TCN/training.ipynb","timestamp":1720018338190}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
