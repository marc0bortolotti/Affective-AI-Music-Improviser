{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from MIDI.PRETTY_MIDI.pretty_midi_tokenization import PrettyMidiTokenizer, SILENCE_TOKEN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "DIRECTORY_PATH = './'\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "MODEL_PATH = os.path.join(DIRECTORY_PATH, 'model/generated_model.pth')\n",
    "RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results')\n",
    "\n",
    "\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 4\n",
    "BATCH_SIZE = 2\n",
    "TRAIN_MODEL = True\n",
    "SPLIT_DATASET = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 1\n",
      "Number of output files: 1\n",
      "\n",
      "\n",
      "1: drum_excited.MID -> bass_example.MID\n",
      "\n",
      "Number of input bars: 24\n",
      "Number of input sequences: 20\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 13\n",
      "\n",
      "Number of output bars: 11\n",
      "Number of output sequences: 7\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 30\n",
      "\n",
      "Number of sequences after truncation: 7, 7\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as input_#.mid and output_#.mid in the corresponding folders\n",
    "'''\n",
    "\n",
    "input_filenames = glob.glob(os.path.join(DATASET_PATH, 'input/*.MID'))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = glob.glob(os.path.join(DATASET_PATH, 'output/*.MID'))\n",
    "print('Number of output files:', len(output_filenames))\n",
    "\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'\\n\\n{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    INPUT_TOK = PrettyMidiTokenizer(in_file)\n",
    "    print(f'\\nNumber of input bars: {INPUT_TOK.num_bars}')\n",
    "    print(f'Number of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "    print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "    print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "\n",
    "    OUTPUT_TOK = PrettyMidiTokenizer(out_file)\n",
    "    print(f'\\nNumber of output bars: {OUTPUT_TOK.num_bars}')\n",
    "    print(f'Number of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "    print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "    print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "    # force the sequences to be the same length\n",
    "    min_length = min(len(INPUT_TOK.sequences), len(OUTPUT_TOK.sequences))\n",
    "    INPUT_TOK.sequences = INPUT_TOK.sequences[:min_length]\n",
    "    OUTPUT_TOK.sequences = OUTPUT_TOK.sequences[:min_length]\n",
    "    print(f'\\nNumber of sequences after truncation: {len(INPUT_TOK.sequences)}, {len(OUTPUT_TOK.sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "if SPLIT_DATASET:\n",
    "    # Split the dataset into training, evaluation and test sets\n",
    "    train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "    # Create the dataloaders\n",
    "    train_sampler = RandomSampler(train_set)          \n",
    "    train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    eval_sampler = RandomSampler(eval_set)\n",
    "    eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    test_sampler = RandomSampler(test_set)\n",
    "    test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "else:\n",
    "    train_set = dataset\n",
    "    train_sampler = RandomSampler(train_set)          \n",
    "    train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    eval_dataloader = None\n",
    "    test_dataloader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111 \n",
    "OUTPUT_VOCAB_SIZE = len(OUTPUT_TOK.VOCAB)\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] vectors (see model.py)\n",
    "LEVELS = 7\n",
    "HIDDEN_UNITS = 192\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# reduce the weights of the silence token since it is overrepresented in the dataset\n",
    "silence_id = OUTPUT_TOK.VOCAB.word2idx[SILENCE_TOKEN]\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_VOCAB_SIZE], dtype=torch.float)\n",
    "LOSS_WEIGTHS[silence_id] = 0.3\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_VOCAB_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data \n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(data_masked)\n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "        final_target = targets.contiguous().view(-1)\n",
    "        final_output = output.contiguous().view(-1, OUTPUT_VOCAB_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        if GRADIENT_CLIP > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1/500 | lr 4.00000 | ms/epoch 360.66103 | train_loss  1.80 | eval_loss  0.00\n",
      "| epoch   2/500 | lr 4.00000 | ms/epoch 323.57669 | train_loss  1.73 | eval_loss  0.00\n",
      "| epoch   3/500 | lr 4.00000 | ms/epoch 354.03943 | train_loss  1.73 | eval_loss  0.00\n",
      "| epoch   4/500 | lr 4.00000 | ms/epoch 341.17174 | train_loss  1.62 | eval_loss  0.00\n",
      "| epoch   5/500 | lr 4.00000 | ms/epoch 419.92021 | train_loss  1.70 | eval_loss  0.00\n",
      "| epoch   6/500 | lr 4.00000 | ms/epoch 365.63921 | train_loss  1.70 | eval_loss  0.00\n",
      "| epoch   7/500 | lr 4.00000 | ms/epoch 313.07602 | train_loss  1.55 | eval_loss  0.00\n",
      "| epoch   8/500 | lr 4.00000 | ms/epoch 390.13481 | train_loss  1.63 | eval_loss  0.00\n",
      "| epoch   9/500 | lr 4.00000 | ms/epoch 453.64738 | train_loss  1.62 | eval_loss  0.00\n",
      "| epoch  10/500 | lr 4.00000 | ms/epoch 569.15903 | train_loss  1.65 | eval_loss  0.00\n",
      "| epoch  11/500 | lr 4.00000 | ms/epoch 572.19148 | train_loss  1.66 | eval_loss  0.00\n",
      "| epoch  12/500 | lr 4.00000 | ms/epoch 495.38875 | train_loss  1.65 | eval_loss  0.00\n",
      "| epoch  13/500 | lr 4.00000 | ms/epoch 477.27060 | train_loss  1.56 | eval_loss  0.00\n",
      "| epoch  14/500 | lr 4.00000 | ms/epoch 497.69211 | train_loss  1.55 | eval_loss  0.00\n",
      "| epoch  15/500 | lr 4.00000 | ms/epoch 545.38417 | train_loss  1.58 | eval_loss  0.00\n",
      "| epoch  16/500 | lr 4.00000 | ms/epoch 760.43177 | train_loss  1.49 | eval_loss  0.00\n",
      "| epoch  17/500 | lr 4.00000 | ms/epoch 838.41610 | train_loss  1.57 | eval_loss  0.00\n",
      "| epoch  18/500 | lr 4.00000 | ms/epoch 869.51995 | train_loss  1.47 | eval_loss  0.00\n",
      "| epoch  19/500 | lr 4.00000 | ms/epoch 758.66485 | train_loss  1.52 | eval_loss  0.00\n",
      "| epoch  20/500 | lr 4.00000 | ms/epoch 882.90215 | train_loss  1.51 | eval_loss  0.00\n",
      "| epoch  21/500 | lr 4.00000 | ms/epoch 1047.60122 | train_loss  1.44 | eval_loss  0.00\n",
      "| epoch  22/500 | lr 4.00000 | ms/epoch 801.63646 | train_loss  1.44 | eval_loss  0.00\n",
      "| epoch  23/500 | lr 4.00000 | ms/epoch 820.35661 | train_loss  1.38 | eval_loss  0.00\n",
      "| epoch  24/500 | lr 4.00000 | ms/epoch 620.26882 | train_loss  1.45 | eval_loss  0.00\n",
      "| epoch  25/500 | lr 4.00000 | ms/epoch 632.60579 | train_loss  1.40 | eval_loss  0.00\n",
      "| epoch  26/500 | lr 4.00000 | ms/epoch 1525.00796 | train_loss  1.35 | eval_loss  0.00\n",
      "| epoch  27/500 | lr 4.00000 | ms/epoch 957.84068 | train_loss  1.37 | eval_loss  0.00\n",
      "| epoch  28/500 | lr 4.00000 | ms/epoch 673.83647 | train_loss  1.32 | eval_loss  0.00\n",
      "| epoch  29/500 | lr 4.00000 | ms/epoch 777.21334 | train_loss  1.37 | eval_loss  0.00\n",
      "| epoch  30/500 | lr 4.00000 | ms/epoch 1135.76317 | train_loss  1.38 | eval_loss  0.00\n",
      "| epoch  31/500 | lr 4.00000 | ms/epoch 965.26766 | train_loss  1.32 | eval_loss  0.00\n",
      "| epoch  32/500 | lr 4.00000 | ms/epoch 675.37594 | train_loss  1.33 | eval_loss  0.00\n",
      "| epoch  33/500 | lr 4.00000 | ms/epoch 583.91809 | train_loss  1.30 | eval_loss  0.00\n",
      "| epoch  34/500 | lr 4.00000 | ms/epoch 696.16270 | train_loss  1.28 | eval_loss  0.00\n",
      "| epoch  35/500 | lr 4.00000 | ms/epoch 643.05735 | train_loss  1.30 | eval_loss  0.00\n",
      "| epoch  36/500 | lr 4.00000 | ms/epoch 629.17638 | train_loss  1.25 | eval_loss  0.00\n",
      "| epoch  37/500 | lr 4.00000 | ms/epoch 599.68686 | train_loss  1.27 | eval_loss  0.00\n",
      "| epoch  38/500 | lr 4.00000 | ms/epoch 810.23502 | train_loss  1.24 | eval_loss  0.00\n",
      "| epoch  39/500 | lr 4.00000 | ms/epoch 634.02152 | train_loss  1.25 | eval_loss  0.00\n",
      "| epoch  40/500 | lr 4.00000 | ms/epoch 580.62148 | train_loss  1.17 | eval_loss  0.00\n",
      "| epoch  41/500 | lr 4.00000 | ms/epoch 609.66372 | train_loss  1.12 | eval_loss  0.00\n",
      "| epoch  42/500 | lr 4.00000 | ms/epoch 593.38260 | train_loss  1.09 | eval_loss  0.00\n",
      "| epoch  43/500 | lr 4.00000 | ms/epoch 663.53226 | train_loss  1.17 | eval_loss  0.00\n",
      "| epoch  44/500 | lr 4.00000 | ms/epoch 686.62262 | train_loss  1.10 | eval_loss  0.00\n",
      "| epoch  45/500 | lr 4.00000 | ms/epoch 668.96033 | train_loss  1.12 | eval_loss  0.00\n",
      "| epoch  46/500 | lr 4.00000 | ms/epoch 585.96277 | train_loss  1.11 | eval_loss  0.00\n",
      "| epoch  47/500 | lr 4.00000 | ms/epoch 569.57889 | train_loss  1.12 | eval_loss  0.00\n",
      "| epoch  48/500 | lr 4.00000 | ms/epoch 632.54380 | train_loss  1.14 | eval_loss  0.00\n",
      "| epoch  49/500 | lr 4.00000 | ms/epoch 807.55663 | train_loss  1.06 | eval_loss  0.00\n",
      "| epoch  50/500 | lr 4.00000 | ms/epoch 830.78504 | train_loss  1.10 | eval_loss  0.00\n",
      "| epoch  51/500 | lr 4.00000 | ms/epoch 844.12813 | train_loss  1.01 | eval_loss  0.00\n",
      "| epoch  52/500 | lr 4.00000 | ms/epoch 804.93569 | train_loss  1.02 | eval_loss  0.00\n",
      "| epoch  53/500 | lr 4.00000 | ms/epoch 799.61562 | train_loss  1.02 | eval_loss  0.00\n",
      "| epoch  54/500 | lr 4.00000 | ms/epoch 692.33131 | train_loss  1.00 | eval_loss  0.00\n",
      "| epoch  55/500 | lr 4.00000 | ms/epoch 626.90544 | train_loss  0.95 | eval_loss  0.00\n",
      "| epoch  56/500 | lr 4.00000 | ms/epoch 659.36470 | train_loss  1.02 | eval_loss  0.00\n",
      "| epoch  57/500 | lr 4.00000 | ms/epoch 609.90024 | train_loss  0.96 | eval_loss  0.00\n",
      "| epoch  58/500 | lr 4.00000 | ms/epoch 615.74507 | train_loss  0.94 | eval_loss  0.00\n",
      "| epoch  59/500 | lr 4.00000 | ms/epoch 551.40948 | train_loss  0.94 | eval_loss  0.00\n",
      "| epoch  60/500 | lr 4.00000 | ms/epoch 633.93307 | train_loss  0.91 | eval_loss  0.00\n",
      "| epoch  61/500 | lr 4.00000 | ms/epoch 646.94500 | train_loss  0.98 | eval_loss  0.00\n",
      "| epoch  62/500 | lr 4.00000 | ms/epoch 603.68633 | train_loss  0.92 | eval_loss  0.00\n",
      "| epoch  63/500 | lr 4.00000 | ms/epoch 587.26335 | train_loss  0.93 | eval_loss  0.00\n",
      "| epoch  64/500 | lr 4.00000 | ms/epoch 570.10102 | train_loss  0.89 | eval_loss  0.00\n",
      "| epoch  65/500 | lr 4.00000 | ms/epoch 569.45276 | train_loss  0.86 | eval_loss  0.00\n",
      "| epoch  66/500 | lr 4.00000 | ms/epoch 621.48547 | train_loss  0.87 | eval_loss  0.00\n",
      "| epoch  67/500 | lr 4.00000 | ms/epoch 609.28941 | train_loss  0.84 | eval_loss  0.00\n",
      "| epoch  68/500 | lr 4.00000 | ms/epoch 612.93340 | train_loss  0.83 | eval_loss  0.00\n",
      "| epoch  69/500 | lr 4.00000 | ms/epoch 651.07179 | train_loss  0.82 | eval_loss  0.00\n",
      "| epoch  70/500 | lr 4.00000 | ms/epoch 583.88495 | train_loss  0.79 | eval_loss  0.00\n",
      "| epoch  71/500 | lr 4.00000 | ms/epoch 610.94785 | train_loss  0.87 | eval_loss  0.00\n",
      "| epoch  72/500 | lr 4.00000 | ms/epoch 657.76539 | train_loss  0.81 | eval_loss  0.00\n",
      "| epoch  73/500 | lr 4.00000 | ms/epoch 603.65152 | train_loss  0.76 | eval_loss  0.00\n",
      "| epoch  74/500 | lr 4.00000 | ms/epoch 636.30915 | train_loss  0.83 | eval_loss  0.00\n",
      "| epoch  75/500 | lr 4.00000 | ms/epoch 599.78986 | train_loss  0.78 | eval_loss  0.00\n",
      "| epoch  76/500 | lr 4.00000 | ms/epoch 738.29412 | train_loss  0.76 | eval_loss  0.00\n",
      "| epoch  77/500 | lr 4.00000 | ms/epoch 611.98592 | train_loss  0.76 | eval_loss  0.00\n",
      "| epoch  78/500 | lr 4.00000 | ms/epoch 576.35570 | train_loss  0.76 | eval_loss  0.00\n",
      "| epoch  79/500 | lr 4.00000 | ms/epoch 755.87058 | train_loss  0.76 | eval_loss  0.00\n",
      "| epoch  80/500 | lr 4.00000 | ms/epoch 923.03085 | train_loss  0.71 | eval_loss  0.00\n",
      "| epoch  81/500 | lr 4.00000 | ms/epoch 634.98378 | train_loss  0.70 | eval_loss  0.00\n",
      "| epoch  82/500 | lr 4.00000 | ms/epoch 724.86925 | train_loss  0.72 | eval_loss  0.00\n",
      "| epoch  83/500 | lr 4.00000 | ms/epoch 679.24762 | train_loss  0.72 | eval_loss  0.00\n",
      "| epoch  84/500 | lr 4.00000 | ms/epoch 798.98667 | train_loss  0.72 | eval_loss  0.00\n",
      "| epoch  85/500 | lr 4.00000 | ms/epoch 668.93435 | train_loss  0.66 | eval_loss  0.00\n",
      "| epoch  86/500 | lr 4.00000 | ms/epoch 632.17187 | train_loss  0.65 | eval_loss  0.00\n",
      "| epoch  87/500 | lr 4.00000 | ms/epoch 1104.88033 | train_loss  0.67 | eval_loss  0.00\n",
      "| epoch  88/500 | lr 4.00000 | ms/epoch 919.81983 | train_loss  0.60 | eval_loss  0.00\n",
      "| epoch  89/500 | lr 4.00000 | ms/epoch 823.41695 | train_loss  0.69 | eval_loss  0.00\n",
      "| epoch  90/500 | lr 4.00000 | ms/epoch 740.20982 | train_loss  0.66 | eval_loss  0.00\n",
      "| epoch  91/500 | lr 4.00000 | ms/epoch 658.03051 | train_loss  0.67 | eval_loss  0.00\n",
      "| epoch  92/500 | lr 4.00000 | ms/epoch 1161.49259 | train_loss  0.63 | eval_loss  0.00\n",
      "| epoch  93/500 | lr 4.00000 | ms/epoch 740.65089 | train_loss  0.62 | eval_loss  0.00\n",
      "| epoch  94/500 | lr 4.00000 | ms/epoch 645.87307 | train_loss  0.63 | eval_loss  0.00\n",
      "| epoch  95/500 | lr 4.00000 | ms/epoch 695.80746 | train_loss  0.58 | eval_loss  0.00\n",
      "| epoch  96/500 | lr 4.00000 | ms/epoch 701.37334 | train_loss  0.61 | eval_loss  0.00\n",
      "| epoch  97/500 | lr 4.00000 | ms/epoch 632.10583 | train_loss  0.60 | eval_loss  0.00\n",
      "| epoch  98/500 | lr 4.00000 | ms/epoch 569.51976 | train_loss  0.57 | eval_loss  0.00\n",
      "| epoch  99/500 | lr 4.00000 | ms/epoch 542.23680 | train_loss  0.56 | eval_loss  0.00\n",
      "| epoch 100/500 | lr 4.00000 | ms/epoch 554.41356 | train_loss  0.61 | eval_loss  0.00\n",
      "| epoch 101/500 | lr 4.00000 | ms/epoch 612.31375 | train_loss  0.55 | eval_loss  0.00\n",
      "| epoch 102/500 | lr 4.00000 | ms/epoch 558.87341 | train_loss  0.49 | eval_loss  0.00\n",
      "| epoch 103/500 | lr 4.00000 | ms/epoch 560.72450 | train_loss  0.53 | eval_loss  0.00\n",
      "| epoch 104/500 | lr 4.00000 | ms/epoch 548.48337 | train_loss  0.56 | eval_loss  0.00\n",
      "| epoch 105/500 | lr 4.00000 | ms/epoch 506.50358 | train_loss  0.59 | eval_loss  0.00\n",
      "| epoch 106/500 | lr 4.00000 | ms/epoch 554.33202 | train_loss  0.52 | eval_loss  0.00\n",
      "| epoch 107/500 | lr 4.00000 | ms/epoch 477.67687 | train_loss  0.46 | eval_loss  0.00\n",
      "| epoch 108/500 | lr 4.00000 | ms/epoch 429.95238 | train_loss  0.51 | eval_loss  0.00\n",
      "| epoch 109/500 | lr 4.00000 | ms/epoch 461.04860 | train_loss  0.55 | eval_loss  0.00\n",
      "| epoch 110/500 | lr 4.00000 | ms/epoch 418.52379 | train_loss  0.49 | eval_loss  0.00\n",
      "| epoch 111/500 | lr 4.00000 | ms/epoch 422.64843 | train_loss  0.50 | eval_loss  0.00\n",
      "| epoch 112/500 | lr 4.00000 | ms/epoch 400.26951 | train_loss  0.49 | eval_loss  0.00\n",
      "| epoch 113/500 | lr 4.00000 | ms/epoch 385.86760 | train_loss  0.43 | eval_loss  0.00\n",
      "| epoch 114/500 | lr 4.00000 | ms/epoch 418.37311 | train_loss  0.45 | eval_loss  0.00\n",
      "| epoch 115/500 | lr 4.00000 | ms/epoch 413.68771 | train_loss  0.48 | eval_loss  0.00\n",
      "| epoch 116/500 | lr 4.00000 | ms/epoch 397.69363 | train_loss  0.50 | eval_loss  0.00\n",
      "| epoch 117/500 | lr 4.00000 | ms/epoch 389.65225 | train_loss  0.48 | eval_loss  0.00\n",
      "| epoch 118/500 | lr 4.00000 | ms/epoch 362.79559 | train_loss  0.47 | eval_loss  0.00\n",
      "| epoch 119/500 | lr 4.00000 | ms/epoch 369.35997 | train_loss  0.46 | eval_loss  0.00\n",
      "| epoch 120/500 | lr 4.00000 | ms/epoch 357.41401 | train_loss  0.50 | eval_loss  0.00\n",
      "| epoch 121/500 | lr 4.00000 | ms/epoch 356.69851 | train_loss  0.42 | eval_loss  0.00\n",
      "| epoch 122/500 | lr 4.00000 | ms/epoch 359.76505 | train_loss  0.43 | eval_loss  0.00\n",
      "| epoch 123/500 | lr 4.00000 | ms/epoch 369.75336 | train_loss  0.47 | eval_loss  0.00\n",
      "| epoch 124/500 | lr 4.00000 | ms/epoch 353.68776 | train_loss  0.47 | eval_loss  0.00\n",
      "| epoch 125/500 | lr 4.00000 | ms/epoch 376.40929 | train_loss  0.45 | eval_loss  0.00\n",
      "| epoch 126/500 | lr 4.00000 | ms/epoch 389.96935 | train_loss  0.46 | eval_loss  0.00\n",
      "| epoch 127/500 | lr 4.00000 | ms/epoch 390.45143 | train_loss  0.42 | eval_loss  0.00\n",
      "| epoch 128/500 | lr 4.00000 | ms/epoch 411.87143 | train_loss  0.43 | eval_loss  0.00\n",
      "| epoch 129/500 | lr 4.00000 | ms/epoch 552.98781 | train_loss  0.45 | eval_loss  0.00\n",
      "| epoch 130/500 | lr 4.00000 | ms/epoch 464.68210 | train_loss  0.41 | eval_loss  0.00\n",
      "| epoch 131/500 | lr 4.00000 | ms/epoch 452.74115 | train_loss  0.39 | eval_loss  0.00\n",
      "| epoch 132/500 | lr 4.00000 | ms/epoch 492.05208 | train_loss  0.40 | eval_loss  0.00\n",
      "| epoch 133/500 | lr 4.00000 | ms/epoch 414.06536 | train_loss  0.41 | eval_loss  0.00\n",
      "| epoch 134/500 | lr 4.00000 | ms/epoch 398.63300 | train_loss  0.38 | eval_loss  0.00\n",
      "| epoch 135/500 | lr 4.00000 | ms/epoch 396.86298 | train_loss  0.41 | eval_loss  0.00\n",
      "| epoch 136/500 | lr 4.00000 | ms/epoch 470.56866 | train_loss  0.43 | eval_loss  0.00\n",
      "| epoch 137/500 | lr 4.00000 | ms/epoch 405.43890 | train_loss  0.39 | eval_loss  0.00\n",
      "| epoch 138/500 | lr 4.00000 | ms/epoch 422.42360 | train_loss  0.38 | eval_loss  0.00\n",
      "| epoch 139/500 | lr 4.00000 | ms/epoch 397.44186 | train_loss  0.34 | eval_loss  0.00\n",
      "| epoch 140/500 | lr 4.00000 | ms/epoch 376.05667 | train_loss  0.44 | eval_loss  0.00\n",
      "| epoch 141/500 | lr 4.00000 | ms/epoch 369.63749 | train_loss  0.36 | eval_loss  0.00\n",
      "| epoch 142/500 | lr 4.00000 | ms/epoch 376.43504 | train_loss  0.34 | eval_loss  0.00\n",
      "| epoch 143/500 | lr 4.00000 | ms/epoch 369.09771 | train_loss  0.39 | eval_loss  0.00\n",
      "| epoch 144/500 | lr 4.00000 | ms/epoch 350.89517 | train_loss  0.38 | eval_loss  0.00\n",
      "| epoch 145/500 | lr 4.00000 | ms/epoch 351.45354 | train_loss  0.35 | eval_loss  0.00\n",
      "| epoch 146/500 | lr 4.00000 | ms/epoch 394.97304 | train_loss  0.35 | eval_loss  0.00\n",
      "| epoch 147/500 | lr 4.00000 | ms/epoch 384.15384 | train_loss  0.38 | eval_loss  0.00\n",
      "| epoch 148/500 | lr 4.00000 | ms/epoch 359.81417 | train_loss  0.32 | eval_loss  0.00\n",
      "| epoch 149/500 | lr 4.00000 | ms/epoch 390.75708 | train_loss  0.34 | eval_loss  0.00\n",
      "| epoch 150/500 | lr 4.00000 | ms/epoch 386.32727 | train_loss  0.36 | eval_loss  0.00\n",
      "| epoch 151/500 | lr 4.00000 | ms/epoch 382.09438 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 152/500 | lr 4.00000 | ms/epoch 415.97056 | train_loss  0.34 | eval_loss  0.00\n",
      "| epoch 153/500 | lr 4.00000 | ms/epoch 398.03576 | train_loss  0.33 | eval_loss  0.00\n",
      "| epoch 154/500 | lr 4.00000 | ms/epoch 794.88611 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 155/500 | lr 4.00000 | ms/epoch 660.77614 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 156/500 | lr 4.00000 | ms/epoch 548.87605 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 157/500 | lr 4.00000 | ms/epoch 401.81303 | train_loss  0.35 | eval_loss  0.00\n",
      "| epoch 158/500 | lr 4.00000 | ms/epoch 497.93315 | train_loss  0.35 | eval_loss  0.00\n",
      "| epoch 159/500 | lr 4.00000 | ms/epoch 410.92706 | train_loss  0.32 | eval_loss  0.00\n",
      "| epoch 160/500 | lr 4.00000 | ms/epoch 496.25421 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 161/500 | lr 4.00000 | ms/epoch 474.17927 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 162/500 | lr 4.00000 | ms/epoch 415.65728 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 163/500 | lr 4.00000 | ms/epoch 428.11966 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 164/500 | lr 4.00000 | ms/epoch 543.60223 | train_loss  0.32 | eval_loss  0.00\n",
      "| epoch 165/500 | lr 4.00000 | ms/epoch 571.47765 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 166/500 | lr 4.00000 | ms/epoch 474.26224 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 167/500 | lr 4.00000 | ms/epoch 462.53705 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 168/500 | lr 4.00000 | ms/epoch 447.58415 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 169/500 | lr 4.00000 | ms/epoch 447.79944 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 170/500 | lr 4.00000 | ms/epoch 467.30542 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 171/500 | lr 4.00000 | ms/epoch 532.99212 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 172/500 | lr 4.00000 | ms/epoch 498.25549 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 173/500 | lr 4.00000 | ms/epoch 514.58120 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 174/500 | lr 4.00000 | ms/epoch 499.82405 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 175/500 | lr 4.00000 | ms/epoch 617.41328 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 176/500 | lr 4.00000 | ms/epoch 602.92673 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 177/500 | lr 4.00000 | ms/epoch 715.23046 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 178/500 | lr 4.00000 | ms/epoch 2320.74094 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 179/500 | lr 4.00000 | ms/epoch 1340.13009 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 180/500 | lr 4.00000 | ms/epoch 866.82415 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 181/500 | lr 4.00000 | ms/epoch 1208.28795 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 182/500 | lr 4.00000 | ms/epoch 1007.22241 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 183/500 | lr 4.00000 | ms/epoch 1172.87302 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 184/500 | lr 4.00000 | ms/epoch 886.56163 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 185/500 | lr 4.00000 | ms/epoch 844.28453 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 186/500 | lr 4.00000 | ms/epoch 974.06721 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 187/500 | lr 4.00000 | ms/epoch 735.46219 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 188/500 | lr 4.00000 | ms/epoch 603.34396 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 189/500 | lr 4.00000 | ms/epoch 570.33110 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 190/500 | lr 4.00000 | ms/epoch 1367.92064 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 191/500 | lr 4.00000 | ms/epoch 1019.72556 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 192/500 | lr 4.00000 | ms/epoch 1123.62576 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 193/500 | lr 4.00000 | ms/epoch 716.21513 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 194/500 | lr 4.00000 | ms/epoch 573.03333 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 195/500 | lr 4.00000 | ms/epoch 580.51848 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 196/500 | lr 4.00000 | ms/epoch 703.24612 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 197/500 | lr 4.00000 | ms/epoch 647.24302 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 198/500 | lr 4.00000 | ms/epoch 655.93934 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 199/500 | lr 4.00000 | ms/epoch 1002.65312 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 200/500 | lr 4.00000 | ms/epoch 941.18667 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 201/500 | lr 4.00000 | ms/epoch 862.16521 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 202/500 | lr 4.00000 | ms/epoch 676.03922 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 203/500 | lr 4.00000 | ms/epoch 581.22706 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 204/500 | lr 4.00000 | ms/epoch 1176.65505 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 205/500 | lr 4.00000 | ms/epoch 796.82803 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 206/500 | lr 4.00000 | ms/epoch 685.11224 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 207/500 | lr 4.00000 | ms/epoch 692.71827 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 208/500 | lr 4.00000 | ms/epoch 1531.50916 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 209/500 | lr 4.00000 | ms/epoch 1393.79382 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 210/500 | lr 4.00000 | ms/epoch 908.94222 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 211/500 | lr 4.00000 | ms/epoch 850.16179 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 212/500 | lr 4.00000 | ms/epoch 738.27696 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 213/500 | lr 4.00000 | ms/epoch 680.53341 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 214/500 | lr 4.00000 | ms/epoch 885.58769 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 215/500 | lr 4.00000 | ms/epoch 841.46619 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 216/500 | lr 4.00000 | ms/epoch 839.94603 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 217/500 | lr 4.00000 | ms/epoch 628.39150 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 218/500 | lr 4.00000 | ms/epoch 719.69533 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 219/500 | lr 4.00000 | ms/epoch 744.53640 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 220/500 | lr 4.00000 | ms/epoch 697.66879 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 221/500 | lr 4.00000 | ms/epoch 1220.79086 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 222/500 | lr 4.00000 | ms/epoch 657.01556 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 223/500 | lr 4.00000 | ms/epoch 626.38116 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 224/500 | lr 4.00000 | ms/epoch 640.33127 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 225/500 | lr 4.00000 | ms/epoch 701.12371 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 226/500 | lr 4.00000 | ms/epoch 2128.76177 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 227/500 | lr 4.00000 | ms/epoch 1666.61501 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 228/500 | lr 4.00000 | ms/epoch 1234.67398 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 229/500 | lr 4.00000 | ms/epoch 1241.08410 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 230/500 | lr 4.00000 | ms/epoch 1130.90849 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 231/500 | lr 4.00000 | ms/epoch 607.57732 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 232/500 | lr 4.00000 | ms/epoch 725.03066 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 233/500 | lr 4.00000 | ms/epoch 699.66412 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 234/500 | lr 4.00000 | ms/epoch 635.03790 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 235/500 | lr 4.00000 | ms/epoch 621.25349 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 236/500 | lr 4.00000 | ms/epoch 618.69144 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 237/500 | lr 4.00000 | ms/epoch 620.93997 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 238/500 | lr 4.00000 | ms/epoch 628.03698 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 239/500 | lr 4.00000 | ms/epoch 596.86470 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 240/500 | lr 4.00000 | ms/epoch 815.20200 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 241/500 | lr 4.00000 | ms/epoch 607.70726 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 242/500 | lr 4.00000 | ms/epoch 580.09672 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 243/500 | lr 4.00000 | ms/epoch 598.70338 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 244/500 | lr 4.00000 | ms/epoch 629.51183 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 245/500 | lr 4.00000 | ms/epoch 708.29630 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 246/500 | lr 4.00000 | ms/epoch 625.11683 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 247/500 | lr 4.00000 | ms/epoch 640.24115 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 248/500 | lr 4.00000 | ms/epoch 670.16482 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 249/500 | lr 4.00000 | ms/epoch 636.65557 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 250/500 | lr 4.00000 | ms/epoch 625.90575 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 251/500 | lr 4.00000 | ms/epoch 722.63288 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 252/500 | lr 4.00000 | ms/epoch 666.73112 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 253/500 | lr 4.00000 | ms/epoch 701.32136 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 254/500 | lr 4.00000 | ms/epoch 718.64939 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 255/500 | lr 4.00000 | ms/epoch 680.13024 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 256/500 | lr 4.00000 | ms/epoch 763.74507 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 257/500 | lr 4.00000 | ms/epoch 914.85119 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 258/500 | lr 4.00000 | ms/epoch 724.18308 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 259/500 | lr 4.00000 | ms/epoch 1120.93091 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 260/500 | lr 4.00000 | ms/epoch 665.03072 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 261/500 | lr 4.00000 | ms/epoch 562.18004 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 262/500 | lr 4.00000 | ms/epoch 768.85796 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 263/500 | lr 4.00000 | ms/epoch 1377.92420 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 264/500 | lr 4.00000 | ms/epoch 868.18361 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 265/500 | lr 4.00000 | ms/epoch 702.51942 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 266/500 | lr 4.00000 | ms/epoch 667.32979 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 267/500 | lr 4.00000 | ms/epoch 535.01630 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 268/500 | lr 4.00000 | ms/epoch 662.88352 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 269/500 | lr 4.00000 | ms/epoch 780.37667 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 270/500 | lr 4.00000 | ms/epoch 1134.01008 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 271/500 | lr 4.00000 | ms/epoch 1368.49070 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 272/500 | lr 4.00000 | ms/epoch 992.21158 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 273/500 | lr 4.00000 | ms/epoch 639.51206 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 274/500 | lr 4.00000 | ms/epoch 857.88965 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 275/500 | lr 4.00000 | ms/epoch 657.46307 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 276/500 | lr 4.00000 | ms/epoch 752.40326 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 277/500 | lr 4.00000 | ms/epoch 606.51255 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 278/500 | lr 4.00000 | ms/epoch 667.55772 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 279/500 | lr 4.00000 | ms/epoch 608.35934 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 280/500 | lr 4.00000 | ms/epoch 614.56251 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 281/500 | lr 4.00000 | ms/epoch 678.82037 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 282/500 | lr 4.00000 | ms/epoch 562.21771 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 283/500 | lr 4.00000 | ms/epoch 556.60987 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 284/500 | lr 4.00000 | ms/epoch 576.41912 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 285/500 | lr 4.00000 | ms/epoch 491.20760 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 286/500 | lr 4.00000 | ms/epoch 473.84906 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 287/500 | lr 4.00000 | ms/epoch 498.62838 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 288/500 | lr 4.00000 | ms/epoch 469.82121 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 289/500 | lr 4.00000 | ms/epoch 439.91232 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 290/500 | lr 4.00000 | ms/epoch 425.82369 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 291/500 | lr 4.00000 | ms/epoch 435.84275 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 292/500 | lr 4.00000 | ms/epoch 413.41829 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 293/500 | lr 4.00000 | ms/epoch 399.01328 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 294/500 | lr 4.00000 | ms/epoch 390.87057 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 295/500 | lr 4.00000 | ms/epoch 405.24793 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 296/500 | lr 4.00000 | ms/epoch 372.78485 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 297/500 | lr 4.00000 | ms/epoch 369.90714 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 298/500 | lr 4.00000 | ms/epoch 375.72145 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 299/500 | lr 4.00000 | ms/epoch 362.52046 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 300/500 | lr 4.00000 | ms/epoch 401.77155 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 301/500 | lr 4.00000 | ms/epoch 402.78244 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 302/500 | lr 4.00000 | ms/epoch 549.86954 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 303/500 | lr 4.00000 | ms/epoch 377.07591 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 304/500 | lr 4.00000 | ms/epoch 367.76400 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 305/500 | lr 4.00000 | ms/epoch 369.67707 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 306/500 | lr 4.00000 | ms/epoch 364.07566 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 307/500 | lr 4.00000 | ms/epoch 358.31881 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 308/500 | lr 4.00000 | ms/epoch 375.30065 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 309/500 | lr 4.00000 | ms/epoch 357.55992 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 310/500 | lr 4.00000 | ms/epoch 385.73647 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 311/500 | lr 4.00000 | ms/epoch 409.82699 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 312/500 | lr 4.00000 | ms/epoch 421.00215 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 313/500 | lr 4.00000 | ms/epoch 363.32583 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 314/500 | lr 4.00000 | ms/epoch 380.19133 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 315/500 | lr 4.00000 | ms/epoch 399.56975 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 316/500 | lr 4.00000 | ms/epoch 426.16296 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 317/500 | lr 4.00000 | ms/epoch 418.93196 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 318/500 | lr 4.00000 | ms/epoch 405.02357 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 319/500 | lr 4.00000 | ms/epoch 399.17588 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 320/500 | lr 4.00000 | ms/epoch 361.78088 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 321/500 | lr 4.00000 | ms/epoch 383.14939 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 322/500 | lr 4.00000 | ms/epoch 385.20646 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 323/500 | lr 4.00000 | ms/epoch 389.57095 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 324/500 | lr 4.00000 | ms/epoch 405.04408 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 325/500 | lr 4.00000 | ms/epoch 378.19099 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 326/500 | lr 4.00000 | ms/epoch 406.56424 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 327/500 | lr 4.00000 | ms/epoch 538.37562 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 328/500 | lr 4.00000 | ms/epoch 420.08638 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 329/500 | lr 4.00000 | ms/epoch 472.60356 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 330/500 | lr 4.00000 | ms/epoch 668.76531 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 331/500 | lr 4.00000 | ms/epoch 522.28236 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 332/500 | lr 4.00000 | ms/epoch 474.74337 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 333/500 | lr 4.00000 | ms/epoch 432.59907 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 334/500 | lr 4.00000 | ms/epoch 404.83856 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 335/500 | lr 4.00000 | ms/epoch 412.60934 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 336/500 | lr 4.00000 | ms/epoch 430.18985 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 337/500 | lr 4.00000 | ms/epoch 501.25480 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 338/500 | lr 4.00000 | ms/epoch 527.43220 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 339/500 | lr 4.00000 | ms/epoch 531.29387 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 340/500 | lr 4.00000 | ms/epoch 509.25398 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 341/500 | lr 4.00000 | ms/epoch 539.90579 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 342/500 | lr 4.00000 | ms/epoch 509.57370 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 343/500 | lr 4.00000 | ms/epoch 560.13656 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 344/500 | lr 4.00000 | ms/epoch 587.52680 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 345/500 | lr 4.00000 | ms/epoch 534.82962 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 346/500 | lr 4.00000 | ms/epoch 485.04758 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 347/500 | lr 4.00000 | ms/epoch 486.97090 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 348/500 | lr 4.00000 | ms/epoch 512.98809 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 349/500 | lr 4.00000 | ms/epoch 498.86179 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 350/500 | lr 4.00000 | ms/epoch 475.79741 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 351/500 | lr 4.00000 | ms/epoch 495.55087 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 352/500 | lr 4.00000 | ms/epoch 499.95303 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 353/500 | lr 4.00000 | ms/epoch 609.87639 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 354/500 | lr 4.00000 | ms/epoch 554.09336 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 355/500 | lr 4.00000 | ms/epoch 564.73041 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 356/500 | lr 4.00000 | ms/epoch 595.06702 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 357/500 | lr 4.00000 | ms/epoch 582.71146 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 358/500 | lr 4.00000 | ms/epoch 627.13361 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 359/500 | lr 4.00000 | ms/epoch 630.16486 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 360/500 | lr 4.00000 | ms/epoch 641.84165 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 361/500 | lr 4.00000 | ms/epoch 590.37828 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 362/500 | lr 4.00000 | ms/epoch 613.64269 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 363/500 | lr 4.00000 | ms/epoch 654.18983 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 364/500 | lr 4.00000 | ms/epoch 621.01197 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 365/500 | lr 4.00000 | ms/epoch 666.65244 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 366/500 | lr 4.00000 | ms/epoch 573.52233 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 367/500 | lr 4.00000 | ms/epoch 604.86317 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 368/500 | lr 4.00000 | ms/epoch 604.55918 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 369/500 | lr 4.00000 | ms/epoch 604.50077 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 370/500 | lr 4.00000 | ms/epoch 611.04941 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 371/500 | lr 4.00000 | ms/epoch 646.40737 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 372/500 | lr 4.00000 | ms/epoch 587.30459 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 373/500 | lr 4.00000 | ms/epoch 623.22402 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 374/500 | lr 4.00000 | ms/epoch 621.63854 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 375/500 | lr 4.00000 | ms/epoch 610.01205 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 376/500 | lr 4.00000 | ms/epoch 648.38028 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 377/500 | lr 4.00000 | ms/epoch 752.64239 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 378/500 | lr 4.00000 | ms/epoch 1882.44677 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 379/500 | lr 4.00000 | ms/epoch 1241.44697 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 380/500 | lr 4.00000 | ms/epoch 1044.04688 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 381/500 | lr 4.00000 | ms/epoch 1092.97323 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 382/500 | lr 4.00000 | ms/epoch 1641.52384 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 383/500 | lr 4.00000 | ms/epoch 1131.87909 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 384/500 | lr 4.00000 | ms/epoch 1080.38521 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 385/500 | lr 4.00000 | ms/epoch 1304.92735 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 386/500 | lr 4.00000 | ms/epoch 1186.19609 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 387/500 | lr 4.00000 | ms/epoch 2482.84864 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 388/500 | lr 4.00000 | ms/epoch 3114.62259 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 389/500 | lr 4.00000 | ms/epoch 2124.90344 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 390/500 | lr 4.00000 | ms/epoch 1133.35252 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 391/500 | lr 4.00000 | ms/epoch 1049.50261 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 392/500 | lr 4.00000 | ms/epoch 874.81165 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 393/500 | lr 4.00000 | ms/epoch 666.38756 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 394/500 | lr 4.00000 | ms/epoch 721.72666 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 395/500 | lr 4.00000 | ms/epoch 871.67311 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 396/500 | lr 4.00000 | ms/epoch 827.71397 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 397/500 | lr 4.00000 | ms/epoch 991.79697 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 398/500 | lr 4.00000 | ms/epoch 885.01811 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 399/500 | lr 4.00000 | ms/epoch 672.98937 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 400/500 | lr 4.00000 | ms/epoch 656.66604 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 401/500 | lr 4.00000 | ms/epoch 904.89554 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 402/500 | lr 4.00000 | ms/epoch 999.04156 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 403/500 | lr 4.00000 | ms/epoch 909.40166 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 404/500 | lr 4.00000 | ms/epoch 1010.01453 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 405/500 | lr 4.00000 | ms/epoch 1139.21094 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 406/500 | lr 4.00000 | ms/epoch 1221.17448 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 407/500 | lr 4.00000 | ms/epoch 731.55975 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 408/500 | lr 4.00000 | ms/epoch 740.93390 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 409/500 | lr 4.00000 | ms/epoch 952.19803 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 410/500 | lr 4.00000 | ms/epoch 792.34028 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 411/500 | lr 4.00000 | ms/epoch 756.02031 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 412/500 | lr 4.00000 | ms/epoch 787.50849 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 413/500 | lr 4.00000 | ms/epoch 786.75699 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 414/500 | lr 4.00000 | ms/epoch 829.03767 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 415/500 | lr 4.00000 | ms/epoch 878.91889 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 416/500 | lr 4.00000 | ms/epoch 927.46663 | train_loss  0.06 | eval_loss  0.00\n",
      "| epoch 417/500 | lr 4.00000 | ms/epoch 888.43584 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 418/500 | lr 4.00000 | ms/epoch 793.76650 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 419/500 | lr 4.00000 | ms/epoch 707.07297 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 420/500 | lr 4.00000 | ms/epoch 827.46434 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 421/500 | lr 4.00000 | ms/epoch 805.98259 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 422/500 | lr 4.00000 | ms/epoch 809.33905 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 423/500 | lr 4.00000 | ms/epoch 770.49422 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 424/500 | lr 4.00000 | ms/epoch 839.86115 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 425/500 | lr 4.00000 | ms/epoch 706.35486 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 426/500 | lr 4.00000 | ms/epoch 1341.32910 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 427/500 | lr 4.00000 | ms/epoch 1172.33562 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 428/500 | lr 4.00000 | ms/epoch 1197.98088 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 429/500 | lr 4.00000 | ms/epoch 996.31882 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 430/500 | lr 4.00000 | ms/epoch 792.03677 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 431/500 | lr 4.00000 | ms/epoch 842.33904 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 432/500 | lr 4.00000 | ms/epoch 1094.97738 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 433/500 | lr 4.00000 | ms/epoch 876.02353 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 434/500 | lr 4.00000 | ms/epoch 765.20491 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 435/500 | lr 4.00000 | ms/epoch 926.76997 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 436/500 | lr 4.00000 | ms/epoch 828.40180 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 437/500 | lr 4.00000 | ms/epoch 877.93517 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 438/500 | lr 4.00000 | ms/epoch 927.36149 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 439/500 | lr 4.00000 | ms/epoch 763.65066 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 440/500 | lr 4.00000 | ms/epoch 925.07648 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 441/500 | lr 4.00000 | ms/epoch 813.43961 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 442/500 | lr 4.00000 | ms/epoch 697.64709 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 443/500 | lr 4.00000 | ms/epoch 810.23026 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 444/500 | lr 4.00000 | ms/epoch 767.29345 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 445/500 | lr 4.00000 | ms/epoch 798.83957 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 446/500 | lr 4.00000 | ms/epoch 1808.93660 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 447/500 | lr 4.00000 | ms/epoch 864.89487 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 448/500 | lr 4.00000 | ms/epoch 743.91055 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 449/500 | lr 4.00000 | ms/epoch 879.75883 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 450/500 | lr 4.00000 | ms/epoch 789.97970 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 451/500 | lr 4.00000 | ms/epoch 630.00560 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 452/500 | lr 4.00000 | ms/epoch 724.29895 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 453/500 | lr 4.00000 | ms/epoch 764.14943 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 454/500 | lr 4.00000 | ms/epoch 676.27406 | train_loss  0.11 | eval_loss  0.00\n",
      "| epoch 455/500 | lr 4.00000 | ms/epoch 650.09904 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 456/500 | lr 4.00000 | ms/epoch 645.09797 | train_loss  0.06 | eval_loss  0.00\n",
      "| epoch 457/500 | lr 4.00000 | ms/epoch 601.17483 | train_loss  0.06 | eval_loss  0.00\n",
      "| epoch 458/500 | lr 4.00000 | ms/epoch 598.99497 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 459/500 | lr 4.00000 | ms/epoch 584.87725 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 460/500 | lr 4.00000 | ms/epoch 622.52975 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 461/500 | lr 4.00000 | ms/epoch 623.99340 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 462/500 | lr 4.00000 | ms/epoch 623.83103 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 463/500 | lr 4.00000 | ms/epoch 617.01393 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 464/500 | lr 4.00000 | ms/epoch 624.11070 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 465/500 | lr 4.00000 | ms/epoch 632.49350 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 466/500 | lr 4.00000 | ms/epoch 649.03474 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 467/500 | lr 4.00000 | ms/epoch 643.47363 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 468/500 | lr 4.00000 | ms/epoch 700.36149 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 469/500 | lr 4.00000 | ms/epoch 618.43443 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 470/500 | lr 4.00000 | ms/epoch 613.00564 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 471/500 | lr 4.00000 | ms/epoch 603.93071 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 472/500 | lr 4.00000 | ms/epoch 605.19075 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 473/500 | lr 4.00000 | ms/epoch 715.37375 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 474/500 | lr 4.00000 | ms/epoch 677.88148 | train_loss  0.06 | eval_loss  0.00\n",
      "| epoch 475/500 | lr 4.00000 | ms/epoch 659.91759 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 476/500 | lr 4.00000 | ms/epoch 677.16384 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 477/500 | lr 4.00000 | ms/epoch 622.29085 | train_loss  0.10 | eval_loss  0.00\n",
      "| epoch 478/500 | lr 4.00000 | ms/epoch 619.92145 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 479/500 | lr 4.00000 | ms/epoch 768.22138 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 480/500 | lr 4.00000 | ms/epoch 730.14307 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 481/500 | lr 4.00000 | ms/epoch 753.79801 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 482/500 | lr 4.00000 | ms/epoch 612.49208 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 483/500 | lr 4.00000 | ms/epoch 534.25241 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 484/500 | lr 4.00000 | ms/epoch 538.46574 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 485/500 | lr 4.00000 | ms/epoch 519.69910 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 486/500 | lr 4.00000 | ms/epoch 492.78283 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 487/500 | lr 4.00000 | ms/epoch 513.13877 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 488/500 | lr 4.00000 | ms/epoch 486.85765 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 489/500 | lr 4.00000 | ms/epoch 487.16044 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 490/500 | lr 4.00000 | ms/epoch 462.93402 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 491/500 | lr 4.00000 | ms/epoch 436.59282 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 492/500 | lr 4.00000 | ms/epoch 436.63001 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 493/500 | lr 4.00000 | ms/epoch 426.84484 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 494/500 | lr 4.00000 | ms/epoch 428.27344 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 495/500 | lr 4.00000 | ms/epoch 467.08202 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 496/500 | lr 4.00000 | ms/epoch 445.08362 | train_loss  0.08 | eval_loss  0.00\n",
      "| epoch 497/500 | lr 4.00000 | ms/epoch 499.78185 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 498/500 | lr 4.00000 | ms/epoch 422.37711 | train_loss  0.07 | eval_loss  0.00\n",
      "| epoch 499/500 | lr 4.00000 | ms/epoch 428.79796 | train_loss  0.09 | eval_loss  0.00\n",
      "| epoch 500/500 | lr 4.00000 | ms/epoch 407.64785 | train_loss  0.07 | eval_loss  0.00\n"
     ]
    }
   ],
   "source": [
    "best_eval_loss = 1e8\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "\n",
    "        if SPLIT_DATASET:\n",
    "            eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "        else:\n",
    "            eval_loss = 0\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if eval_loss < best_eval_loss:\n",
    "            torch.save(model, MODEL_PATH)\n",
    "            best_eval_loss = eval_loss\n",
    "\n",
    "        # Anneal the learning rate if the validation loss plateaus\n",
    "        if epoch > 5 and eval_loss > max(eval_losses[-5:]):\n",
    "            lr = lr / 2.\n",
    "            if lr < 0.1:\n",
    "                lr = 2\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(MODEL_PATH, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# get the first sequence from the batch keeping the batch dimension\n",
    "samples, targets = next(iter(train_dataloader))\n",
    "sample = samples[0].unsqueeze(0)\n",
    "# Mask the last bar of the input data.\n",
    "sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "# Make the prediction.\n",
    "prediction = model(sample)\n",
    "prediction = prediction.contiguous().view(-1, OUTPUT_VOCAB_SIZE)\n",
    "\n",
    "# Get the predicted tokens.\n",
    "predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "# Get the predicted sequence.\n",
    "predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "# Convert the predicted sequence to MIDI.\n",
    "out_file_path = os.path.join(RESULTS_PATH, 'predicted.mid')\n",
    "OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
