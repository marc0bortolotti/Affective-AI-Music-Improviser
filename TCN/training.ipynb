{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from MIDI.PRETTY_MIDI.pretty_midi_tokenization import PrettyMidiTokenizer, SILENCE_TOKEN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "MODEL_PATH = os.path.join(DIRECTORY_PATH, 'model/generated_model_state.pth')\n",
    "VOCAB_PATH = os.path.join(DIRECTORY_PATH, 'vocabularies')\n",
    "RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results')\n",
    "\n",
    "\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 2 # 4\n",
    "BATCH_SIZE = 2\n",
    "TRAIN_MODEL = True\n",
    "SPLIT_DATASET = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 1\n",
      "Number of output files: 1\n",
      "\n",
      "\n",
      "1: drum.MID -> bass.MID\n",
      "\n",
      "Number of input bars: 96\n",
      "Number of input sequences: 92\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 20\n",
      "\n",
      "Number of output bars: 96\n",
      "Number of output sequences: 92\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 39\n",
      "\n",
      "Number of sequences after truncation: 92, 92\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as input_#.mid and output_#.mid in the corresponding folders\n",
    "'''\n",
    "\n",
    "input_filenames = glob.glob(os.path.join(DATASET_PATH, 'input/*.MID'))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = glob.glob(os.path.join(DATASET_PATH, 'output/*.MID'))\n",
    "print('Number of output files:', len(output_filenames))\n",
    "\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'\\n\\n{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    INPUT_TOK = PrettyMidiTokenizer(in_file)\n",
    "    print(f'\\nNumber of input bars: {INPUT_TOK.num_bars}')\n",
    "    print(f'Number of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "    print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "    print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "\n",
    "    OUTPUT_TOK = PrettyMidiTokenizer(out_file)\n",
    "    print(f'\\nNumber of output bars: {OUTPUT_TOK.num_bars}')\n",
    "    print(f'Number of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "    print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "    print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "    # force the sequences to be the same length\n",
    "    min_length = min(len(INPUT_TOK.sequences), len(OUTPUT_TOK.sequences))\n",
    "    INPUT_TOK.sequences = INPUT_TOK.sequences[:min_length]\n",
    "    OUTPUT_TOK.sequences = OUTPUT_TOK.sequences[:min_length]\n",
    "    print(f'\\nNumber of sequences after truncation: {len(INPUT_TOK.sequences)}, {len(OUTPUT_TOK.sequences)}')\n",
    "   \n",
    "\n",
    "    # save the vocabularies\n",
    "    INPUT_TOK.VOCAB.save(os.path.join(VOCAB_PATH, 'input_vocab.txt'))\n",
    "    OUTPUT_TOK.VOCAB.save(os.path.join(VOCAB_PATH, 'output_vocab.txt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 56\n",
      "Evaluation set size: 18\n",
      "Test set size: 18\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "if SPLIT_DATASET:\n",
    "    # Split the dataset into training, evaluation and test sets\n",
    "    train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "    # Create the dataloaders\n",
    "    train_sampler = RandomSampler(train_set)          \n",
    "    train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    eval_sampler = RandomSampler(eval_set)\n",
    "    eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    test_sampler = RandomSampler(test_set)\n",
    "    test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "else:\n",
    "    train_set = dataset\n",
    "    train_sampler = RandomSampler(train_set)          \n",
    "    train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    eval_dataloader = []\n",
    "    test_dataloader = []\n",
    "\n",
    "print(f'Train set size: {len(train_set)}')\n",
    "print(f'Evaluation set size: {len(eval_set)}')\n",
    "print(f'Test set size: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111 \n",
    "OUTPUT_VOCAB_SIZE = len(OUTPUT_TOK.VOCAB)\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] vectors (see model.py)\n",
    "LEVELS = 7\n",
    "HIDDEN_UNITS = 192\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# balance the loss function by assigning a weight to each token related to its frequency\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_VOCAB_SIZE], dtype=torch.float)\n",
    "for i, weigth in enumerate(OUTPUT_TOK.tokens_weights):\n",
    "    LOSS_WEIGTHS[i] = 1 - weigth\n",
    "    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_VOCAB_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data \n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(data_masked)\n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "        final_target = targets.contiguous().view(-1)\n",
    "        final_output = output.contiguous().view(-1, OUTPUT_VOCAB_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        if GRADIENT_CLIP > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1e8\n",
    "best_model_epoch = 0\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "\n",
    "\n",
    "        if SPLIT_DATASET:\n",
    "            eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if eval_loss < best_loss:\n",
    "                torch.save(model.state_dict(), MODEL_PATH)\n",
    "                best_loss = eval_loss\n",
    "                best_model_epoch = epoch \n",
    "\n",
    "            # # Anneal the learning rate if the validation loss plateaus\n",
    "            # if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n",
    "            #     lr = lr / 2.\n",
    "            #     if lr < 0.1:\n",
    "            #         lr = 2\n",
    "            #     for param_group in optimizer.param_groups:\n",
    "            #         param_group['lr'] = lr\n",
    "\n",
    "\n",
    "        else:\n",
    "            eval_loss = 0\n",
    "\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if train_loss < best_loss:\n",
    "                torch.save(model.state_dict(), MODEL_PATH)\n",
    "                best_loss = train_loss\n",
    "                best_model_epoch = epoch \n",
    "\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n",
    "        \n",
    "    \n",
    "    if TRAIN_MODEL:\n",
    "        print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d}' \\\n",
    "              .format(best_loss, best_model_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # plot the losses over the epochs \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(eval_losses, label='eval')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # save images\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 0 1 1 1 1 1 2 3 4 4 4 4 5 6 6 6 6 6 2 2 3 4 4 4 4 0 1 1 1 1 1 2 2 3 4\n",
      " 4 4 5 6 6 6 6 6 2 3 4 4 4 3 0 1 1 1 1 1 2 7 8 8 8 5 3 4 6 6 6 6 2 2 7 8 8\n",
      " 8 0 1 1 1 1 1 1 2 7 8 8 8 5 6 6 6 6 6 6 7 8 8 8 7 0 1 1 1 1 1 2 7 8 8 8 8\n",
      " 5 6 6 6 6 6 6 7 8 8 8 0 3 4 1 1 1 1 1 2 2 2 2 5 6 6 6 6 6 2 3 4 4 4 4 0 1\n",
      " 1 1 1 1 2 3 4 4 4 4 5 6 6 6 6 6 2 3 4 4 4 4 0 1 1 1 1 1 2 2 3 4 4 4 5 6 6\n",
      " 6 6 6 2 2 3 4 4]\n",
      "[[0, 1, 0], [36, 6, 127], [0, 1, 0], [42, 5, 127], [38, 6, 127], [0, 2, 0], [42, 5, 127], [36, 6, 127], [0, 2, 0], [42, 4, 127], [38, 6, 127], [0, 1, 0], [42, 4, 127], [42, 1, 127], [36, 6, 127], [0, 1, 0], [42, 4, 90], [38, 1, 127], [42, 2, 127], [38, 4, 127], [0, 2, 0], [42, 4, 90], [36, 7, 127], [0, 1, 0], [42, 4, 90], [38, 7, 127], [42, 4, 90], [42, 1, 90], [36, 6, 127], [0, 1, 0], [42, 5, 90], [38, 7, 127], [42, 4, 90], [36, 1, 127], [42, 2, 127], [36, 5, 127], [0, 4, 0], [38, 6, 127], [0, 1, 0], [42, 5, 127], [36, 6, 127], [0, 1, 0], [42, 5, 127], [38, 6, 127], [0, 1, 0], [42, 5, 127], [36, 6, 127], [0, 2, 0], [42, 4, 127], [38, 6, 127], [0, 2, 0]]\n",
      "MIDI file saved at results\\predicted_blues.mid\n",
      "['O', '48Sf', '43f', '48f', '43f', '48f', 'O', '48f', '43f', '48Sf', '43f', '48f', 'O', '52Sf', '47f', '52f', '47f', '52f', '47f', 'O', '47Sf', '52f', '47f', 'O', 'O', '50Sf', '50f', '50f', '50f', '50f', 'O', 'O', 'O', '50Sf', '50f', '50f', 'O', '53Sf', '53f', '53f', 'O', '52Sf', '52f', 'O', 'O', '50Sf', '50f', '50f', 'O', '43Sf', '43f', '43f', '43f', '43f', '43f', 'O', 'O', '43Sf', '43f', '43f', '43f', '47Sf', '47f', '47f', '47f', '47f', '47f', 'O', 'O', 'O', '47Sf', 'O', '50f', '45f', '43f', '43f', '43f', '48f', 'O', 'O', 'O', 'O', '43f', '53f', '53f', '53f', '47f', '47f', '47f', 'O', 'O', 'O', 'O', 'O', '50f', '50f', '50f', '50f', '50f', '50f', '48f', '48f', 'O', 'O', 'O', 'O', '50f', '43f', '53f', '47f', '47f', '47f', '47f', '47f', 'O', 'O', '47f', 'O', '50f', '50f', '50f', '50f', '50f', '48f', '52f', '52f', 'O', 'O', 'O', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', 'O', 'O', 'O', 'O', '52f', '52f', '52f', '50f', '52f', '52f', '52f', '52f', '52f', '43f', '50f', 'O', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', 'O', 'O', 'O', 'O', 'O', '50Sf', '50f', '52f', '50f', '50f', '50f', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '54f', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[[0, 1, 0], [48, 1, 127], [43, 1, 127], [48, 1, 127], [43, 1, 127], [48, 1, 127], [0, 1, 0], [48, 1, 127], [43, 1, 127], [48, 1, 127], [43, 1, 127], [48, 1, 127], [0, 1, 0], [52, 1, 127], [47, 1, 127], [52, 1, 127], [47, 1, 127], [52, 1, 127], [47, 1, 127], [0, 1, 0], [47, 1, 127], [52, 1, 127], [47, 1, 127], [0, 2, 0], [50, 5, 127], [0, 3, 0], [50, 3, 127], [0, 1, 0], [53, 3, 127], [0, 1, 0], [52, 2, 127], [0, 2, 0], [50, 3, 127], [0, 1, 0], [43, 6, 127], [0, 2, 0], [43, 4, 127], [47, 6, 127], [0, 3, 0], [47, 1, 127], [0, 1, 0], [50, 1, 127], [45, 1, 127], [43, 3, 127], [48, 1, 127], [0, 4, 0], [43, 1, 127], [53, 3, 127], [47, 3, 127], [0, 5, 0], [50, 6, 127], [48, 2, 127], [0, 4, 0], [50, 1, 127], [43, 1, 127], [53, 1, 127], [47, 5, 127], [0, 2, 0], [47, 1, 127], [0, 1, 0], [50, 5, 127], [48, 1, 127], [52, 2, 127], [0, 3, 0], [52, 8, 127], [0, 4, 0], [52, 3, 127], [50, 1, 127], [52, 5, 127], [43, 1, 127], [50, 1, 127], [0, 1, 0], [52, 10, 127], [0, 5, 0], [50, 2, 127], [52, 1, 127], [50, 3, 127], [0, 7, 0], [54, 1, 127]]\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# select a genere to be predicted\n",
    "generes = ['blues', 'rock_excited', 'rock_relax']\n",
    "genere = generes[0]\n",
    "\n",
    "# get a sample to be predicted\n",
    "sample_path = os.path.join(DATASET_PATH, f'test/drum_{genere}.mid')\n",
    "sample, _, _, _ = INPUT_TOK.midi_to_tokens(sample_path, update_vocab=False)\n",
    "print(sample[0])\n",
    "print(INPUT_TOK.tokens_to_midi(sample[0]))\n",
    "sample = torch.LongTensor(sample).to(device)\n",
    "\n",
    "# Get the last sequence from the batch and unsqueeze it to add a batch dimension.\n",
    "sample = sample[-1].unsqueeze(0)\n",
    "\n",
    "# Mask the last bar of the input data.\n",
    "sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "# Make the prediction.\n",
    "prediction = model(sample)\n",
    "prediction = prediction.contiguous().view(-1, OUTPUT_VOCAB_SIZE)\n",
    "\n",
    "# Get the predicted tokens.\n",
    "predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "# Get the predicted sequence.\n",
    "predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "# Convert the predicted sequence to MIDI.\n",
    "out_file_path = os.path.join(RESULTS_PATH, f'predicted_{genere}.mid')\n",
    "pitch_ticks_list =  OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path) # midi is a pretty midi object\n",
    "\n",
    "# check \n",
    "predicted_sequence_string = []\n",
    "for id in predicted_sequence:\n",
    "    predicted_sequence_string.append(OUTPUT_TOK.VOCAB.idx2word[id])\n",
    "print(predicted_sequence_string)\n",
    "print(pitch_ticks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
