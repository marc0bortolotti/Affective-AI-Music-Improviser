{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from MIDI.PRETTY_MIDI.pretty_midi_tokenization import PrettyMidiTokenizer, BCI_TOKENS\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 2 # 4\n",
    "BATCH_SIZE = 16 # 16\n",
    "TRAIN_MODEL = False\n",
    "FEEDBACK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 6\n",
      "Number of output files: 6 \n",
      "\n",
      "1: 0_Drum_HardRock_EXCITED.mid -> 0_Bass_HardRock_EXCITED.mid\n",
      "Input sequence length: 26\n",
      "Emotion token: C\n",
      "\n",
      "2: 1_Drum_HardRock_RELAX.mid -> 1_Bass_HardRock_RELAX.mid\n",
      "Input sequence length: 32\n",
      "Emotion token: R\n",
      "\n",
      "3: 2_Drum_Blues_EXCITED.mid -> 2_Bass_Blues_EXCITED.mid\n",
      "Input sequence length: 31\n",
      "Emotion token: C\n",
      "\n",
      "4: 3_Drum_Blues_RELAX.mid -> 3_Bass_Blues_RELAX.mid\n",
      "Input sequence length: 23\n",
      "Emotion token: R\n",
      "\n",
      "5: 4_Drum_PopRock_EXCITED.mid -> 4_Bass_PopRock_EXCITED.mid\n",
      "Input sequence length: 35\n",
      "Emotion token: C\n",
      "\n",
      "6: 5_Drum_PopRock_RELAX.mid -> 5_Bass_PopRock_RELAX.mid\n",
      "Input sequence length: 23\n",
      "Emotion token: R\n",
      "\n",
      "\n",
      "Number of input sequences: 185\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 52\n",
      "\n",
      "Number of output sequences: 185\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 55\n",
      "\n",
      "Input vocab: {'O': 0, '36S': 1, '36': 2, '36_42S': 3, '38S': 4, '38': 5, '38_42S': 6, '42S_36S': 7, '42S_38S': 8, '38_42': 9, '36_42S_36S': 10, '36S_42S': 11, '36_42': 12, '42_36': 13, '38S_42S': 14, '42S': 15, '42': 16, '42_38': 17, '42_36S': 18, '42_38S': 19, '42_38_42S': 20, '42_36_42S': 21, '42_36_42': 22, '38_42S_36S': 23, '36_42S_38S': 24, '38_36S': 25, '38_36_42S': 26, '36_36S_42S': 27, '38_42_36S': 28, '38_42_36_42S': 29, 'C': 30, '36_42_36S': 31, '42_42S': 32, '36_42_38S': 33, '36_38S': 34, '36_38_42S': 35, '36_42_36S_42S': 36, '42_42': 37, 'R': 38, '42_38_42': 39, '42_36_42S_38S': 40, '42_36_42_38': 41, '42_36_38S': 42, '42_36_38_42S': 43, '42_36_38_42': 44, '36_42_38_42S': 45, '36_38': 46, '36_42_38': 47, '36_38_42': 48, '38_42_38S': 49, '38_42_38': 50, '38_42_38_42S': 51}\n",
      "Output vocab: {'O': 0, '52S': 1, '52': 2, '55S': 3, '55': 4, '45S': 5, '45': 6, '48S': 7, '48': 8, '47S': 9, '47': 10, '43S': 11, '43': 12, '57S': 13, '57': 14, '50S': 15, '50': 16, '49S': 17, '49': 18, '40S': 19, '40': 20, '44S': 21, '44': 22, '67S': 23, '67': 24, '59S': 25, '59': 26, '62S': 27, '62': 28, '60S': 29, '60': 30, '53S': 31, '53': 32, '64S': 33, '64': 34, '56S': 35, '56': 36, '51S': 37, '51': 38, '58S': 39, '58': 40, '83S': 41, '83': 42, '63S': 43, '63': 44, '66S': 45, '66': 46, '61S': 47, '61': 48, '54S': 49, '54': 50, '81S': 51, '81': 52, '80S': 53, '80': 54}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as \n",
    "    input:  drum_genere_emotion.mid \n",
    "    output: bass_genere_emotion.mid \n",
    "in the corresponding folders\n",
    "'''\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "\n",
    "input_filenames = glob.glob(os.path.join(DATASET_PATH, 'input/*.MID'))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = glob.glob(os.path.join(DATASET_PATH, 'output/*.MID'))\n",
    "print('Number of output files:', len(output_filenames), '\\n')\n",
    "\n",
    "INPUT_TOK = PrettyMidiTokenizer()\n",
    "OUTPUT_TOK = PrettyMidiTokenizer()\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    if 'RELAX' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['relax']\n",
    "    elif 'EXCITED' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['concentrate']\n",
    "    else:\n",
    "        raise Exception('Emotion not found in file name. Please add the emotion to the file name.')\n",
    "\n",
    "    in_seq, in_df = INPUT_TOK.midi_to_tokens(in_file, update_vocab=True, update_sequences=True, emotion_token = emotion_token)\n",
    "    out_seq, out_df = OUTPUT_TOK.midi_to_tokens(out_file, update_vocab=True, update_sequences=True)\n",
    "\n",
    "    print(f'Input sequence length: {len(in_seq)}')\n",
    "    print(f'Emotion token: {emotion_token}\\n')\n",
    "\n",
    "print(f'\\nNumber of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "print(f'\\nNumber of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "print('\\nInput vocab:', INPUT_TOK.VOCAB.word2idx)\n",
    "print('Output vocab:', OUTPUT_TOK.VOCAB.word2idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 111\n",
      "Evaluation set size: 37\n",
      "Test set size: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gianni\\AppData\\Local\\Temp\\ipykernel_27772\\3823352634.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "# Split the dataset into training, evaluation and test sets\n",
    "train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_sampler = RandomSampler(train_set)          \n",
    "train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "eval_sampler = RandomSampler(eval_set)\n",
    "eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = RandomSampler(test_set)\n",
    "test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train set size: {len(train_set)}')\n",
    "print(f'Evaluation set size: {len(eval_set)}')\n",
    "print(f'Test set size: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, '52S': 1, '52': 2, '55S': 3, '55': 4, '45S': 5, '45': 6, '48S': 7, '48': 8, '47S': 9, '47': 10, '43S': 11, '43': 12, '57S': 13, '57': 14, '50S': 15, '50': 16, '49S': 17, '49': 18, '40S': 19, '40': 20, '44S': 21, '44': 22, '67S': 23, '67': 24, '59S': 25, '59': 26, '62S': 27, '62': 28, '60S': 29, '60': 30, '53S': 31, '53': 32, '64S': 33, '64': 34, '56S': 35, '56': 36, '51S': 37, '51': 38, '58S': 39, '58': 40, '83S': 41, '83': 42, '63S': 43, '63': 44, '66S': 45, '66': 46, '61S': 47, '61': 48, '54S': 49, '54': 50, '81S': 51, '81': 52, '80S': 53, '80': 54}\n",
      "\n",
      "Model created: TCN(\n",
      "  (encoder): Embedding(52, 20)\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(20, 192, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (5): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (6): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(192, 20, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=20, out_features=55, bias=True)\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111\n",
    "OUTPUT_SIZE = len(OUTPUT_TOK.VOCAB) \n",
    "\n",
    "\n",
    "'''\n",
    "IMPORTANT:\n",
    "to cover all the sequence of tokens k * d must be >= hidden units (see the paper)\n",
    "k = kernel_size\n",
    "d = dilation = 2 ^ (n_levels - 1) \n",
    "'''\n",
    "if FEEDBACK:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) + OUTPUT_SIZE\n",
    "    LEVELS = 8\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH * 2 # 192 * 2 = 384\n",
    "else:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) \n",
    "    LEVELS = 7\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH # 192 \n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] real value vectors (see model.py)\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE] # [192, 192, 192, 192, 192, 192, 20]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# balance the loss function by assigning a weight to each token related to its frequency\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_SIZE], dtype=torch.float, device=device)\n",
    "OUTPUT_TOK.VOCAB.compute_weights()\n",
    "for i, weigth in enumerate(OUTPUT_TOK.VOCAB.weights):\n",
    "    LOSS_WEIGTHS[i] = 1 - weigth\n",
    "    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = INPUT_SIZE,\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f'\\nModel created: {model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters():\n",
    "    # plot the losses over the epochs \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(eval_losses, label='eval')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))\n",
    "\n",
    "\n",
    "    # save the vocabularies\n",
    "    INPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'input_vocab.txt'))\n",
    "    OUTPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'output_vocab.txt'))\n",
    "\n",
    "     # save the model hyperparameters in a file txt\n",
    "    with open(os.path.join(RESULTS_PATH, 'model_hyperparameters.txt'), 'w') as f:\n",
    "\n",
    "        f.write(f'----------OPTIMIZATION PARAMETERS----------\\n')\n",
    "        f.write(f'DATE: {time.strftime(\"%Y%m%d-%H%M%S\")}\\n')\n",
    "        f.write(f'DATASET_PATH: {DATASET_PATH}\\n')\n",
    "        f.write(f'FEEDBACK: {FEEDBACK}\\n')\n",
    "        f.write(f'SEED: {SEED}\\n')\n",
    "        f.write(f'INPUT_SIZE: {INPUT_SIZE}\\n')\n",
    "        f.write(f'EMBEDDING_SIZE: {EMBEDDING_SIZE}\\n')\n",
    "        f.write(f'LEVELS: {LEVELS}\\n')\n",
    "        f.write(f'HIDDEN_UNITS: {HIDDEN_UNITS}\\n')\n",
    "        f.write(f'NUM_CHANNELS: {NUM_CHANNELS}\\n')\n",
    "        f.write(f'OUTPUT_SIZE: {OUTPUT_SIZE}\\n')\n",
    "        f.write(f'LOSS_WEIGTHS: {LOSS_WEIGTHS}\\n')\n",
    "        f.write(f'LEARNING_RATE: {LEARNING_RATE}\\n')\n",
    "        f.write(f'BATCH_SIZE: {BATCH_SIZE}\\n')\n",
    "        f.write(f'EPOCHS: {EPOCHS}\\n')\n",
    "        f.write(f'GRADIENT_CLIP: {GRADIENT_CLIP}\\n')\n",
    "        f.write(f'------------------------------------------\\n')\n",
    "        f.write(f'----------RESULTS----------\\n')\n",
    "        f.write(f'BEST_TRAIN_LOSSES: {best_train_loss}\\n')\n",
    "        f.write(f'BEST_EVAL_LOSS: {best_eval_loss}\\n')\n",
    "        f.write(f'TEST_LOSS: {test_loss}\\n')\n",
    "        f.write(f'BEST_MODEL_EPOCH: {best_model_epoch}\\n')\n",
    "        f.write(f'------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if FEEDBACK:\n",
    "        prev_output = torch.zeros([BATCH_SIZE, INPUT_TOK.SEQ_LENGTH], dtype=torch.long, device=device)\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data\n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long, device=device)), dim = 1) \n",
    "\n",
    "        if FEEDBACK:\n",
    "            input = torch.cat((data_masked, prev_output[:batch_size, :]), dim = 1)\n",
    "        else:\n",
    "            input = data_masked\n",
    "           \n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(input)[:, :INPUT_TOK.SEQ_LENGTH] \n",
    "        prev_output = torch.argmax(output, 2)# batch, seq_len (hidden units), vocab_size\n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "    \n",
    "        final_target = targets.contiguous().view(-1)    \n",
    "        final_output = output.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        if mode == 'train':\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # clip the gradients to avoid exploding gradients\n",
    "            if GRADIENT_CLIP > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eval_loss = 1e8\n",
    "best_train_loss = 1e8\n",
    "best_model_epoch = 0\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "early_stop = True\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "\n",
    "    RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results', time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    if not os.path.exists(RESULTS_PATH):\n",
    "        os.makedirs(RESULTS_PATH)\n",
    "        \n",
    "    MODEL_PATH = os.path.join(RESULTS_PATH, 'model_state_dict.pth')\n",
    "\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "        \n",
    "        eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if eval_loss < best_eval_loss:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_eval_loss = eval_loss\n",
    "            best_model_epoch = epoch \n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "\n",
    "        # # Anneal the learning rate if the validation loss plateaus\n",
    "        # if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n",
    "        #     lr = lr / 2.\n",
    "        #     if lr < 0.1:\n",
    "        #         lr = 2\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = lr\n",
    "\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop:\n",
    "            if epoch > 15:\n",
    "                if min(eval_losses[-15:]) > best_eval_loss:\n",
    "                    break\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n",
    "\n",
    "    print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d} \\n\\n' \\\n",
    "            .format(best_eval_loss, best_model_epoch))\n",
    "    \n",
    "    # test the model\n",
    "    test_loss = epoch_step(test_dataloader, 'eval')\n",
    "    print(f'\\n\\nTEST LOSS: {test_loss}')\n",
    "    save_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 2, 2, 0, 15, 16, 16, 16, 19, 20, 5, 5, 5, 5, 5, 0, 15, 16,\n",
      "       16, 16, 18, 21, 22, 2, 2, 2, 2, 0, 15, 16, 16, 16, 19, 20, 39, 5,\n",
      "       5, 5, 5, 0, 0, 15, 16, 16, 16, 7, 13, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0,\n",
      "       8, 17, 5, 5, 5, 5, 5, 0, 15, 16, 16, 16, 7, 13, 2, 2, 2, 2, 2, 0,\n",
      "       15, 16, 16, 16, 8, 17, 5, 5, 5, 5, 5, 15, 16, 16, 16, 16, 7, 13, 1,\n",
      "       2, 2, 2, 0, 15, 16, 16, 16, 16, 8, 17, 5, 5, 5, 5, 5, 0, 15, 16,\n",
      "       16, 18, 21, 22, 2, 2, 2, 2, 0, 15, 16, 16, 16, 8, 17, 17, 5, 5, 5,\n",
      "       5, 0, 0, 15, 16, 16, 16, 7, 13, 1, 2, 2, 2, 2, 15, 16, 16, 16, 19,\n",
      "       20, 39, 5, 5, 5, 5, 0, 15, 16, 16, 16, 18, 21, 22, 2, 2, 2, 0, 0,\n",
      "       7, 13, 13, 13, 40, 41, 41, 5, 5, 5, 5, 0, 15, 16, 16, 16, 16, 7,\n",
      "       13], dtype=object)]\n",
      "[2, 2, 6, 0, 14, 2, 6, 0, 0, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 0, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0, 26, 26, 26, 26, 26, 26, 0, 0, 0, 0, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 0, 0, 15, 16, 16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 37, 38, 38, 38, 38, 38, 38, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 35, 36, 36, 36, 36, 36, 36, 36, 36, 0, 0, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0, 25, 26, 26, 26, 26, 26, 26, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 0, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0, 26, 26, 26, 26, 26, 26, 0, 0, 0, 0, 1, 2, 2, 2]\n",
      "MIDI file saved at results/model_short\\predicted_blues.mid\n",
      "[array([11, 12, 12, 16, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16, 0, 0, 0, 0,\n",
      "       0, 0, 14, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16,\n",
      "       0, 0, 0, 0, 0, 7, 13, 13, 13, 7, 13, 13, 0, 0, 0, 0, 0, 15, 16, 16,\n",
      "       16, 16, 16, 16, 0, 0, 0, 0, 0, 14, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0,\n",
      "       0, 15, 16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 1, 3, 12, 12, 11, 12, 12,\n",
      "       0, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 4, 6, 9,\n",
      "       9, 9, 9, 9, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16, 16, 0, 0, 0, 0,\n",
      "       0, 7, 13, 13, 13, 7, 13, 13, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16,\n",
      "       16, 0, 0, 0, 0, 0, 0, 14, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 15, 16,\n",
      "       16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 11, 12, 12], dtype=object)]\n",
      "[9, 10, 10, 12, 10, 10, 10, 12, 10, 10, 10, 12, 10, 12, 12, 12, 10, 12, 12, 16, 10, 0, 0, 0, 10, 0, 10, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16, 16, 12, 16, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 16, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 12, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 20, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 11, 12, 12, 12]\n",
      "MIDI file saved at results/model_short\\predicted_rock_relax.mid\n",
      "[array([1, 2, 2, 2, 0, 0, 0, 0, 0, 8, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 15,\n",
      "       1, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 4, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0,\n",
      "       0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 14, 9, 9, 9, 9, 9, 9, 0,\n",
      "       0, 0, 0, 0, 7, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 4, 6, 9, 9, 9, 9, 9,\n",
      "       16, 0, 0, 0, 0, 7, 2, 2, 1, 2, 2, 2, 0, 0, 0, 0, 0, 4, 6, 9, 9, 9,\n",
      "       9, 9, 0, 0, 0, 0, 0, 7, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 8, 5, 5,\n",
      "       5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 7, 13, 7, 13, 13, 13, 13, 0, 0, 0, 0,\n",
      "       0, 14, 9, 9, 9, 9, 9, 18, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 1, 2, 2,\n",
      "       2, 2, 2, 24, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 7, 2], dtype=object)]\n",
      "[2, 2, 6, 0, 14, 14, 14, 0, 0, 18, 2, 2, 2, 0, 13, 1, 2, 2, 14, 0, 0, 2, 10, 10, 10, 0, 0, 10, 10, 2, 10, 10, 0, 9, 10, 10, 2, 0, 0, 9, 10, 20, 20, 20, 19, 20, 20, 20, 20, 0, 19, 19, 20, 20, 20, 20, 0, 9, 20, 8, 8, 0, 0, 7, 7, 8, 8, 8, 0, 7, 8, 8, 8, 8, 0, 7, 7, 8, 8, 0, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 12, 0, 0, 0, 11, 12, 12, 12, 0, 0, 11, 12, 12, 12, 0, 11, 12, 12, 12, 12, 0, 0, 11, 12, 12, 12, 0, 0, 11, 12, 12, 12, 0, 0, 11, 12, 12, 12, 0, 0, 11, 12, 12, 12, 12, 0, 9, 10, 10, 10, 2, 2, 10, 2, 2, 2, 0, 0, 1, 2, 2, 2, 0, 0, 0, 9, 10, 0, 0, 0, 7, 8, 8, 8, 0, 0, 0, 10, 8, 10, 10, 0, 0, 2, 10, 10, 0, 0, 0, 0, 0, 0, 0, 7, 7, 2, 2]\n",
      "MIDI file saved at results/model_short\\predicted_rock_excited.mid\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "RESULTS_PATH = 'results/model_short'\n",
    "MODEL_PATH = f'{RESULTS_PATH}/model_state_dict.pth'\n",
    "\n",
    "INPUT_TOK.load_vocab(f'{RESULTS_PATH}/input_vocab.txt')\n",
    "OUTPUT_TOK.load_vocab(f'{RESULTS_PATH}/output_vocab.txt')\n",
    "\n",
    "model = TCN(input_size = len(INPUT_TOK.VOCAB),\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = len(OUTPUT_TOK.VOCAB), \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# select a genere to be predicted\n",
    "generes = ['blues', 'rock_relax', 'rock_excited']\n",
    "\n",
    "for genere in generes:\n",
    "    # get a sample to be predicted\n",
    "    sample_path = os.path.join(DATASET_PATH, f'test/drum_{genere}_2.mid')\n",
    "    sample = INPUT_TOK.midi_to_tokens(sample_path, update_vocab=False) [0]\n",
    "\n",
    "    print(sample)\n",
    "    sample = torch.LongTensor(sample)\n",
    "\n",
    "    # Get the last sequence from the batch and unsqueeze it to add a batch dimension.\n",
    "    sample = sample[-1].unsqueeze(0)\n",
    "\n",
    "    # Mask the last bar of the input data.\n",
    "    sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "    # Make the prediction.\n",
    "    prediction = model(sample.to(device))\n",
    "    prediction = prediction.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "    # Get the predicted tokens.\n",
    "    predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "    # Get the predicted sequence.\n",
    "    predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "    print(predicted_sequence)\n",
    "\n",
    "    # Convert the predicted sequence to MIDI.\n",
    "    out_file_path = os.path.join(RESULTS_PATH, f'predicted_{genere}.mid')\n",
    "    pitch_ticks_velocity =  OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path = out_file_path, ticks_filter = 3, instrument_name = 'Electric Bass (finger)') \n",
    "\n",
    "\n",
    "# # check \n",
    "# predicted_sequence_string = []\n",
    "# for id in predicted_sequence:\n",
    "#     predicted_sequence_string.append(OUTPUT_TOK.VOCAB.idx2word[id])\n",
    "# print(predicted_sequence_string)\n",
    "# print(pitch_ticks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
