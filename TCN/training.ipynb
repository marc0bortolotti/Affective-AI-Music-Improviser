{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from APPLICATION.model.tokenization import PrettyMidiTokenizer, BCI_TOKENS\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 2 # 4\n",
    "BATCH_SIZE = 16 # 16\n",
    "TRAIN_MODEL = False\n",
    "FEEDBACK = False\n",
    "EMPHAZISE_EEG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 6\n",
      "Number of output files: 6 \n",
      "\n",
      "1: 0_Drum_HardRock_EXCITED.mid -> 0_Bass_HardRock_EXCITED.mid\n",
      "Input sequence length: 26\n",
      "Emotion token: C\n",
      "\n",
      "    pitch velocity start end bar\n",
      "0      36      127     0   6   0\n",
      "1      42       48     5   6   0\n",
      "2      38      127    11  18   0\n",
      "3      42       54    17  18   0\n",
      "4      42      125    23  24   0\n",
      "5      36      127    23  30   0\n",
      "6      42       59    29  30   0\n",
      "7      38      127    35  42   0\n",
      "8      42       67    41  42   0\n",
      "9      42      127    47  48   0\n",
      "10     36      127    47  48   0\n",
      "11     36      127     0   6   1\n",
      "12     42      127    11  12   1\n",
      "13     38      127    11  18   1\n",
      "14     42       40    17  18   1\n",
      "15     42      126    23  24   1\n",
      "16     36      127    23  29   1\n",
      "17     42      127    35  36   1\n",
      "18     38      127    35  41   1\n",
      "19     42      127    47  48   1\n",
      "20     36      127    47  48   1\n",
      "21     36      127     0   6   2\n",
      "22     42       53     5   6   2\n",
      "23     42      127    11  12   2\n",
      "24     38      127    11  17   2\n",
      "25     42      127    23  24   2\n",
      "26     36      127    23  30   2\n",
      "27     42       67    29  30   2\n",
      "28     42      127    35  36   2\n",
      "29     38      127    35  42   2\n",
      "30     42       60    41  42   2\n",
      "31     42      127    47  48   2\n",
      "32     36      127    47  48   2\n",
      "33     36      127     0   6   3\n",
      "34     42       70     5   6   3\n",
      "35     38      127    11  18   3\n",
      "36     42       84    16  18   3\n",
      "37     42      127    23  24   3\n",
      "38     36      127    23  30   3\n",
      "39     42       64    29  30   3\n",
      "40     38      127    35  42   3\n",
      "41     42       82    41  42   3\n",
      "42     36      127    47  48   3\n",
      "43     36      127     0   6   4\n",
      "44     42       53     5   6   4\n",
      "45     36      127     5   6   4\n",
      "46     42      127    11  12   4\n",
      "47     38      122    11  18   4\n",
      "48     42       48    16  18   4\n",
      "49     36      127    22  28   4\n",
      "50     42      127    22  30   4\n",
      "51     42       43    28  30   4\n",
      "52     36      127    28  34   4\n",
      "53     38      127    34  41   4\n",
      "54     42      127    34  41   4\n",
      "55     42       61    40  41   4\n",
      "56     36      127    46  48   4\n",
      "57     36      127     0   4   5\n",
      "58     42      127    46  48   4\n",
      "59     42      127     0   6   5\n",
      "60     42       78     4   6   5\n",
      "61     38      127    11  17   5\n",
      "62     42      127    11  17   5\n",
      "63     42       64    16  17   5\n",
      "64     36      127    22  28   5\n",
      "65     42      127    23  24   5\n",
      "66     36      127    28  34   5\n",
      "67     42      127    34  36   5\n",
      "68     38      127    34  40   5\n",
      "69     42       70    40  42   5\n",
      "70     36      127    46  48   5\n",
      "71     36      127     0   4   6\n",
      "72     42      127    47  48   5\n",
      "73     42      127     0   5   6\n",
      "74     36      127     4  10   6\n",
      "75     42       78     5  12   6\n",
      "76     38      127    10  16   6\n",
      "77     42      127    11  12   6\n",
      "78     42       71    16  18   6\n",
      "79     36      127    22  28   6\n",
      "80     42      125    23  30   6\n",
      "81     42       76    28  30   6\n",
      "82     36      127    28  34   6\n",
      "83     42      127    35  36   6\n",
      "84     38      127    35  42   6\n",
      "85     42       78    40  42   6\n",
      "86     36      127    46  48   6\n",
      "87     36      127     0   6   7\n",
      "88     36      127     4   6   7\n",
      "89     42       66     5   6   7\n",
      "90     38      127    11  18   7\n",
      "91     42       78    17  18   7\n",
      "92     36      127    22  30   7\n",
      "93     36      127    29  30   7\n",
      "94     42       66    29  30   7\n",
      "95     38      127    35  41   7\n",
      "96     42      127    47  48   7\n",
      "97     36      127    47  48   7\n",
      "98     36      127     0   6   8\n",
      "99     42       61     5   6   8\n",
      "100    38      127    12  17   8\n",
      "101    42      127    12  19   8\n",
      "102    42       76    17  19   8\n",
      "103    42      126    24  31   8\n",
      "104    36      127    24  31   8\n",
      "105    42       60    29  31   8\n",
      "106    38      127    36  43   8\n",
      "107    42      127    36  43   8\n",
      "108    42       66    42  43   8\n",
      "109    42      127     0   1   9\n",
      "110    36      127     0   7   9\n",
      "111    42       66     6   7   9\n",
      "112    42      127    12  13   9\n",
      "113    38      127    12  19   9\n",
      "114    42       64    18  19   9\n",
      "115    36      127    23  30   9\n",
      "116    42      127    24  25   9\n",
      "117    42       64    30  31   9\n",
      "118    42      127    36  37   9\n",
      "119    38      127    36  41   9\n",
      "120    42       64    41  43   9\n",
      "121    36      127    47  48   9\n",
      "122    36      127     0   6  10\n",
      "123    42       82     5   6  10\n",
      "124    38      127    11  18  10\n",
      "125    42       67    17  18  10\n",
      "126    36      127    23  30  10\n",
      "127    42       76    29  30  10\n",
      "128    38      127    35  42  10\n",
      "129    42       74    41  42  10\n",
      "130    36      127    47  48  10\n",
      "131    36      127     0   6  11\n",
      "132    42       74     5   6  11\n",
      "133    38      127    11  18  11\n",
      "134    42       74    17  18  11\n",
      "135    36      127    17  23  11\n",
      "136    36      127    29  36  11\n",
      "137    42      123    35  36  11\n",
      "138    38      127    35  42  11\n",
      "139    42       59    41  42  11\n",
      "140    42      127    47  48  11\n",
      "141    36      127    47  48  11\n",
      "142    36      127     0   5  12\n",
      "143    42      127    11  12  12\n",
      "144    38      127    11  17  12\n",
      "145    36      127    24  29  12\n",
      "146    42      127    24  31  12\n",
      "147    42       82    29  31  12\n",
      "148    38      127    35  41  12\n",
      "149    42      127    36  43  12\n",
      "150    42       79    41  43  12\n",
      "151    42      127    47  48  12\n",
      "152    36      127    47  48  12\n",
      "153    36      127     0   5  13\n",
      "154    36      127     5  11  13\n",
      "155    42       85     5  13  13\n",
      "156    42      127    11  13  13\n",
      "157    38      127    11  18  13\n",
      "158    36      127    16  24  13\n",
      "159    42       79    17  18  13\n",
      "160    42      127    23  24  13\n",
      "161    36      127    29  35  13\n",
      "162    38      127    35  42  13\n",
      "163    42       59    41  42  13\n",
      "164    36      127    47  48  13\n",
      "165    36      127     0   6  14\n",
      "166    42      127     0   7  14\n",
      "167    42       66     6   7  14\n",
      "168    38      127    11  18  14\n",
      "169    42      127    12  19  14\n",
      "170    42       72    18  19  14\n",
      "171    36      127    23  30  14\n",
      "172    42      127    24  25  14\n",
      "173    36      127    29  30  14\n",
      "174    42       66    29  31  14\n",
      "175    38      127    35  42  14\n",
      "176    42      127    36  43  14\n",
      "177    42       74    41  43  14\n",
      "178    36      127    47  48  14\n",
      "179    36      127     0   6  15\n",
      "180    42      127     0   7  15\n",
      "181    42       70     5   7  15\n",
      "182    38      127    11  17  15\n",
      "183    42      127    11  17  15\n",
      "184    42       78    17  24  15\n",
      "185    36      127    23  29  15\n",
      "186    42      127    24  29  15\n",
      "187    36      127    29  36  15\n",
      "188    42       85    29  36  15\n",
      "189    42      126    35  36  15\n",
      "190    38      127    35  41  15\n",
      "191    42      126    47  48  15\n",
      "192    36      127    47  48  15\n",
      "193    36      127     0   5  16\n",
      "194    42      127    11  12  16\n",
      "195    38      127    11  17  16\n",
      "196    36      127    29  36  16\n",
      "197    42      127    35  36  16\n",
      "198    38      127    35  41  16\n",
      "199    36      127    47  48  16\n",
      "200    36      127     0   6  17\n",
      "201    42       78     4   6  17\n",
      "202    36       23    10  16  17\n",
      "203    38      127    11  17  17\n",
      "204    42       70    28  30  17\n",
      "205    36      127    29  35  17\n",
      "206    38      127    35  42  17\n",
      "207    42       72    40  42  17\n",
      "208    36      127    47  48  17\n",
      "209    36      127     0   6  18\n",
      "210    42       71     5   6  18\n",
      "211    38      127    11  18  18\n",
      "212    42       67    17  18  18\n",
      "213    42       72    28  29  18\n",
      "214    36      127    28  35  18\n",
      "215    42      127    34  35  18\n",
      "216    38      127    34  40  18\n",
      "217    42      127    46  47  18\n",
      "218    36      127    46  48  18\n",
      "219    36      127     0   5  19\n",
      "220    42       70     4   5  19\n",
      "221    36      127     4   5  19\n",
      "222    38      127    10  17  19\n",
      "223    42      127    11  17  19\n",
      "224    42       84    16  17  19\n",
      "225    42      127    22  29  19\n",
      "226    42       88    28  29  19\n",
      "227    36      127    28  34  19\n",
      "228    38      127    34  40  19\n",
      "229    42      127    34  41  19\n",
      "230    42       82    40  41  19\n",
      "231    36        6    40  46  19\n",
      "232    42      127    46  48  19\n",
      "233    42      127     0   5  20\n",
      "234    36      127    46  48  19\n",
      "235    36      127     0   5  20\n",
      "236    42       82     4   5  20\n",
      "237    38      127    10  17  20\n",
      "238    42      127    10  17  20\n",
      "239    42       76    16  17  20\n",
      "240    36      127    23  29  20\n",
      "241    36      127    29  35  20\n",
      "242    38      127    35  42  20\n",
      "243    42       88    41  42  20\n",
      "244    36      127    47  48  20\n",
      "245    36      127     0   5  21\n",
      "246    42       92     4   5  21\n",
      "247    36        6     9  16  21\n",
      "248    38      127    11  18  21\n",
      "249    42       78    17  18  21\n",
      "250    36      127    23  30  21\n",
      "251    36      127    29  30  21\n",
      "252    42       92    29  30  21\n",
      "253    38      127    35  42  21\n",
      "254    42       71    41  42  21\n",
      "255    36      127    47  48  21\n",
      "256    36      127     0   6  22\n",
      "257    42       85     5   6  22\n",
      "258    38      127    11  17  22\n",
      "259    42      127    11  17  22\n",
      "260    42       74    17  23  22\n",
      "261    42      127    23  29  22\n",
      "262    36      127    23  30  22\n",
      "263    36      127    29  30  22\n",
      "264    42       88    29  35  22\n",
      "265    38      127    35  41  22\n",
      "266    42      127    35  41  22\n",
      "267    42       87    41  48  22\n",
      "268    36      127     0   5  23\n",
      "269    42      127     0   7  23\n",
      "270    42      100     5   7  23\n",
      "271    38      127    11  19  23\n",
      "272    42      127    12  19  23\n",
      "273    42       84    17  19  23\n",
      "274    36      127    17  23  23\n",
      "275    42      101    29  30  23\n",
      "276    36      127    29  35  23\n",
      "277    38        5    35  41  23\n",
      "278    38       53    36  41  23\n",
      "279    42      108    36  41  23\n",
      "280    36      127    47  48  23\n",
      "281    36      127     0   6  24\n",
      "282    42      127    47  48  23\n",
      "283    42      127     0   6  24\n",
      "284    42       82     5   6  24\n",
      "285    38      127    11  18  24\n",
      "286    42       71    17  18  24\n",
      "287    36      127    17  23  24\n",
      "288    36      127    28  34  24\n",
      "289    38      127    34  41  24\n",
      "290    42       78    40  41  24\n",
      "291    36      127    47  48  24\n",
      "292    36      127     0   5  25\n",
      "293    38      127    11  17  25\n",
      "294    36      127    17  23  25\n",
      "295    36      127    28  36  25\n",
      "296    42      127    35  36  25\n",
      "297    38      127    35  41  25\n",
      "298    42      127    47  48  25\n",
      "299    36      127    47  48  25\n",
      "300    36      127     0   5  26\n",
      "301    38      127    11  17  26\n",
      "302    36      127    17  24  26\n",
      "303    42      127    23  24  26\n",
      "304    36      127    29  35  26\n",
      "305    38      127    35  41  26\n",
      "306    36      127    47  48  26\n",
      "307    36      127     0   5  27\n",
      "308    42      127     0   7  27\n",
      "309    42       64     5   7  27\n",
      "310    38      127    11  19  27\n",
      "311    42      127    12  19  27\n",
      "312    36      127    17  23  27\n",
      "313    42       67    18  19  27\n",
      "314    42      127    23  25  27\n",
      "315    36      127    29  35  27\n",
      "316    38      127    35  36  27\n",
      "317    42      127    35  36  27\n",
      "318    38      127    36  43  27\n",
      "319    42      127    36  43  27\n",
      "320    42       79    42  43  27\n",
      "321    36      127    42  48  27\n",
      "322    36      127     6  12  28\n",
      "323    38      127    12  18  28\n",
      "324    36      127    24  30  28\n",
      "325    38      127    36  42  28\n",
      "326    42      127    36  43  28\n",
      "327    42       82    42  43  28\n",
      "328    36      127    42  48  28\n",
      "329    36      127     6  12  29\n",
      "330    38      127    12  18  29\n",
      "331    42      127    12  19  29\n",
      "332    36        8    12  19  29\n",
      "333    42       78    18  19  29\n",
      "334    36      127    24  30  29\n",
      "335    42      127    24  31  29\n",
      "336    42       82    30  31  29\n",
      "337    38      115    36  42  29\n",
      "338    42      127    36  43  29\n",
      "339    42       74    42  43  29\n",
      "340    36      127    42  48  29\n",
      "2: 1_Drum_HardRock_RELAX.mid -> 1_Bass_HardRock_RELAX.mid\n",
      "Input sequence length: 32\n",
      "Emotion token: R\n",
      "\n",
      "3: 2_Drum_Blues_EXCITED.mid -> 2_Bass_Blues_EXCITED.mid\n",
      "Input sequence length: 20\n",
      "Emotion token: C\n",
      "\n",
      "4: 3_Drum_Blues_RELAX.mid -> 3_Bass_Blues_RELAX.mid\n",
      "Input sequence length: 20\n",
      "Emotion token: R\n",
      "\n",
      "5: 4_Drum_PopRock_EXCITED.mid -> 4_Bass_PopRock_EXCITED.mid\n",
      "Input sequence length: 35\n",
      "Emotion token: C\n",
      "\n",
      "6: 5_Drum_PopRock_RELAX.mid -> 5_Bass_PopRock_RELAX.mid\n",
      "Input sequence length: 23\n",
      "Emotion token: R\n",
      "\n",
      "\n",
      "Number of input sequences: 171\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 92\n",
      "\n",
      "Number of output sequences: 171\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 87\n",
      "\n",
      "Input vocab: {'O': 0, '36fS': 1, '36f': 2, '36f_42pS': 3, '38fS': 4, '38f': 5, '38f_42pS': 6, '42fS_36fS': 7, '42fS_38fS': 8, '38f_42fS': 9, '38f_42f': 10, '36f_42pS_36fS': 11, '38f_42p': 12, '36fS_42fS': 13, '36f_42f': 14, '42pS_36fS': 15, '42p_36f': 16, '38fS_42fS': 17, '42pS': 18, '42p': 19, '36f_42fS': 20, '42f_38f': 21, '42f_36fS': 22, '36f_42p': 23, '42p_38fS': 24, '42p_38f_42fS': 25, '36fS_42pS': 26, '42f_36f': 27, '42f_36f_42pS': 28, '42f_36f_42p': 29, '38f_42pS_36fS': 30, '36f_42fS_38fS': 31, '42fS': 32, '42f': 33, '38f_36fS': 34, '38f_36f_42pS': 35, '36f_36fS_42pS': 36, '42p_36fS': 37, '42f_36f_42fS': 38, '38pS_42fS': 39, '38p_42f': 40, '38f_42f_36fS': 41, '38f_42f_36f_42pS': 42, 'C': 43, '42pS_38fS': 44, '36pS': 45, '36p': 46, '38fS_42pS': 47, '42pS_36pS': 48, '36p_42pS_38fS': 49, '38pS': 50, '38p': 51, '42pS_38pS': 52, '36f_42f_36fS': 53, '42f_42pS': 54, '36f_42p_38fS': 55, '36f_38fS': 56, '36f_38f_42fS': 57, '36f_42fS_36fS': 58, '36f_42f_36fS_42pS': 59, '42f_42p': 60, '42p_38pS': 61, '42p_38p_42fS': 62, '42p_36f_42fS': 63, 'R': 64, '42p_36f_42f': 65, '42p_38f_42f': 66, '42f_38fS': 67, '42f_38f_42fS': 68, '42f_38f_42f': 69, '42p_36f_42fS_38fS': 70, '42p_36f_42f_38f': 71, '42p_36f_38fS': 72, '42p_36f_38f_42fS': 73, '42p_36f_38f_42f': 74, '42f_36f_42fS_38fS': 75, '42f_36f_42f_38f': 76, '36f_42p_38f_42fS': 77, '42f_36f_42f': 78, '36f_42f_38fS': 79, '36f_42f_38f_42fS': 80, '36f_38f': 81, '42fS_38pS': 82, '42f_38p': 83, '36f_42fS_38pS': 84, '36f_42f_38p': 85, '36f_42f_38f': 86, '36f_38f_42f': 87, '38f_42f_38pS': 88, '38f_42f_38p': 89, '38f_42f_38p_42pS': 90, '42p_38p': 91}\n",
      "Output vocab: {'O': 0, '52fS': 1, '52f': 2, '55fS': 3, '55f': 4, '45fS': 5, '45f': 6, '48fS': 7, '48f': 8, '47fS': 9, '47f': 10, '43fS': 11, '43f': 12, '57fS': 13, '57f': 14, '50fS': 15, '50f': 16, '40fS': 17, '40f': 18, '52pS': 19, '52p': 20, '48pS': 21, '48p': 22, '43pS': 23, '43p': 24, '47pS': 25, '47p': 26, '67pS': 27, '67p': 28, '50pS': 29, '50p': 30, '40pS': 31, '40p': 32, '55pS': 33, '55p': 34, '59pS': 35, '59p': 36, '62pS': 37, '62p': 38, '60pS': 39, '60p': 40, '57pS': 41, '57p': 42, '53pS': 43, '53p': 44, '64pS': 45, '64p': 46, '45pS': 47, '45p': 48, '56fS': 49, '56f': 50, '59fS': 51, '59f': 52, '49fS': 53, '49f': 54, '51fS': 55, '51f': 56, '54fS': 57, '54f': 58, '58fS': 59, '58f': 60, '63fS': 61, '63f': 62, '64fS': 63, '64f': 64, '66fS': 65, '66f': 66, '61fS': 67, '61f': 68, '62fS': 69, '62f': 70, '56pS': 71, '56p': 72, '49pS': 73, '49p': 74, '51pS': 75, '51p': 76, '54pS': 77, '54p': 78, '58pS': 79, '58p': 80, '63pS': 81, '63p': 82, '66pS': 83, '66p': 84, '61pS': 85, '61p': 86}\n",
      "C\n",
      "36fS\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "38fS\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_36fS\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "38fS\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_36fS\n",
      "36fS\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_38fS\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_36fS\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_38fS\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_36fS\n",
      "36fS\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_38fS\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_36fS\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_38fS\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_36fS\n",
      "36fS\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "38fS\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f_42fS\n",
      "38f_42f\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "42fS_36fS\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f\n",
      "36f_42pS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "38fS\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f\n",
      "38f_42fS\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as \n",
    "    input:  drum_genere_emotion.mid \n",
    "    output: bass_genere_emotion.mid \n",
    "in the corresponding folders\n",
    "'''\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "\n",
    "input_filenames = glob.glob(os.path.join(DATASET_PATH, 'input/*.MID'))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = glob.glob(os.path.join(DATASET_PATH, 'output/*.MID'))\n",
    "print('Number of output files:', len(output_filenames), '\\n')\n",
    "\n",
    "INPUT_TOK = PrettyMidiTokenizer()\n",
    "OUTPUT_TOK = PrettyMidiTokenizer()\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    if 'RELAX' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['relax']\n",
    "    elif 'EXCITED' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['concentrate']\n",
    "    else:\n",
    "        raise Exception('Emotion not found in file name. Please add the emotion to the file name.')\n",
    "\n",
    "    in_seq, in_df = INPUT_TOK.midi_to_tokens(in_file, update_vocab=True, update_sequences=True, emotion_token = emotion_token)\n",
    "    out_seq, out_df = OUTPUT_TOK.midi_to_tokens(out_file, update_vocab=True, update_sequences=True)\n",
    "\n",
    "    print(f'Input sequence length: {len(in_seq)}')\n",
    "    print(f'Emotion token: {emotion_token}\\n')\n",
    "\n",
    "    if i == 0:\n",
    "        print(in_df)\n",
    "\n",
    "print(f'\\nNumber of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "print(f'\\nNumber of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "print('\\nInput vocab:', INPUT_TOK.VOCAB.word2idx)\n",
    "print('Output vocab:', OUTPUT_TOK.VOCAB.word2idx)\n",
    "    \n",
    "\n",
    "for t in INPUT_TOK.sequences[0]:\n",
    "    print(INPUT_TOK.VOCAB.idx2word[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of input sequences after data augmentation: 1197\n",
      "Number of output sequences after data augmentation: 1197\n"
     ]
    }
   ],
   "source": [
    "# Perform data augmentation\n",
    "input_shifts = [-3, -2, -1, 1, 2, 3]\n",
    "output_shifts = list(np.zeros(len(input_shifts)))\n",
    "\n",
    "INPUT_TOK.data_augmentation_shift(input_shifts)\n",
    "OUTPUT_TOK.data_augmentation_shift(output_shifts)\n",
    "\n",
    "print(f'\\nNumber of input sequences after data augmentation: {len(INPUT_TOK.sequences)}')\n",
    "print(f'Number of output sequences after data augmentation: {len(OUTPUT_TOK.sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 719\n",
      "Evaluation set size: 239\n",
      "Test set size: 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gianni\\AppData\\Local\\Temp\\ipykernel_21196\\3823352634.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "# Split the dataset into training, evaluation and test sets\n",
    "train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_sampler = RandomSampler(train_set)          \n",
    "train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "eval_sampler = RandomSampler(eval_set)\n",
    "eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = RandomSampler(test_set)\n",
    "test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train set size: {len(train_set)}')\n",
    "print(f'Evaluation set size: {len(eval_set)}')\n",
    "print(f'Test set size: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created: TCN(\n",
      "  (encoder): Embedding(92, 20, padding_idx=0)\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(20, 192, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (5): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (6): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(192, 20, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=20, out_features=87, bias=True)\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111\n",
    "OUTPUT_SIZE = len(OUTPUT_TOK.VOCAB) \n",
    "\n",
    "\n",
    "'''\n",
    "IMPORTANT:\n",
    "to cover all the sequence of tokens k * d must be >= hidden units (see the paper)\n",
    "k = kernel_size\n",
    "d = dilation = 2 ^ (n_levels - 1) \n",
    "'''\n",
    "if FEEDBACK:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) + OUTPUT_SIZE\n",
    "    LEVELS = 8\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH * 2 # 192 * 2 = 384\n",
    "else:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) \n",
    "    LEVELS = 7\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH # 192 \n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] real value vectors (see model.py)\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE] # [192, 192, 192, 192, 192, 192, 20]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# balance the loss function by assigning a weight to each token related to its frequency\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_SIZE], dtype=torch.float, device=device)\n",
    "OUTPUT_TOK.VOCAB.compute_weights()\n",
    "for i, weigth in enumerate(OUTPUT_TOK.VOCAB.weights):\n",
    "    LOSS_WEIGTHS[i] = 1 - weigth\n",
    "    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = INPUT_SIZE,\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            emphasize_eeg = EMPHAZISE_EEG,\n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f'\\nModel created: {model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters():\n",
    "    # plot the losses over the epochs \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(eval_losses, label='eval')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))\n",
    "\n",
    "\n",
    "    # save the vocabularies\n",
    "    INPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'input_vocab.txt'))\n",
    "    OUTPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'output_vocab.txt'))\n",
    "\n",
    "     # save the model hyperparameters in a file txt\n",
    "    with open(os.path.join(RESULTS_PATH, 'model_hyperparameters.txt'), 'w') as f:\n",
    "\n",
    "        f.write(f'----------OPTIMIZATION PARAMETERS----------\\n')\n",
    "        f.write(f'DATE: {time.strftime(\"%Y%m%d-%H%M%S\")}\\n')\n",
    "        f.write(f'DATASET_PATH: {DATASET_PATH}\\n')\n",
    "        f.write(f'FEEDBACK: {FEEDBACK}\\n')\n",
    "        f.write(f'SEED: {SEED}\\n')\n",
    "        f.write(f'INPUT_SIZE: {INPUT_SIZE}\\n')\n",
    "        f.write(f'EMBEDDING_SIZE: {EMBEDDING_SIZE}\\n')\n",
    "        f.write(f'LEVELS: {LEVELS}\\n')\n",
    "        f.write(f'HIDDEN_UNITS: {HIDDEN_UNITS}\\n')\n",
    "        f.write(f'NUM_CHANNELS: {NUM_CHANNELS}\\n')\n",
    "        f.write(f'OUTPUT_SIZE: {OUTPUT_SIZE}\\n')\n",
    "        f.write(f'LOSS_WEIGTHS: {LOSS_WEIGTHS}\\n')\n",
    "        f.write(f'LEARNING_RATE: {LEARNING_RATE}\\n')\n",
    "        f.write(f'BATCH_SIZE: {BATCH_SIZE}\\n')\n",
    "        f.write(f'EPOCHS: {EPOCHS}\\n')\n",
    "        f.write(f'GRADIENT_CLIP: {GRADIENT_CLIP}\\n')\n",
    "        f.write(f'------------------------------------------\\n')\n",
    "        f.write(f'----------RESULTS----------\\n')\n",
    "        f.write(f'BEST_TRAIN_LOSSES: {best_train_loss}\\n')\n",
    "        f.write(f'BEST_EVAL_LOSS: {best_eval_loss}\\n')\n",
    "        f.write(f'TEST_LOSS: {test_loss}\\n')\n",
    "        f.write(f'BEST_MODEL_EPOCH: {best_model_epoch}\\n')\n",
    "        f.write(f'------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if FEEDBACK:\n",
    "        prev_output = torch.zeros([BATCH_SIZE, INPUT_TOK.SEQ_LENGTH], dtype=torch.long, device=device)\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data\n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long, device=device)), dim = 1) \n",
    "\n",
    "        if FEEDBACK:\n",
    "            input = torch.cat((data_masked, prev_output[:batch_size, :]), dim = 1)\n",
    "        else:\n",
    "            input = data_masked\n",
    "           \n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(input)[:, :INPUT_TOK.SEQ_LENGTH] \n",
    "        prev_output = torch.argmax(output, 2)# batch, seq_len (hidden units), vocab_size\n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "    \n",
    "        final_target = targets.contiguous().view(-1)    \n",
    "        final_output = output.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        if mode == 'train':\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # clip the gradients to avoid exploding gradients\n",
    "            if GRADIENT_CLIP > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eval_loss = 1e8\n",
    "best_train_loss = 1e8\n",
    "best_model_epoch = 0\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "early_stop = True\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "\n",
    "    RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results', time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    if not os.path.exists(RESULTS_PATH):\n",
    "        os.makedirs(RESULTS_PATH)\n",
    "        \n",
    "    MODEL_PATH = os.path.join(RESULTS_PATH, 'model_state_dict.pth')\n",
    "\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "        \n",
    "        eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if eval_loss < best_eval_loss:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_eval_loss = eval_loss\n",
    "            best_model_epoch = epoch \n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "\n",
    "        # # Anneal the learning rate if the validation loss plateaus\n",
    "        # if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n",
    "        #     lr = lr / 2.\n",
    "        #     if lr < 0.1:\n",
    "        #         lr = 2\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = lr\n",
    "\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop:\n",
    "            if epoch > 15:\n",
    "                if min(eval_losses[-15:]) > best_eval_loss:\n",
    "                    break\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n",
    "\n",
    "    print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d} \\n\\n' \\\n",
    "            .format(best_eval_loss, best_model_epoch))\n",
    "    \n",
    "    # test the model\n",
    "    test_loss = epoch_step(test_dataloader, 'eval')\n",
    "    print(f'\\n\\nTEST LOSS: {test_loss}')\n",
    "    save_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 2, 2, 0, 18, 19, 19, 19, 24, 25, 5, 5, 5, 5, 5, 0, 18, 19,\n",
      "       19, 19, 37, 63, 65, 2, 2, 2, 2, 0, 18, 19, 19, 19, 24, 25, 66, 5,\n",
      "       5, 5, 5, 0, 0, 18, 19, 19, 19, 7, 27, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0,\n",
      "       8, 21, 5, 5, 5, 5, 5, 0, 18, 19, 19, 19, 7, 27, 2, 2, 2, 2, 2, 0,\n",
      "       18, 19, 19, 19, 8, 21, 5, 5, 5, 5, 5, 18, 19, 19, 19, 19, 7, 27, 1,\n",
      "       2, 2, 2, 0, 32, 33, 33, 33, 33, 8, 21, 5, 5, 5, 5, 5, 0, 18, 19,\n",
      "       19, 37, 63, 65, 2, 2, 2, 2, 0, 18, 19, 19, 19, 8, 21, 21, 5, 5, 5,\n",
      "       5, 0, 0, 18, 19, 19, 19, 7, 27, 1, 2, 2, 2, 2, 32, 33, 33, 33, 67,\n",
      "       68, 69, 5, 5, 5, 5, 0, 18, 19, 19, 19, 37, 63, 65, 2, 2, 2, 0, 0,\n",
      "       15, 16, 16, 16, 70, 71, 71, 5, 5, 5, 5, 0, 18, 19, 19, 19, 19, 7,\n",
      "       27], dtype=object)]\n",
      "MIDI file saved at models/model\\predicted_blues.mid\n",
      "[array([13, 14, 14, 33, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33, 33, 0, 0, 0, 0,\n",
      "       0, 0, 17, 10, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 32, 33, 33, 33,\n",
      "       33, 33, 0, 0, 0, 0, 0, 7, 27, 27, 27, 7, 27, 27, 0, 0, 0, 0, 0, 32,\n",
      "       33, 33, 33, 33, 33, 33, 0, 0, 0, 0, 0, 17, 10, 10, 10, 10, 10, 10,\n",
      "       0, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33, 33, 0, 0, 0, 0, 0, 1, 20, 14,\n",
      "       14, 13, 14, 14, 0, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33, 33, 0, 0, 0,\n",
      "       0, 0, 4, 9, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33,\n",
      "       33, 33, 0, 0, 0, 0, 0, 7, 27, 27, 27, 7, 27, 27, 0, 0, 0, 0, 0, 32,\n",
      "       33, 33, 33, 33, 33, 33, 0, 0, 0, 0, 0, 0, 17, 10, 10, 10, 10, 10,\n",
      "       10, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33, 33, 33, 0, 0, 0, 0, 0, 13,\n",
      "       14, 14], dtype=object)]\n",
      "MIDI file saved at models/model\\predicted_rock_relax.mid\n",
      "[array([1, 2, 2, 2, 0, 0, 0, 0, 0, 44, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 18,\n",
      "       45, 46, 46, 46, 46, 45, 46, 0, 0, 0, 0, 4, 5, 5, 5, 5, 5, 5, 0, 0,\n",
      "       0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 47, 12, 12, 12, 12,\n",
      "       12, 12, 0, 0, 0, 0, 0, 48, 46, 46, 46, 46, 46, 45, 0, 0, 0, 0, 4,\n",
      "       9, 10, 10, 10, 10, 10, 33, 0, 0, 0, 0, 48, 46, 46, 45, 46, 46, 46,\n",
      "       0, 0, 0, 0, 0, 4, 6, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 15, 2, 2,\n",
      "       2, 2, 1, 2, 0, 0, 0, 0, 0, 44, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0,\n",
      "       15, 16, 15, 16, 16, 16, 16, 0, 0, 0, 0, 0, 47, 12, 12, 12, 12, 12,\n",
      "       37, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 45, 46, 46, 46, 46, 46, 49, 5,\n",
      "       5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 15, 2], dtype=object)]\n",
      "MIDI file saved at models/model\\predicted_rock_excited.mid\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "RESULTS_PATH = 'models/model'\n",
    "MODEL_PATH = f'{RESULTS_PATH}/model_state_dict.pth'\n",
    "\n",
    "INPUT_TOK.load_vocab(f'{RESULTS_PATH}/input_vocab.txt')\n",
    "OUTPUT_TOK.load_vocab(f'{RESULTS_PATH}/output_vocab.txt')\n",
    "\n",
    "model = TCN(input_size = len(INPUT_TOK.VOCAB),\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = len(OUTPUT_TOK.VOCAB), \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# select a genere to be predicted\n",
    "generes = ['blues', 'rock_relax', 'rock_excited']\n",
    "\n",
    "for genere in generes:\n",
    "    # get a sample to be predicted\n",
    "    sample_path = os.path.join(DATASET_PATH, f'test/drum_{genere}_2.mid')\n",
    "    sample = INPUT_TOK.midi_to_tokens(sample_path, update_vocab=False) [0]\n",
    "\n",
    "    print(sample)\n",
    "    sample = torch.LongTensor(sample)\n",
    "\n",
    "    # Get the last sequence from the batch and unsqueeze it to add a batch dimension.\n",
    "    sample = sample[-1].unsqueeze(0)\n",
    "\n",
    "    # Mask the last bar of the input data.\n",
    "    sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "    # Make the prediction.\n",
    "    prediction = model(sample.to(device))\n",
    "    prediction = prediction.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "    # Get the predicted tokens.\n",
    "    predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "    # Get the predicted sequence.\n",
    "    predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "    # Convert the predicted sequence to MIDI.\n",
    "    out_file_path = os.path.join(RESULTS_PATH, f'predicted_{genere}.mid')\n",
    "    pitch_ticks_velocity =  OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path = out_file_path, ticks_filter = 3, instrument_name = 'Electric Bass (finger)') \n",
    "\n",
    "\n",
    "# # check \n",
    "# predicted_sequence_string = []\n",
    "# for id in predicted_sequence:\n",
    "#     predicted_sequence_string.append(OUTPUT_TOK.VOCAB.idx2word[id])\n",
    "# print(predicted_sequence_string)\n",
    "# print(pitch_ticks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
