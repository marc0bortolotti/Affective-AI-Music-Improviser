{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "/home/marco_bortolotti/project/Affective-AI-Music-Improviser/TCN\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from APPLICATION.model.tokenization import PrettyMidiTokenizer, BCI_TOKENS\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 1 # 4\n",
    "BATCH_SIZE = 4 # 16\n",
    "TRAIN_MODEL = True\n",
    "FEEDBACK = False\n",
    "EMPHASIZE_EEG = True\n",
    "EARLY_STOP = True\n",
    "\n",
    "pwd = os.getcwd()\n",
    "print(pwd)\n",
    "\n",
    "DIRECTORY_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: dataset\n",
      "Number of input files: 6\n",
      "Number of output files: 6 \n",
      "\n",
      "1: 0_Drum_HardRock_EXCITED.mid -> 0_Bass_HardRock_EXCITED.mid\n",
      "Input sequence length: 26\n",
      "Emotion token: C\n",
      "\n",
      "2: 1_Drum_HardRock_EXCITED.mid -> 1_Bass_HardRock_EXCITED.mid\n",
      "Input sequence length: 32\n",
      "Emotion token: C\n",
      "\n",
      "3: 2_Drum_Blues_EXCITED.mid -> 2_Bass_Blues_EXCITED.mid\n",
      "Input sequence length: 20\n",
      "Emotion token: C\n",
      "\n",
      "4: 3_Drum_Blues_EXCITED.mid -> 3_Bass_Blues_EXCITED.mid\n",
      "Input sequence length: 20\n",
      "Emotion token: C\n",
      "\n",
      "5: 4_Drum_PopRock_RELAX.mid -> 4_Bass_PopRock_RELAX.mid\n",
      "Input sequence length: 35\n",
      "Emotion token: R\n",
      "\n",
      "6: 5_Drum_PopRock_RELAX.mid -> 5_Bass_PopRock_RELAX.mid\n",
      "Input sequence length: 23\n",
      "Emotion token: R\n",
      "\n",
      "\n",
      "Number of input sequences: 171\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 100\n",
      "\n",
      "Number of output sequences: 171\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 87\n",
      "\n",
      "Input vocab: {'O': 0, 'R': 1, 'C': 2, '36fS': 3, '36f': 4, '36f_42pS': 5, '38fS': 6, '38f': 7, '38f_42pS': 8, '36fS_42fS': 9, '38fS_42fS': 10, '38f_42fS': 11, '38f_42f': 12, '36f_36fS_42pS': 13, '38f_42p': 14, '36f_42f': 15, '36fS_42f_42pS': 16, '36f_42f_42p': 17, '38f_42f_42pS': 18, '42f_42pS': 19, '42f_42p': 20, '36f_42fS': 21, '42pS': 22, '42p': 23, '36fS_42f': 24, '36f_42p': 25, '38fS_42p': 26, '38f_42fS_42p': 27, '36f_36fS': 28, '36f_36f_42pS': 29, '36f_42f_42pS': 30, '36fS_38f_42pS': 31, '36f_38fS_42fS': 32, '42f_42fS': 33, '42f_42f': 34, '38fS_42f_42fS': 35, '38f_42f_42f': 36, '36fS_38f': 37, '36f_38f_42pS': 38, '36fS_42p': 39, '36f_38fS_42f_42fS': 40, '36fS_42pS': 41, '38f_42f_42fS': 42, '42fS': 43, '42f': 44, '36fS_42f_42fS': 45, '36f_42f_42fS': 46, '36f_36fS_42fS': 47, '36fS_38f_42f_42fS': 48, '36f_38f_42f_42f': 49, '38pS_42fS': 50, '38p_42f': 51, '36fS_38f_42f': 52, '36f_38f_42f_42pS': 53, '36fS_38f_42f_42pS': 54, '38fS_42pS': 55, '36pS': 56, '36p': 57, '36p_36pS': 58, '36p_36p': 59, '36pS_42pS': 60, '36f_36f': 61, '36p_38fS_42pS': 62, '36f_36fS_42p': 63, '36fS_36p': 64, '38pS': 65, '38p': 66, '38pS_42pS': 67, '36f_36fS_42f': 68, '36f_38fS_42p': 69, '36f_38fS': 70, '36f_38f_42fS': 71, '42fS_42p': 72, '36f_36fS_42f_42pS': 73, '36fS_42fS_42p': 74, '38pS_42p': 75, '38p_42fS_42p': 76, '38p_42pS': 77, '36f_42fS_42p': 78, '38f_42f_42p': 79, '38fS_42fS_42p': 80, '38fS_42f': 81, '36f_38fS_42fS_42p': 82, '36f_38f_42f_42p': 83, '36f_38f_42fS_42p': 84, '36f_42f_42f': 85, '36f_38fS_42f': 86, '36f_38f_42f_42fS': 87, '36f_38f': 88, '38pS_42fS_42p': 89, '38p_42f_42p': 90, '36f_38pS_42fS': 91, '36f_38p_42f': 92, '36fS_42p_42pS': 93, '36f_42p_42p': 94, '36f_38f_42f': 95, '38f_38pS_42f': 96, '38f_38p_42f': 97, '38f_38p_42f_42pS': 98, '38p_42p': 99}\n",
      "Output vocab: {'O': 0, '52fS': 1, '52f': 2, '55fS': 3, '55f': 4, '45fS': 5, '45f': 6, '48fS': 7, '48f': 8, '47fS': 9, '47f': 10, '43fS': 11, '43f': 12, '57fS': 13, '57f': 14, '50fS': 15, '50f': 16, '40fS': 17, '40f': 18, '52pS': 19, '52p': 20, '48pS': 21, '48p': 22, '43pS': 23, '43p': 24, '47pS': 25, '47p': 26, '67pS': 27, '67p': 28, '50pS': 29, '50p': 30, '40pS': 31, '40p': 32, '55pS': 33, '55p': 34, '59pS': 35, '59p': 36, '62pS': 37, '62p': 38, '60pS': 39, '60p': 40, '57pS': 41, '57p': 42, '53pS': 43, '53p': 44, '64pS': 45, '64p': 46, '45pS': 47, '45p': 48, '56fS': 49, '56f': 50, '59fS': 51, '59f': 52, '49fS': 53, '49f': 54, '51fS': 55, '51f': 56, '54fS': 57, '54f': 58, '58fS': 59, '58f': 60, '63fS': 61, '63f': 62, '64fS': 63, '64f': 64, '66fS': 65, '66f': 66, '61fS': 67, '61f': 68, '62fS': 69, '62f': 70, '56pS': 71, '56p': 72, '49pS': 73, '49p': 74, '51pS': 75, '51p': 76, '54pS': 77, '54p': 78, '58pS': 79, '58p': 80, '63pS': 81, '63p': 82, '66pS': 83, '66p': 84, '61pS': 85, '61p': 86}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as \n",
    "    input:  drum_genere_emotion.mid \n",
    "    output: bass_genere_emotion.mid \n",
    "in the corresponding folders\n",
    "'''\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "\n",
    "print('Dataset path:', DATASET_PATH)\n",
    "\n",
    "input_filenames = sorted(glob.glob(os.path.join(DATASET_PATH, 'input/*.mid')))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = sorted(glob.glob(os.path.join(DATASET_PATH, 'output/*.mid')))\n",
    "print('Number of output files:', len(output_filenames), '\\n')\n",
    "\n",
    "INPUT_TOK = PrettyMidiTokenizer(eeg=True)\n",
    "OUTPUT_TOK = PrettyMidiTokenizer()\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    if 'RELAX' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['relax']\n",
    "    elif 'EXCITED' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['concentrate']\n",
    "    else:\n",
    "        raise Exception('Emotion not found in file name. Please add the emotion to the file name.')\n",
    "\n",
    "    in_seq, in_df = INPUT_TOK.midi_to_tokens(in_file, update_vocab=True, update_sequences=True, emotion_token = emotion_token)\n",
    "    out_seq, out_df = OUTPUT_TOK.midi_to_tokens(out_file, update_vocab=True, update_sequences=True)\n",
    "\n",
    "    print(f'Input sequence length: {len(in_seq)}')\n",
    "    print(f'Emotion token: {emotion_token}\\n')\n",
    "\n",
    "\n",
    "print(f'\\nNumber of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "print(f'\\nNumber of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "print('\\nInput vocab:', INPUT_TOK.VOCAB.word2idx)\n",
    "print('Output vocab:', OUTPUT_TOK.VOCAB.word2idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of input sequences after data augmentation: 1197\n",
      "Number of output sequences after data augmentation: 1197\n"
     ]
    }
   ],
   "source": [
    "# Perform data augmentation\n",
    "input_shifts = [-3, -2, -1, 1, 2, 3]\n",
    "output_shifts = list(np.zeros(len(input_shifts)))\n",
    "\n",
    "INPUT_TOK.data_augmentation_shift(input_shifts)\n",
    "OUTPUT_TOK.data_augmentation_shift(output_shifts)\n",
    "\n",
    "print(f'\\nNumber of input sequences after data augmentation: {len(INPUT_TOK.sequences)}')\n",
    "print(f'Number of output sequences after data augmentation: {len(OUTPUT_TOK.sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 719\n",
      "Evaluation set size: 239\n",
      "Test set size: 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gianni\\AppData\\Local\\Temp\\ipykernel_21196\\3823352634.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "# Split the dataset into training, evaluation and test sets\n",
    "train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_sampler = RandomSampler(train_set)          \n",
    "train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "eval_sampler = RandomSampler(eval_set)\n",
    "eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = RandomSampler(test_set)\n",
    "test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train set size: {len(train_set)}')\n",
    "print(f'Evaluation set size: {len(eval_set)}')\n",
    "print(f'Test set size: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created: TCN(\n",
      "  (encoder): Embedding(92, 20, padding_idx=0)\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(20, 192, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (5): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (6): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(192, 20, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=20, out_features=87, bias=True)\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111\n",
    "OUTPUT_SIZE = len(OUTPUT_TOK.VOCAB) \n",
    "\n",
    "\n",
    "'''\n",
    "IMPORTANT:\n",
    "to cover all the sequence of tokens k * d must be >= hidden units (see the paper)\n",
    "k = kernel_size\n",
    "d = dilation = 2 ^ (n_levels - 1) \n",
    "'''\n",
    "if FEEDBACK:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) + OUTPUT_SIZE\n",
    "    LEVELS = 8\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH * 2 # 192 * 2 = 384\n",
    "else:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) \n",
    "    LEVELS = 7\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH # 192 \n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] real value vectors (see model.py)\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE] # [192, 192, 192, 192, 192, 192, 20]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# balance the loss function by assigning a weight to each token related to its frequency\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_SIZE], dtype=torch.float, device=device)\n",
    "OUTPUT_TOK.VOCAB.compute_weights()\n",
    "for i, weigth in enumerate(OUTPUT_TOK.VOCAB.weights):\n",
    "    LOSS_WEIGTHS[i] = 1 - weigth\n",
    "    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = INPUT_SIZE,\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            emphasize_eeg = EMPHAZISE_EEG,\n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f'\\nModel created: {model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters():\n",
    "    # plot the losses over the epochs \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(eval_losses, label='eval')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    # save the vocabularies\n",
    "    INPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'input_vocab.txt'))\n",
    "    OUTPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'output_vocab.txt'))\n",
    "\n",
    "     # save the model hyperparameters in a file txt\n",
    "    with open(os.path.join(RESULTS_PATH, 'statistics.txt'), 'w') as f:\n",
    "\n",
    "        f.write(f'----------OPTIMIZATION PARAMETERS----------\\n')\n",
    "        f.write(f'DATE: {time.strftime(\"%Y%m%d-%H%M%S\")}\\n')\n",
    "        f.write(f'DATASET_PATH: {DATASET_PATH}\\n')\n",
    "        f.write(f'FEEDBACK: {FEEDBACK}\\n')\n",
    "        f.write(f'SEED: {SEED}\\n')\n",
    "        f.write(f'INPUT_SIZE: {INPUT_SIZE}\\n')\n",
    "        f.write(f'EMBEDDING_SIZE: {EMBEDDING_SIZE}\\n')\n",
    "        f.write(f'LEVELS: {LEVELS}\\n')\n",
    "        f.write(f'HIDDEN_UNITS: {HIDDEN_UNITS}\\n')\n",
    "        f.write(f'NUM_CHANNELS: {NUM_CHANNELS}\\n')\n",
    "        f.write(f'OUTPUT_SIZE: {OUTPUT_SIZE}\\n')\n",
    "        f.write(f'LOSS_WEIGTHS: {LOSS_WEIGTHS}\\n')\n",
    "        f.write(f'LEARNING_RATE: {LEARNING_RATE}\\n')\n",
    "        f.write(f'BATCH_SIZE: {BATCH_SIZE}\\n')\n",
    "        f.write(f'EPOCHS: {EPOCHS}\\n')\n",
    "        f.write(f'GRADIENT_CLIP: {GRADIENT_CLIP}\\n')\n",
    "        f.write(f'------------------------------------------\\n')\n",
    "        f.write(f'----------RESULTS----------\\n')\n",
    "        f.write(f'BEST_TRAIN_LOSSES: {best_train_loss}\\n')\n",
    "        f.write(f'BEST_EVAL_LOSS: {best_eval_loss}\\n')\n",
    "        f.write(f'TEST_LOSS: {test_loss}\\n')\n",
    "        f.write(f'BEST_MODEL_EPOCH: {best_model_epoch}\\n')\n",
    "        f.write(f'------------------------------------------\\n')\n",
    "\n",
    "import yaml\n",
    "\n",
    "data = {\n",
    "    'DATE': time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    'DATASET_PATH': DATASET_PATH,\n",
    "    'FEEDBACK': FEEDBACK,\n",
    "    'SEED': SEED,\n",
    "    'INPUT_SIZE': INPUT_SIZE,\n",
    "    'EMBEDDING_SIZE': EMBEDDING_SIZE,\n",
    "    'LEVELS': LEVELS,\n",
    "    'HIDDEN_UNITS': HIDDEN_UNITS,\n",
    "    'NUM_CHANNELS': NUM_CHANNELS,\n",
    "    'OUTPUT_SIZE': OUTPUT_SIZE,\n",
    "    'LOSS_WEIGTHS': LOSS_WEIGTHS,\n",
    "    'LEARNING_RATE': LEARNING_RATE,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'EPOCHS': EPOCHS,\n",
    "    'GRADIENT_CLIP': GRADIENT_CLIP,\n",
    "    'INPUT_VOCAB': INPUT_TOK.VOCAB,\n",
    "    'OUTPUT_VOCAB': OUTPUT_TOK.VOCAB,\n",
    "    'CRITERION': criterion,\n",
    "    'OPTIMIZER': optimizer,\n",
    "    'MODEL' : model\n",
    "}\n",
    "\n",
    "RESULTS_PATH = './'\n",
    "path = os.path.join(RESULTS_PATH, 'config.yaml')\n",
    "with open(path, 'w') as file:\n",
    "    yaml.safe_dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if FEEDBACK:\n",
    "        prev_output = torch.zeros([BATCH_SIZE, INPUT_TOK.SEQ_LENGTH], dtype=torch.long, device=device)\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data\n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long, device=device)), dim = 1) \n",
    "\n",
    "        if FEEDBACK:\n",
    "            input = torch.cat((data_masked, prev_output[:batch_size, :]), dim = 1)\n",
    "        else:\n",
    "            input = data_masked\n",
    "           \n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(input)[:, :INPUT_TOK.SEQ_LENGTH] \n",
    "        prev_output = torch.argmax(output, 2)# batch, seq_len (hidden units), vocab_size\n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "    \n",
    "        final_target = targets.contiguous().view(-1)    \n",
    "        final_output = output.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        if mode == 'train':\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # clip the gradients to avoid exploding gradients\n",
    "            if GRADIENT_CLIP > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eval_loss = 1e8\n",
    "best_train_loss = 1e8\n",
    "best_model_epoch = 0\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "early_stop = True\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "\n",
    "    RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results', time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    if not os.path.exists(RESULTS_PATH):\n",
    "        os.makedirs(RESULTS_PATH)\n",
    "        \n",
    "    MODEL_PATH = os.path.join(RESULTS_PATH, 'model_state_dict.pth')\n",
    "\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "        \n",
    "        eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if eval_loss < best_eval_loss:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_eval_loss = eval_loss\n",
    "            best_model_epoch = epoch \n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "\n",
    "        # # Anneal the learning rate if the validation loss plateaus\n",
    "        # if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n",
    "        #     lr = lr / 2.\n",
    "        #     if lr < 0.1:\n",
    "        #         lr = 2\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = lr\n",
    "\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop:\n",
    "            if epoch > 15:\n",
    "                if min(eval_losses[-15:]) > best_eval_loss:\n",
    "                    break\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n",
    "\n",
    "    print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d} \\n\\n' \\\n",
    "            .format(best_eval_loss, best_model_epoch))\n",
    "    \n",
    "    # test the model\n",
    "    test_loss = epoch_step(test_dataloader, 'eval')\n",
    "    print(f'\\n\\nTEST LOSS: {test_loss}')\n",
    "    save_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 2, 2, 0, 18, 19, 19, 19, 24, 25, 5, 5, 5, 5, 5, 0, 18, 19,\n",
      "       19, 19, 37, 63, 65, 2, 2, 2, 2, 0, 18, 19, 19, 19, 24, 25, 66, 5,\n",
      "       5, 5, 5, 0, 0, 18, 19, 19, 19, 7, 27, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0,\n",
      "       8, 21, 5, 5, 5, 5, 5, 0, 18, 19, 19, 19, 7, 27, 2, 2, 2, 2, 2, 0,\n",
      "       18, 19, 19, 19, 8, 21, 5, 5, 5, 5, 5, 18, 19, 19, 19, 19, 7, 27, 1,\n",
      "       2, 2, 2, 0, 32, 33, 33, 33, 33, 8, 21, 5, 5, 5, 5, 5, 0, 18, 19,\n",
      "       19, 37, 63, 65, 2, 2, 2, 2, 0, 18, 19, 19, 19, 8, 21, 21, 5, 5, 5,\n",
      "       5, 0, 0, 18, 19, 19, 19, 7, 27, 1, 2, 2, 2, 2, 32, 33, 33, 33, 67,\n",
      "       68, 69, 5, 5, 5, 5, 0, 18, 19, 19, 19, 37, 63, 65, 2, 2, 2, 0, 0,\n",
      "       15, 16, 16, 16, 70, 71, 71, 5, 5, 5, 5, 0, 18, 19, 19, 19, 19, 7,\n",
      "       27], dtype=object)]\n",
      "MIDI file saved at models/model\\predicted_blues.mid\n",
      "[array([13, 14, 14, 33, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33, 33, 0, 0, 0, 0,\n",
      "       0, 0, 17, 10, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 32, 33, 33, 33,\n",
      "       33, 33, 0, 0, 0, 0, 0, 7, 27, 27, 27, 7, 27, 27, 0, 0, 0, 0, 0, 32,\n",
      "       33, 33, 33, 33, 33, 33, 0, 0, 0, 0, 0, 17, 10, 10, 10, 10, 10, 10,\n",
      "       0, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33, 33, 0, 0, 0, 0, 0, 1, 20, 14,\n",
      "       14, 13, 14, 14, 0, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33, 33, 0, 0, 0,\n",
      "       0, 0, 4, 9, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33,\n",
      "       33, 33, 0, 0, 0, 0, 0, 7, 27, 27, 27, 7, 27, 27, 0, 0, 0, 0, 0, 32,\n",
      "       33, 33, 33, 33, 33, 33, 0, 0, 0, 0, 0, 0, 17, 10, 10, 10, 10, 10,\n",
      "       10, 0, 0, 0, 0, 0, 32, 33, 33, 33, 33, 33, 33, 0, 0, 0, 0, 0, 13,\n",
      "       14, 14], dtype=object)]\n",
      "MIDI file saved at models/model\\predicted_rock_relax.mid\n",
      "[array([1, 2, 2, 2, 0, 0, 0, 0, 0, 44, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 18,\n",
      "       45, 46, 46, 46, 46, 45, 46, 0, 0, 0, 0, 4, 5, 5, 5, 5, 5, 5, 0, 0,\n",
      "       0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 47, 12, 12, 12, 12,\n",
      "       12, 12, 0, 0, 0, 0, 0, 48, 46, 46, 46, 46, 46, 45, 0, 0, 0, 0, 4,\n",
      "       9, 10, 10, 10, 10, 10, 33, 0, 0, 0, 0, 48, 46, 46, 45, 46, 46, 46,\n",
      "       0, 0, 0, 0, 0, 4, 6, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 15, 2, 2,\n",
      "       2, 2, 1, 2, 0, 0, 0, 0, 0, 44, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0,\n",
      "       15, 16, 15, 16, 16, 16, 16, 0, 0, 0, 0, 0, 47, 12, 12, 12, 12, 12,\n",
      "       37, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 45, 46, 46, 46, 46, 46, 49, 5,\n",
      "       5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 15, 2], dtype=object)]\n",
      "MIDI file saved at models/model\\predicted_rock_excited.mid\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "RESULTS_PATH = 'models/model'\n",
    "MODEL_PATH = f'{RESULTS_PATH}/model_state_dict.pth'\n",
    "\n",
    "INPUT_TOK.load_vocab(f'{RESULTS_PATH}/input_vocab.txt')\n",
    "OUTPUT_TOK.load_vocab(f'{RESULTS_PATH}/output_vocab.txt')\n",
    "\n",
    "model = TCN(input_size = len(INPUT_TOK.VOCAB),\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = len(OUTPUT_TOK.VOCAB), \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# select a genere to be predicted\n",
    "generes = ['blues', 'rock_relax', 'rock_excited']\n",
    "\n",
    "for genere in generes:\n",
    "    # get a sample to be predicted\n",
    "    sample_path = os.path.join(DATASET_PATH, f'test/drum_{genere}_2.mid')\n",
    "    sample = INPUT_TOK.midi_to_tokens(sample_path, update_vocab=False) [0]\n",
    "\n",
    "    print(sample)\n",
    "    sample = torch.LongTensor(sample)\n",
    "\n",
    "    # Get the last sequence from the batch and unsqueeze it to add a batch dimension.\n",
    "    sample = sample[-1].unsqueeze(0)\n",
    "\n",
    "    # Mask the last bar of the input data.\n",
    "    sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "    # Make the prediction.\n",
    "    prediction = model(sample.to(device))\n",
    "    prediction = prediction.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "    # Get the predicted tokens.\n",
    "    predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "    # Get the predicted sequence.\n",
    "    predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "    # Convert the predicted sequence to MIDI.\n",
    "    out_file_path = os.path.join(RESULTS_PATH, f'predicted_{genere}.mid')\n",
    "    pitch_ticks_velocity =  OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path = out_file_path, ticks_filter = 3, instrument_name = 'Electric Bass (finger)') \n",
    "\n",
    "\n",
    "# # check \n",
    "# predicted_sequence_string = []\n",
    "# for id in predicted_sequence:\n",
    "#     predicted_sequence_string.append(OUTPUT_TOK.VOCAB.idx2word[id])\n",
    "# print(predicted_sequence_string)\n",
    "# print(pitch_ticks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
