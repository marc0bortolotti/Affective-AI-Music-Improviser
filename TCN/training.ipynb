{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from MIDI.PRETTY_MIDI.pretty_midi_tokenization import PrettyMidiTokenizer, BCI_TOKENS\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 2 # 4\n",
    "BATCH_SIZE = 16 # 16\n",
    "TRAIN_MODEL = True\n",
    "FEEDBACK = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 6\n",
      "Number of output files: 6 \n",
      "\n",
      "1: 0_Drum_HardRock_EXCITED.mid -> 0_Bass_HardRock_EXCITED.mid\n",
      "Input sequence length: 26\n",
      "Emotion token: C\n",
      "\n",
      "2: 1_Drum_HardRock_RELAX.mid -> 1_Bass_HardRock_RELAX.mid\n",
      "Input sequence length: 32\n",
      "Emotion token: R\n",
      "\n",
      "3: 2_Drum_Blues_EXCITED.mid -> 2_Bass_Blues_EXCITED.mid\n",
      "Input sequence length: 31\n",
      "Emotion token: C\n",
      "\n",
      "4: 3_Drum_Blues_RELAX.mid -> 3_Bass_Blues_RELAX.mid\n",
      "Input sequence length: 23\n",
      "Emotion token: R\n",
      "\n",
      "5: 4_Drum_PopRock_EXCITED.mid -> 4_Bass_PopRock_EXCITED.mid\n",
      "Input sequence length: 35\n",
      "Emotion token: C\n",
      "\n",
      "6: 5_Drum_PopRock_RELAX.mid -> 5_Bass_PopRock_RELAX.mid\n",
      "Input sequence length: 23\n",
      "Emotion token: R\n",
      "\n",
      "\n",
      "Number of input sequences: 170\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 92\n",
      "\n",
      "Number of output sequences: 170\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 95\n",
      "\n",
      "Input vocab: {'O': 0, '36fS': 1, '36f': 2, '36f_42pS': 3, '38fS': 4, '38f': 5, '38f_42pS': 6, '42fS_36fS': 7, '42fS_38fS': 8, '38f_42fS': 9, '38f_42f': 10, '36f_42pS_36fS': 11, '38f_42p': 12, '36fS_42fS': 13, '36f_42f': 14, '42pS_36fS': 15, '42p_36f': 16, '38fS_42fS': 17, '42pS': 18, '42p': 19, '36f_42fS': 20, '42f_38f': 21, '42f_36fS': 22, '36f_42p': 23, '42p_38fS': 24, '42p_38f_42fS': 25, '36fS_42pS': 26, '42f_36f': 27, '42f_36f_42pS': 28, '42f_36f_42p': 29, '38f_42pS_36fS': 30, '36f_42fS_38fS': 31, '42fS': 32, '42f': 33, '38f_36fS': 34, '38f_36f_42pS': 35, '36f_36fS_42pS': 36, '42p_36fS': 37, '42f_36f_42fS': 38, '38pS_42fS': 39, '38p_42f': 40, '38f_42f_36fS': 41, '38f_42f_36f_42pS': 42, 'C': 43, '42pS_38fS': 44, '36pS': 45, '36p': 46, '38fS_42pS': 47, '42pS_36pS': 48, '36p_42pS_38fS': 49, '38pS': 50, '38p': 51, '42pS_38pS': 52, '36f_42f_36fS': 53, '42f_42pS': 54, '36f_42p_38fS': 55, '36f_38fS': 56, '36f_38f_42fS': 57, '36f_42fS_36fS': 58, '36f_42f_36fS_42pS': 59, '42f_42p': 60, '42p_38pS': 61, '42p_38p_42fS': 62, '42p_36f_42fS': 63, 'R': 64, '42p_36f_42f': 65, '42p_38f_42f': 66, '42f_38fS': 67, '42f_38f_42fS': 68, '42f_38f_42f': 69, '42p_36f_42fS_38fS': 70, '42p_36f_42f_38f': 71, '42p_36f_38fS': 72, '42p_36f_38f_42fS': 73, '42p_36f_38f_42f': 74, '42f_36f_42fS_38fS': 75, '42f_36f_42f_38f': 76, '36f_42p_38f_42fS': 77, '42f_36f_42f': 78, '36f_42f_38fS': 79, '36f_42f_38f_42fS': 80, '36f_38f': 81, '36f_42f_38f': 82, '42fS_38pS': 83, '42f_38p': 84, '36f_42fS_38pS': 85, '36f_42f_38p': 86, '36f_38f_42f': 87, '38f_42f_38pS': 88, '38f_42f_38p': 89, '38f_42f_38p_42pS': 90, '42p_38p': 91}\n",
      "Output vocab: {'O': 0, '52fS': 1, '52f': 2, '55fS': 3, '55f': 4, '45fS': 5, '45f': 6, '48fS': 7, '48f': 8, '47fS': 9, '47f': 10, '43fS': 11, '43f': 12, '57fS': 13, '57f': 14, '50fS': 15, '50f': 16, '49fS': 17, '49f': 18, '40fS': 19, '40f': 20, '44fS': 21, '44f': 22, '52pS': 23, '52p': 24, '48pS': 25, '48p': 26, '43pS': 27, '43p': 28, '47pS': 29, '47p': 30, '67pS': 31, '67p': 32, '50pS': 33, '50p': 34, '40pS': 35, '40p': 36, '55pS': 37, '55p': 38, '59pS': 39, '59p': 40, '62pS': 41, '62p': 42, '60pS': 43, '60p': 44, '57pS': 45, '57p': 46, '53pS': 47, '53p': 48, '64pS': 49, '64p': 50, '45pS': 51, '45p': 52, '56fS': 53, '56f': 54, '59fS': 55, '59f': 56, '51fS': 57, '51f': 58, '58fS': 59, '58f': 60, '83fS': 61, '83f': 62, '64fS': 63, '63fS': 64, '63f': 65, '64f': 66, '66fS': 67, '66f': 68, '61fS': 69, '61f': 70, '62fS': 71, '62f': 72, '54fS': 73, '54f': 74, '56pS': 75, '56p': 76, '49pS': 77, '49p': 78, '51pS': 79, '51p': 80, '58pS': 81, '58p': 82, '66pS': 83, '66p': 84, '63pS': 85, '63p': 86, '81fS': 87, '81f': 88, '80fS': 89, '80f': 90, '81pS': 91, '81p': 92, '80pS': 93, '80p': 94}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as input_#.mid and output_#.mid in the corresponding folders\n",
    "'''\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "\n",
    "input_filenames = glob.glob(os.path.join(DATASET_PATH, 'input/*.MID'))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = glob.glob(os.path.join(DATASET_PATH, 'output/*.MID'))\n",
    "print('Number of output files:', len(output_filenames), '\\n')\n",
    "\n",
    "INPUT_TOK = PrettyMidiTokenizer()\n",
    "OUTPUT_TOK = PrettyMidiTokenizer()\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    if 'RELAX' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['relax']\n",
    "    elif 'EXCITED' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['concentrate']\n",
    "    else:\n",
    "        raise Exception('Emotion not found in file name. Please add the emotion to the file name.')\n",
    "\n",
    "    in_seq, in_df = INPUT_TOK.midi_to_tokens(in_file, update_vocab=True, update_sequences=True, emotion_token = emotion_token)\n",
    "    out_seq, out_df = OUTPUT_TOK.midi_to_tokens(out_file, update_vocab=True, update_sequences=True)\n",
    "\n",
    "    print(f'Input sequence length: {len(in_seq)}')\n",
    "    print(f'Emotion token: {emotion_token}\\n')\n",
    "\n",
    "print(f'\\nNumber of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "print(f'\\nNumber of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "print('\\nInput vocab:', INPUT_TOK.VOCAB.word2idx)\n",
    "print('Output vocab:', OUTPUT_TOK.VOCAB.word2idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 102\n",
      "Evaluation set size: 34\n",
      "Test set size: 34\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "# Split the dataset into training, evaluation and test sets\n",
    "train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_sampler = RandomSampler(train_set)          \n",
    "train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "eval_sampler = RandomSampler(eval_set)\n",
    "eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = RandomSampler(test_set)\n",
    "test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train set size: {len(train_set)}')\n",
    "print(f'Evaluation set size: {len(eval_set)}')\n",
    "print(f'Test set size: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created: TCN(\n",
      "  (encoder): Embedding(187, 20)\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(20, 384, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(20, 384, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(20, 384, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): TemporalBlock(\n",
      "        (conv1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (5): TemporalBlock(\n",
      "        (conv1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (6): TemporalBlock(\n",
      "        (conv1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (7): TemporalBlock(\n",
      "        (conv1): Conv1d(384, 20, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(128,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(128,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(384, 20, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(128,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(128,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(384, 20, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=20, out_features=95, bias=True)\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111\n",
    "OUTPUT_SIZE = len(OUTPUT_TOK.VOCAB) \n",
    "\n",
    "\n",
    "'''\n",
    "IMPORTANT:\n",
    "to cover all the sequence of tokens k * d must be >= hidden units (see the paper)\n",
    "k = kernel_size\n",
    "d = dilation = 2 ^ (n_levels - 1) \n",
    "'''\n",
    "if FEEDBACK:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) + OUTPUT_SIZE\n",
    "    LEVELS = 8\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH * 2 # 192 * 2 = 384\n",
    "else:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) \n",
    "    LEVELS = 7\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH # 192 \n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] real value vectors (see model.py)\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE] # [192, 192, 192, 192, 192, 192, 20]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# balance the loss function by assigning a weight to each token related to its frequency\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_SIZE], dtype=torch.float)\n",
    "OUTPUT_TOK.VOCAB.compute_weights()\n",
    "for i, weigth in enumerate(OUTPUT_TOK.VOCAB.weights):\n",
    "    LOSS_WEIGTHS[i] = 1 - weigth\n",
    "    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = INPUT_SIZE,\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f'\\nModel created: {model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if FEEDBACK:\n",
    "        prev_output = torch.zeros([BATCH_SIZE, INPUT_TOK.SEQ_LENGTH], dtype=torch.long, device=device)\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data\n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long)), dim = 1) \n",
    "\n",
    "        if FEEDBACK:\n",
    "            input = torch.cat((data_masked, prev_output[:batch_size, :]), dim = 1)\n",
    "        else:\n",
    "            input = data_masked\n",
    "           \n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(input)\n",
    "        prev_output = torch.argmax(output, 2)# batch, seq_len (hidden units), vocab_size\n",
    "        prev_output = prev_output[:, :INPUT_TOK.SEQ_LENGTH] \n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "    \n",
    "        if FEEDBACK:\n",
    "            final_target = torch.cat((targets.contiguous().view(-1),targets.contiguous().view(-1)))\n",
    "        else:\n",
    "            final_target = targets.contiguous().view(-1)\n",
    "            \n",
    "        final_output = output.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        if mode == 'train':\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # clip the gradients to avoid exploding gradients\n",
    "            if GRADIENT_CLIP > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1/500 | lr 2.00000 | ms/epoch 24148.30327 | train_loss  3.51 | eval_loss  3.59\n",
      "| epoch   2/500 | lr 2.00000 | ms/epoch 29405.52521 | train_loss  3.50 | eval_loss  3.56\n",
      "| epoch   3/500 | lr 2.00000 | ms/epoch 26099.39575 | train_loss  3.50 | eval_loss  3.56\n",
      "| epoch   4/500 | lr 2.00000 | ms/epoch 25321.39730 | train_loss  3.49 | eval_loss  3.56\n",
      "| epoch   5/500 | lr 2.00000 | ms/epoch 26021.42453 | train_loss  3.49 | eval_loss  3.57\n",
      "| epoch   6/500 | lr 2.00000 | ms/epoch 27082.67784 | train_loss  3.50 | eval_loss  3.57\n",
      "| epoch   7/500 | lr 2.00000 | ms/epoch 24548.33412 | train_loss  3.48 | eval_loss  3.55\n",
      "| epoch   8/500 | lr 2.00000 | ms/epoch 24559.70454 | train_loss  3.47 | eval_loss  3.55\n",
      "| epoch   9/500 | lr 2.00000 | ms/epoch 24464.55622 | train_loss  3.48 | eval_loss  3.57\n",
      "| epoch  10/500 | lr 2.00000 | ms/epoch 29560.15301 | train_loss  3.48 | eval_loss  3.60\n",
      "| epoch  11/500 | lr 2.00000 | ms/epoch 26954.83828 | train_loss  3.49 | eval_loss  3.57\n",
      "| epoch  12/500 | lr 2.00000 | ms/epoch 26064.49175 | train_loss  3.47 | eval_loss  3.56\n",
      "| epoch  13/500 | lr 2.00000 | ms/epoch 28473.57202 | train_loss  3.47 | eval_loss  3.56\n",
      "| epoch  14/500 | lr 2.00000 | ms/epoch 26905.94602 | train_loss  3.48 | eval_loss  3.56\n",
      "| epoch  15/500 | lr 2.00000 | ms/epoch 30099.83134 | train_loss  3.47 | eval_loss  3.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [68], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     21\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 23\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mepoch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m epoch_step(eval_dataloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Save the model if the validation loss is the best we've seen so far.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [67], line 32\u001b[0m, in \u001b[0;36mepoch_step\u001b[1;34m(dataloader, mode)\u001b[0m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# make the prediction\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m prev_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;66;03m# batch, seq_len (hidden units), vocab_size\u001b[39;00m\n\u001b[0;32m     34\u001b[0m prev_output \u001b[38;5;241m=\u001b[39m prev_output[:, :INPUT_TOK\u001b[38;5;241m.\u001b[39mSEQ_LENGTH] \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Gianni\\Desktop\\MARCO\\UNI\\Magistrale\\TESI\\Code\\TCN\\word_cnn\\model.py:36\u001b[0m, in \u001b[0;36mTCN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"Input ought to have dimension (N, C_in, L_in), where L_in is the seq_len; here the input is (N, L, C)\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m---> 36\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     37\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(y)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Gianni\\Desktop\\MARCO\\UNI\\Magistrale\\TESI\\Code\\TCN\\..\\TCN\\tcn.py:63\u001b[0m, in \u001b[0;36mTemporalConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Gianni\\Desktop\\MARCO\\UNI\\Magistrale\\TESI\\Code\\TCN\\..\\TCN\\tcn.py:43\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 43\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_eval_loss = 1e8\n",
    "best_train_loss = 1e8\n",
    "best_model_epoch = 0\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "early_stop = False\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "\n",
    "    RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results', time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    if not os.path.exists(RESULTS_PATH):\n",
    "        os.makedirs(RESULTS_PATH)\n",
    "        \n",
    "    MODEL_PATH = os.path.join(RESULTS_PATH, 'model_state_dict.pth')\n",
    "\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "        \n",
    "        eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if eval_loss < best_eval_loss:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_eval_loss = eval_loss\n",
    "            best_model_epoch = epoch \n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "\n",
    "        # # Anneal the learning rate if the validation loss plateaus\n",
    "        # if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n",
    "        #     lr = lr / 2.\n",
    "        #     if lr < 0.1:\n",
    "        #         lr = 2\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = lr\n",
    "\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch > 15:\n",
    "            if min(eval_losses[-15:]) > best_eval_loss:\n",
    "                break\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n",
    "\n",
    "    print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d}' \\\n",
    "            .format(best_eval_loss, best_model_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TEST LOSS: 0.08584819696843624\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_loss = epoch_step(test_dataloader, 'eval')\n",
    "print(f'\\n\\nTEST LOSS: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # plot the losses over the epochs \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(eval_losses, label='eval')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # save the vocabularies\n",
    "    INPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'input_vocab.txt'))\n",
    "    OUTPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'output_vocab.txt'))\n",
    "\n",
    "     # save the model hyperparameters in a file txt\n",
    "    with open(os.path.join(RESULTS_PATH, 'model_hyperparameters.txt'), 'w') as f:\n",
    "\n",
    "        f.write(f'----------OPTIMIZATION PARAMETERS----------\\n')\n",
    "        f.write(f'DATE: {time.strftime(\"%Y%m%d-%H%M%S\")}\\n')\n",
    "        f.write(f'DATASET_PATH: {DATASET_PATH}\\n')\n",
    "        f.write(f'FEEDBACK: {FEEDBACK}\\n')\n",
    "        f.write(f'SEED: {SEED}\\n')\n",
    "        f.write(f'INPUT_SIZE: {INPUT_SIZE}\\n')\n",
    "        f.write(f'EMBEDDING_SIZE: {EMBEDDING_SIZE}\\n')\n",
    "        f.write(f'LEVELS: {LEVELS}\\n')\n",
    "        f.write(f'HIDDEN_UNITS: {HIDDEN_UNITS}\\n')\n",
    "        f.write(f'NUM_CHANNELS: {NUM_CHANNELS}\\n')\n",
    "        f.write(f'OUTPUT_SIZE: {OUTPUT_SIZE}\\n')\n",
    "        f.write(f'LOSS_WEIGTHS: {LOSS_WEIGTHS}\\n')\n",
    "        f.write(f'LEARNING_RATE: {LEARNING_RATE}\\n')\n",
    "        f.write(f'BATCH_SIZE: {BATCH_SIZE}\\n')\n",
    "        f.write(f'EPOCHS: {EPOCHS}\\n')\n",
    "        f.write(f'GRADIENT_CLIP: {GRADIENT_CLIP}\\n')\n",
    "        f.write(f'------------------------------------------\\n')\n",
    "        f.write(f'----------RESULTS----------\\n')\n",
    "        f.write(f'TRAIN_LOSSES: {train_losses}\\n')\n",
    "        f.write(f'BEST_EVAL_LOSS: {best_train_loss}\\n')\n",
    "        f.write(f'TEST_LOSS: {test_loss}\\n')\n",
    "        f.write(f'BEST_MODEL_EPOCH: {best_model_epoch}\\n')\n",
    "        f.write(f'------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 40 41 41 41 41 4 0 0 0 0 0 0 3 4 4 4 4 50 8 8 8 8 8 88 74 74 74 74 74 0\n",
      " 0 0 0 0 3 4 4 4 4 4 20 7 7 7 7 7 50 40 41 41 41 41 4 0 0 0 0 0 3 4 4 4 4\n",
      " 4 50 8 8 8 8 8 88 74 74 74 74 74 4 4 0 0 0 0 0 3 4 4 4 4 4 0 0 0 0 0 0 1\n",
      " 2 2 2 2 2 2 0 0 0 0 0 3 4 4 4 4 4 17 8 8 8 8 8 88 74 74 74 74 74 74 0 0 0\n",
      " 0 3 4 4 4 4 4 20 6 7 7 7 7 1 1 2 2 2 2 124 125 37 37 37 37 0 1 2 2 2 2 2\n",
      " 2 0 0 0 0 0 88 74 74 74 74 74 74 0 0 0 0 0 3 4 4 4 4 4 0 0 0 0 0 0]\n",
      "MIDI file saved at results\\20240628-183038\\predicted_rock_relax.mid\n",
      "['43fS', '55fS', '55f', '55f', '55f', '55f', '50f', '50f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '57fS', '52f', '55fS', '57f', '57f', '57f', '55f', '55f', '64f', '64f', '64f', '64f', '62f', 'O', '57f', '57f', '57f', '57f', '57f', '50fS', '50f', '50f', '50f', '50f', '50f', '50f', '50f', '50f', '50f', 'O', 'O', '48fS', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', 'O', '55fS', '55f', '55f', '55f', '55f', '55f', '62fS', '62f', '62f', '62f', '62f', '62f', 'O', '55fS', '55f', '55f', '55f', 'O', 'O', '48fS', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', '48f', 'O', '47fS', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', '47f', 'O', '54fS', '54f', '54f', '54f', '54f', 'O', '60fS', '60f', '60f', '60f', '60f', '54fS', '54f', '54f', '54f', '54f', '54f', '47fS', '47f', '47f', '47f', '47f', '47f', '47f', '47f', 'O', 'O', 'O', 'O', 'O', '52fS', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', 'O', 'O', '59fS', '59f', '59f', '59f', '59f', 'O', '67fS', '67f', '67f', '67f', '67f', '59fS', '59f', '59f', '59f', '59f', 'O', '52fS', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', '52f', 'O', 'O']\n",
      "[[0, 1, 0], [55, 5, 127], [0, 2, 0], [52, 8, 127], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 3, 0], [0, 2, 0], [64, 4, 127], [0, 1, 0], [0, 1, 0], [57, 5, 127], [50, 10, 127], [0, 2, 0], [48, 17, 127], [0, 1, 0], [55, 6, 127], [62, 6, 127], [0, 1, 0], [55, 4, 127], [0, 2, 0], [48, 10, 127], [0, 1, 0], [47, 17, 127], [0, 1, 0], [54, 5, 127], [0, 1, 0], [60, 5, 127], [54, 6, 127], [47, 8, 127], [0, 5, 0], [52, 17, 127], [0, 2, 0], [59, 5, 127], [0, 1, 0], [67, 5, 127], [59, 5, 127], [0, 1, 0], [52, 11, 127]]\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# select a genere to be predicted\n",
    "generes = ['blues', 'rock_excited', 'rock_relax']\n",
    "\n",
    "for genere in generes:\n",
    "    # get a sample to be predicted\n",
    "    sample_path = os.path.join(DATASET_PATH, f'test/drum_{genere}.mid')\n",
    "    sample = INPUT_TOK.midi_to_tokens(sample_path, update_vocab=False) [0]\n",
    "    print(sample[0])\n",
    "    sample = torch.LongTensor(sample).to(device)\n",
    "\n",
    "    # Get the last sequence from the batch and unsqueeze it to add a batch dimension.\n",
    "    sample = sample[-1].unsqueeze(0)\n",
    "\n",
    "    # Mask the last bar of the input data.\n",
    "    sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "    # Make the prediction.\n",
    "    prediction = model(sample)\n",
    "    prediction = prediction.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "    # Get the predicted tokens.\n",
    "    predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "    # Get the predicted sequence.\n",
    "    predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "    # Convert the predicted sequence to MIDI.\n",
    "    out_file_path = os.path.join(RESULTS_PATH, f'predicted_{genere}.mid')\n",
    "    pitch_ticks_list =  OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path) # midi is a pretty midi object\n",
    "\n",
    "# check \n",
    "predicted_sequence_string = []\n",
    "for id in predicted_sequence:\n",
    "    predicted_sequence_string.append(OUTPUT_TOK.VOCAB.idx2word[id])\n",
    "print(predicted_sequence_string)\n",
    "print(pitch_ticks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
