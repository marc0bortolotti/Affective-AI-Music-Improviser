{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from MIDI.PRETTY_MIDI.pretty_midi_tokenization import PrettyMidiTokenizer, SILENCE_TOKEN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "MODEL_PATH = os.path.join(DIRECTORY_PATH, 'model/generated_model.pth')\n",
    "RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results')\n",
    "\n",
    "\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 4\n",
    "BATCH_SIZE = 2\n",
    "TRAIN_MODEL = True\n",
    "SPLIT_DATASET = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 1\n",
      "Number of output files: 1\n",
      "\n",
      "\n",
      "1: drum_excited.MID -> bass_example.MID\n",
      "\n",
      "Number of input bars: 24\n",
      "Number of input sequences: 20\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 13\n",
      "\n",
      "Number of output bars: 33\n",
      "Number of output sequences: 29\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 30\n",
      "\n",
      "Number of sequences after truncation: 20, 20\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as input_#.mid and output_#.mid in the corresponding folders\n",
    "'''\n",
    "\n",
    "input_filenames = glob.glob(os.path.join(DATASET_PATH, 'input/*.MID'))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = glob.glob(os.path.join(DATASET_PATH, 'output/*.MID'))\n",
    "print('Number of output files:', len(output_filenames))\n",
    "\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'\\n\\n{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    INPUT_TOK = PrettyMidiTokenizer(in_file)\n",
    "    print(f'\\nNumber of input bars: {INPUT_TOK.num_bars}')\n",
    "    print(f'Number of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "    print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "    print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "\n",
    "    OUTPUT_TOK = PrettyMidiTokenizer(out_file)\n",
    "    print(f'\\nNumber of output bars: {OUTPUT_TOK.num_bars}')\n",
    "    print(f'Number of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "    print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "    print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "    # force the sequences to be the same length\n",
    "    min_length = min(len(INPUT_TOK.sequences), len(OUTPUT_TOK.sequences))\n",
    "    INPUT_TOK.sequences = INPUT_TOK.sequences[:min_length]\n",
    "    OUTPUT_TOK.sequences = OUTPUT_TOK.sequences[:min_length]\n",
    "    print(f'\\nNumber of sequences after truncation: {len(INPUT_TOK.sequences)}, {len(OUTPUT_TOK.sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 12\n",
      "Evaluation set size: 4\n",
      "Test set size: 4\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "if SPLIT_DATASET:\n",
    "    # Split the dataset into training, evaluation and test sets\n",
    "    train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "    # Create the dataloaders\n",
    "    train_sampler = RandomSampler(train_set)          \n",
    "    train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    eval_sampler = RandomSampler(eval_set)\n",
    "    eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    test_sampler = RandomSampler(test_set)\n",
    "    test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "else:\n",
    "    train_set = dataset\n",
    "    train_sampler = RandomSampler(train_set)          \n",
    "    train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    eval_dataloader = []\n",
    "    test_dataloader = []\n",
    "\n",
    "print(f'Train set size: {len(train_set)}')\n",
    "print(f'Evaluation set size: {len(eval_set)}')\n",
    "print(f'Test set size: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111 \n",
    "OUTPUT_VOCAB_SIZE = len(OUTPUT_TOK.VOCAB)\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] vectors (see model.py)\n",
    "LEVELS = 7\n",
    "HIDDEN_UNITS = 192\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# balance the loss function by assigning a weight to each token related to its frequency\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_VOCAB_SIZE], dtype=torch.float)\n",
    "for i, weigth in enumerate(OUTPUT_TOK.tokens_weights):\n",
    "    LOSS_WEIGTHS[i] = 1 - weigth\n",
    "    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_VOCAB_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data \n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(data_masked)\n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "        final_target = targets.contiguous().view(-1)\n",
    "        final_output = output.contiguous().view(-1, OUTPUT_VOCAB_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        if GRADIENT_CLIP > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1/500 | lr 4.00000 | ms/epoch 601.93229 | train_loss  3.01 | eval_loss  2.94\n",
      "| epoch   2/500 | lr 4.00000 | ms/epoch 583.51421 | train_loss  2.79 | eval_loss  2.88\n",
      "| epoch   3/500 | lr 4.00000 | ms/epoch 617.82098 | train_loss  2.73 | eval_loss  2.89\n",
      "| epoch   4/500 | lr 4.00000 | ms/epoch 619.94481 | train_loss  2.76 | eval_loss  2.86\n",
      "| epoch   5/500 | lr 4.00000 | ms/epoch 664.42108 | train_loss  2.71 | eval_loss  2.85\n",
      "| epoch   6/500 | lr 4.00000 | ms/epoch 613.29603 | train_loss  2.70 | eval_loss  2.86\n",
      "| epoch   7/500 | lr 4.00000 | ms/epoch 617.92064 | train_loss  2.72 | eval_loss  2.85\n",
      "| epoch   8/500 | lr 4.00000 | ms/epoch 584.23638 | train_loss  2.70 | eval_loss  2.86\n",
      "| epoch   9/500 | lr 4.00000 | ms/epoch 619.50088 | train_loss  2.71 | eval_loss  2.85\n",
      "| epoch  10/500 | lr 4.00000 | ms/epoch 630.04565 | train_loss  2.71 | eval_loss  2.85\n",
      "| epoch  11/500 | lr 4.00000 | ms/epoch 610.11386 | train_loss  2.70 | eval_loss  2.85\n",
      "| epoch  12/500 | lr 4.00000 | ms/epoch 633.34441 | train_loss  2.70 | eval_loss  2.84\n",
      "| epoch  13/500 | lr 4.00000 | ms/epoch 746.24681 | train_loss  2.70 | eval_loss  2.83\n",
      "| epoch  14/500 | lr 4.00000 | ms/epoch 680.20749 | train_loss  2.69 | eval_loss  2.81\n",
      "| epoch  15/500 | lr 4.00000 | ms/epoch 715.36970 | train_loss  2.70 | eval_loss  2.76\n",
      "| epoch  16/500 | lr 4.00000 | ms/epoch 940.95445 | train_loss  2.67 | eval_loss  2.73\n",
      "| epoch  17/500 | lr 2.00000 | ms/epoch 866.49394 | train_loss  2.67 | eval_loss  3.11\n",
      "| epoch  18/500 | lr 2.00000 | ms/epoch 902.83632 | train_loss  2.39 | eval_loss  2.57\n",
      "| epoch  19/500 | lr 2.00000 | ms/epoch 844.12360 | train_loss  2.36 | eval_loss  2.52\n",
      "| epoch  20/500 | lr 2.00000 | ms/epoch 866.60624 | train_loss  2.28 | eval_loss  2.51\n",
      "| epoch  21/500 | lr 2.00000 | ms/epoch 836.33494 | train_loss  2.30 | eval_loss  2.53\n",
      "| epoch  22/500 | lr 2.00000 | ms/epoch 860.13889 | train_loss  2.28 | eval_loss  2.52\n",
      "| epoch  23/500 | lr 2.00000 | ms/epoch 910.99405 | train_loss  2.21 | eval_loss  2.45\n",
      "| epoch  24/500 | lr 2.00000 | ms/epoch 968.53733 | train_loss  2.19 | eval_loss  2.42\n",
      "| epoch  25/500 | lr 2.00000 | ms/epoch 1521.95072 | train_loss  2.17 | eval_loss  2.40\n",
      "| epoch  26/500 | lr 2.00000 | ms/epoch 964.04505 | train_loss  2.20 | eval_loss  2.36\n",
      "| epoch  27/500 | lr 2.00000 | ms/epoch 983.98662 | train_loss  2.14 | eval_loss  2.32\n",
      "| epoch  28/500 | lr 2.00000 | ms/epoch 965.80935 | train_loss  2.16 | eval_loss  2.28\n",
      "| epoch  29/500 | lr 2.00000 | ms/epoch 1284.46341 | train_loss  2.10 | eval_loss  2.29\n",
      "| epoch  30/500 | lr 2.00000 | ms/epoch 1199.72968 | train_loss  2.05 | eval_loss  2.26\n",
      "| epoch  31/500 | lr 2.00000 | ms/epoch 1205.00636 | train_loss  2.01 | eval_loss  2.20\n",
      "| epoch  32/500 | lr 2.00000 | ms/epoch 1225.24595 | train_loss  2.05 | eval_loss  2.20\n",
      "| epoch  33/500 | lr 2.00000 | ms/epoch 1504.61483 | train_loss  1.96 | eval_loss  2.16\n",
      "| epoch  34/500 | lr 2.00000 | ms/epoch 1283.31590 | train_loss  1.95 | eval_loss  2.17\n",
      "| epoch  35/500 | lr 2.00000 | ms/epoch 1261.62124 | train_loss  1.95 | eval_loss  2.18\n",
      "| epoch  36/500 | lr 2.00000 | ms/epoch 1685.50110 | train_loss  1.96 | eval_loss  2.12\n",
      "| epoch  37/500 | lr 2.00000 | ms/epoch 1194.49115 | train_loss  1.92 | eval_loss  2.16\n",
      "| epoch  38/500 | lr 2.00000 | ms/epoch 1344.57397 | train_loss  1.91 | eval_loss  2.10\n",
      "| epoch  39/500 | lr 2.00000 | ms/epoch 1622.23077 | train_loss  1.91 | eval_loss  2.10\n",
      "| epoch  40/500 | lr 2.00000 | ms/epoch 1718.50801 | train_loss  1.91 | eval_loss  2.10\n",
      "| epoch  41/500 | lr 2.00000 | ms/epoch 2432.48367 | train_loss  1.88 | eval_loss  2.12\n",
      "| epoch  42/500 | lr 2.00000 | ms/epoch 1861.34768 | train_loss  1.91 | eval_loss  2.02\n",
      "| epoch  43/500 | lr 2.00000 | ms/epoch 2619.92693 | train_loss  1.85 | eval_loss  2.09\n",
      "| epoch  44/500 | lr 2.00000 | ms/epoch 1487.02788 | train_loss  1.84 | eval_loss  2.02\n",
      "| epoch  45/500 | lr 2.00000 | ms/epoch 1832.13234 | train_loss  1.89 | eval_loss  2.04\n",
      "| epoch  46/500 | lr 2.00000 | ms/epoch 1406.23236 | train_loss  1.84 | eval_loss  2.06\n",
      "| epoch  47/500 | lr 2.00000 | ms/epoch 1852.56934 | train_loss  1.86 | eval_loss  2.03\n",
      "| epoch  48/500 | lr 2.00000 | ms/epoch 2156.40473 | train_loss  1.85 | eval_loss  2.01\n",
      "| epoch  49/500 | lr 2.00000 | ms/epoch 1821.70439 | train_loss  1.86 | eval_loss  2.00\n",
      "| epoch  50/500 | lr 2.00000 | ms/epoch 1538.15532 | train_loss  1.83 | eval_loss  1.99\n",
      "| epoch  51/500 | lr 2.00000 | ms/epoch 1139.64367 | train_loss  1.81 | eval_loss  1.98\n",
      "| epoch  52/500 | lr 2.00000 | ms/epoch 1467.48734 | train_loss  1.78 | eval_loss  1.94\n",
      "| epoch  53/500 | lr 2.00000 | ms/epoch 1199.58687 | train_loss  1.81 | eval_loss  1.91\n",
      "| epoch  54/500 | lr 2.00000 | ms/epoch 1269.05680 | train_loss  1.78 | eval_loss  1.94\n",
      "| epoch  55/500 | lr 2.00000 | ms/epoch 1380.74064 | train_loss  1.78 | eval_loss  1.94\n",
      "| epoch  56/500 | lr 2.00000 | ms/epoch 1929.28219 | train_loss  1.77 | eval_loss  1.92\n",
      "| epoch  57/500 | lr 2.00000 | ms/epoch 1368.50619 | train_loss  1.75 | eval_loss  1.90\n",
      "| epoch  58/500 | lr 2.00000 | ms/epoch 2001.32227 | train_loss  1.72 | eval_loss  1.87\n",
      "| epoch  59/500 | lr 2.00000 | ms/epoch 1622.23053 | train_loss  1.77 | eval_loss  1.84\n",
      "| epoch  60/500 | lr 2.00000 | ms/epoch 2158.89835 | train_loss  1.69 | eval_loss  1.84\n",
      "| epoch  61/500 | lr 2.00000 | ms/epoch 1351.51291 | train_loss  1.70 | eval_loss  1.81\n",
      "| epoch  62/500 | lr 2.00000 | ms/epoch 1518.54610 | train_loss  1.73 | eval_loss  1.83\n",
      "| epoch  63/500 | lr 2.00000 | ms/epoch 1743.34216 | train_loss  1.72 | eval_loss  1.76\n",
      "| epoch  64/500 | lr 2.00000 | ms/epoch 1476.02773 | train_loss  1.68 | eval_loss  1.75\n",
      "| epoch  65/500 | lr 2.00000 | ms/epoch 1698.03977 | train_loss  1.67 | eval_loss  1.75\n",
      "| epoch  66/500 | lr 2.00000 | ms/epoch 1621.18483 | train_loss  1.65 | eval_loss  1.71\n",
      "| epoch  67/500 | lr 2.00000 | ms/epoch 1895.14089 | train_loss  1.63 | eval_loss  1.74\n",
      "| epoch  68/500 | lr 2.00000 | ms/epoch 1983.97112 | train_loss  1.65 | eval_loss  1.68\n",
      "| epoch  69/500 | lr 2.00000 | ms/epoch 2213.98759 | train_loss  1.62 | eval_loss  1.67\n",
      "| epoch  70/500 | lr 2.00000 | ms/epoch 3147.31812 | train_loss  1.62 | eval_loss  1.63\n",
      "| epoch  71/500 | lr 2.00000 | ms/epoch 1682.15799 | train_loss  1.60 | eval_loss  1.65\n",
      "| epoch  72/500 | lr 2.00000 | ms/epoch 2181.26917 | train_loss  1.61 | eval_loss  1.60\n",
      "| epoch  73/500 | lr 2.00000 | ms/epoch 2712.61191 | train_loss  1.53 | eval_loss  1.65\n",
      "| epoch  74/500 | lr 2.00000 | ms/epoch 3020.33520 | train_loss  1.56 | eval_loss  1.57\n",
      "| epoch  75/500 | lr 2.00000 | ms/epoch 2973.88220 | train_loss  1.61 | eval_loss  1.58\n",
      "| epoch  76/500 | lr 2.00000 | ms/epoch 2257.27129 | train_loss  1.55 | eval_loss  1.54\n",
      "| epoch  77/500 | lr 2.00000 | ms/epoch 1951.10989 | train_loss  1.59 | eval_loss  1.59\n",
      "| epoch  78/500 | lr 2.00000 | ms/epoch 1970.98970 | train_loss  1.54 | eval_loss  1.54\n",
      "| epoch  79/500 | lr 2.00000 | ms/epoch 2478.56116 | train_loss  1.53 | eval_loss  1.46\n",
      "| epoch  80/500 | lr 2.00000 | ms/epoch 1974.91074 | train_loss  1.51 | eval_loss  1.41\n",
      "| epoch  81/500 | lr 2.00000 | ms/epoch 2305.48930 | train_loss  1.48 | eval_loss  1.49\n",
      "| epoch  82/500 | lr 2.00000 | ms/epoch 1901.39222 | train_loss  1.49 | eval_loss  1.42\n",
      "| epoch  83/500 | lr 2.00000 | ms/epoch 2055.83239 | train_loss  1.49 | eval_loss  1.44\n",
      "| epoch  84/500 | lr 2.00000 | ms/epoch 2526.34549 | train_loss  1.51 | eval_loss  1.39\n",
      "| epoch  85/500 | lr 2.00000 | ms/epoch 2669.74258 | train_loss  1.48 | eval_loss  1.39\n",
      "| epoch  86/500 | lr 2.00000 | ms/epoch 3986.33647 | train_loss  1.45 | eval_loss  1.37\n",
      "| epoch  87/500 | lr 2.00000 | ms/epoch 2809.21006 | train_loss  1.44 | eval_loss  1.35\n",
      "| epoch  88/500 | lr 2.00000 | ms/epoch 2804.18229 | train_loss  1.40 | eval_loss  1.31\n",
      "| epoch  89/500 | lr 2.00000 | ms/epoch 2844.11311 | train_loss  1.43 | eval_loss  1.30\n",
      "| epoch  90/500 | lr 2.00000 | ms/epoch 2776.39818 | train_loss  1.42 | eval_loss  1.28\n",
      "| epoch  91/500 | lr 2.00000 | ms/epoch 2667.78708 | train_loss  1.39 | eval_loss  1.22\n",
      "| epoch  92/500 | lr 2.00000 | ms/epoch 2405.27129 | train_loss  1.36 | eval_loss  1.27\n",
      "| epoch  93/500 | lr 2.00000 | ms/epoch 2230.69072 | train_loss  1.40 | eval_loss  1.22\n",
      "| epoch  94/500 | lr 2.00000 | ms/epoch 1719.94901 | train_loss  1.40 | eval_loss  1.21\n",
      "| epoch  95/500 | lr 2.00000 | ms/epoch 2697.53504 | train_loss  1.36 | eval_loss  1.21\n",
      "| epoch  96/500 | lr 2.00000 | ms/epoch 2701.50661 | train_loss  1.37 | eval_loss  1.17\n",
      "| epoch  97/500 | lr 2.00000 | ms/epoch 2556.81825 | train_loss  1.35 | eval_loss  1.13\n",
      "| epoch  98/500 | lr 2.00000 | ms/epoch 1665.95697 | train_loss  1.33 | eval_loss  1.10\n",
      "| epoch  99/500 | lr 2.00000 | ms/epoch 1931.33307 | train_loss  1.38 | eval_loss  1.10\n",
      "| epoch 100/500 | lr 2.00000 | ms/epoch 2822.27206 | train_loss  1.31 | eval_loss  1.03\n",
      "| epoch 101/500 | lr 2.00000 | ms/epoch 2332.55267 | train_loss  1.33 | eval_loss  1.04\n",
      "| epoch 102/500 | lr 2.00000 | ms/epoch 1923.26260 | train_loss  1.28 | eval_loss  1.04\n",
      "| epoch 103/500 | lr 2.00000 | ms/epoch 1374.86196 | train_loss  1.32 | eval_loss  1.04\n",
      "| epoch 104/500 | lr 2.00000 | ms/epoch 1830.21569 | train_loss  1.29 | eval_loss  0.99\n",
      "| epoch 105/500 | lr 2.00000 | ms/epoch 1412.86778 | train_loss  1.28 | eval_loss  1.01\n",
      "| epoch 106/500 | lr 2.00000 | ms/epoch 1385.19454 | train_loss  1.27 | eval_loss  1.02\n",
      "| epoch 107/500 | lr 2.00000 | ms/epoch 1366.66250 | train_loss  1.26 | eval_loss  0.96\n",
      "| epoch 108/500 | lr 2.00000 | ms/epoch 1428.35927 | train_loss  1.22 | eval_loss  0.93\n",
      "| epoch 109/500 | lr 2.00000 | ms/epoch 1513.05866 | train_loss  1.25 | eval_loss  0.84\n",
      "| epoch 110/500 | lr 2.00000 | ms/epoch 1431.03933 | train_loss  1.23 | eval_loss  0.91\n",
      "| epoch 111/500 | lr 2.00000 | ms/epoch 1063.41243 | train_loss  1.25 | eval_loss  0.88\n",
      "| epoch 112/500 | lr 2.00000 | ms/epoch 1026.63517 | train_loss  1.18 | eval_loss  0.85\n",
      "| epoch 113/500 | lr 2.00000 | ms/epoch 983.43372 | train_loss  1.18 | eval_loss  0.90\n",
      "| epoch 114/500 | lr 2.00000 | ms/epoch 967.65161 | train_loss  1.21 | eval_loss  0.86\n",
      "| epoch 115/500 | lr 2.00000 | ms/epoch 968.83702 | train_loss  1.17 | eval_loss  0.82\n",
      "| epoch 116/500 | lr 2.00000 | ms/epoch 912.30941 | train_loss  1.20 | eval_loss  0.79\n",
      "| epoch 117/500 | lr 2.00000 | ms/epoch 936.29265 | train_loss  1.21 | eval_loss  0.73\n",
      "| epoch 118/500 | lr 2.00000 | ms/epoch 1021.21258 | train_loss  1.18 | eval_loss  0.70\n",
      "| epoch 119/500 | lr 2.00000 | ms/epoch 939.79740 | train_loss  1.19 | eval_loss  0.75\n",
      "| epoch 120/500 | lr 2.00000 | ms/epoch 930.36222 | train_loss  1.11 | eval_loss  0.74\n",
      "| epoch 121/500 | lr 2.00000 | ms/epoch 1038.14697 | train_loss  1.13 | eval_loss  0.58\n",
      "| epoch 122/500 | lr 2.00000 | ms/epoch 981.50253 | train_loss  1.24 | eval_loss  0.64\n",
      "| epoch 123/500 | lr 2.00000 | ms/epoch 1024.11962 | train_loss  1.15 | eval_loss  0.63\n",
      "| epoch 124/500 | lr 2.00000 | ms/epoch 897.85051 | train_loss  1.14 | eval_loss  0.67\n",
      "| epoch 125/500 | lr 2.00000 | ms/epoch 837.57615 | train_loss  1.10 | eval_loss  0.61\n",
      "| epoch 126/500 | lr 2.00000 | ms/epoch 937.96349 | train_loss  1.10 | eval_loss  0.54\n",
      "| epoch 127/500 | lr 2.00000 | ms/epoch 1247.59769 | train_loss  1.20 | eval_loss  0.60\n",
      "| epoch 128/500 | lr 2.00000 | ms/epoch 1160.82215 | train_loss  1.09 | eval_loss  0.60\n",
      "| epoch 129/500 | lr 2.00000 | ms/epoch 1033.18691 | train_loss  1.07 | eval_loss  0.59\n",
      "| epoch 130/500 | lr 2.00000 | ms/epoch 988.79433 | train_loss  1.08 | eval_loss  0.57\n",
      "| epoch 131/500 | lr 1.00000 | ms/epoch 956.69818 | train_loss  1.06 | eval_loss  0.64\n",
      "| epoch 132/500 | lr 1.00000 | ms/epoch 1073.52090 | train_loss  1.02 | eval_loss  0.44\n",
      "| epoch 133/500 | lr 1.00000 | ms/epoch 1018.73708 | train_loss  0.98 | eval_loss  0.37\n",
      "| epoch 134/500 | lr 1.00000 | ms/epoch 972.05472 | train_loss  0.91 | eval_loss  0.34\n",
      "| epoch 135/500 | lr 1.00000 | ms/epoch 1034.42812 | train_loss  0.96 | eval_loss  0.33\n",
      "| epoch 136/500 | lr 1.00000 | ms/epoch 1096.73333 | train_loss  0.91 | eval_loss  0.29\n",
      "| epoch 137/500 | lr 1.00000 | ms/epoch 1038.69820 | train_loss  0.99 | eval_loss  0.29\n",
      "| epoch 138/500 | lr 1.00000 | ms/epoch 1097.93639 | train_loss  0.92 | eval_loss  0.27\n",
      "| epoch 139/500 | lr 1.00000 | ms/epoch 1087.86225 | train_loss  0.99 | eval_loss  0.26\n",
      "| epoch 140/500 | lr 1.00000 | ms/epoch 1122.55168 | train_loss  0.95 | eval_loss  0.27\n",
      "| epoch 141/500 | lr 1.00000 | ms/epoch 1185.68206 | train_loss  0.90 | eval_loss  0.27\n",
      "| epoch 142/500 | lr 1.00000 | ms/epoch 1282.29642 | train_loss  0.95 | eval_loss  0.23\n",
      "| epoch 143/500 | lr 1.00000 | ms/epoch 1716.79664 | train_loss  0.93 | eval_loss  0.21\n",
      "| epoch 144/500 | lr 1.00000 | ms/epoch 2861.28807 | train_loss  0.94 | eval_loss  0.21\n",
      "| epoch 145/500 | lr 1.00000 | ms/epoch 2087.82387 | train_loss  0.91 | eval_loss  0.20\n",
      "| epoch 146/500 | lr 1.00000 | ms/epoch 2020.09773 | train_loss  0.90 | eval_loss  0.19\n",
      "| epoch 147/500 | lr 1.00000 | ms/epoch 1662.54377 | train_loss  0.91 | eval_loss  0.20\n",
      "| epoch 148/500 | lr 1.00000 | ms/epoch 1560.96220 | train_loss  0.93 | eval_loss  0.19\n",
      "| epoch 149/500 | lr 1.00000 | ms/epoch 1631.43206 | train_loss  0.90 | eval_loss  0.19\n",
      "| epoch 150/500 | lr 1.00000 | ms/epoch 1849.27773 | train_loss  0.91 | eval_loss  0.18\n",
      "| epoch 151/500 | lr 1.00000 | ms/epoch 1639.11867 | train_loss  0.90 | eval_loss  0.15\n",
      "| epoch 152/500 | lr 1.00000 | ms/epoch 1602.75245 | train_loss  0.89 | eval_loss  0.17\n",
      "| epoch 153/500 | lr 1.00000 | ms/epoch 1635.26177 | train_loss  0.87 | eval_loss  0.15\n",
      "| epoch 154/500 | lr 1.00000 | ms/epoch 1476.15910 | train_loss  0.91 | eval_loss  0.16\n",
      "| epoch 155/500 | lr 1.00000 | ms/epoch 1533.68950 | train_loss  0.89 | eval_loss  0.15\n",
      "| epoch 156/500 | lr 1.00000 | ms/epoch 1429.74925 | train_loss  0.92 | eval_loss  0.15\n",
      "| epoch 157/500 | lr 1.00000 | ms/epoch 1733.48641 | train_loss  0.86 | eval_loss  0.14\n",
      "| epoch 158/500 | lr 1.00000 | ms/epoch 1968.63747 | train_loss  0.83 | eval_loss  0.13\n",
      "| epoch 159/500 | lr 1.00000 | ms/epoch 1831.32505 | train_loss  0.85 | eval_loss  0.12\n",
      "| epoch 160/500 | lr 1.00000 | ms/epoch 1734.63178 | train_loss  0.83 | eval_loss  0.13\n",
      "| epoch 161/500 | lr 1.00000 | ms/epoch 1306.94985 | train_loss  0.91 | eval_loss  0.14\n",
      "| epoch 162/500 | lr 1.00000 | ms/epoch 1945.52946 | train_loss  0.85 | eval_loss  0.12\n",
      "| epoch 163/500 | lr 1.00000 | ms/epoch 2025.69771 | train_loss  0.94 | eval_loss  0.12\n",
      "| epoch 164/500 | lr 1.00000 | ms/epoch 1540.02619 | train_loss  0.88 | eval_loss  0.12\n",
      "| epoch 165/500 | lr 1.00000 | ms/epoch 1822.64519 | train_loss  0.83 | eval_loss  0.10\n",
      "| epoch 166/500 | lr 1.00000 | ms/epoch 1672.65606 | train_loss  0.86 | eval_loss  0.10\n",
      "| epoch 167/500 | lr 1.00000 | ms/epoch 1579.72956 | train_loss  0.82 | eval_loss  0.10\n",
      "| epoch 168/500 | lr 1.00000 | ms/epoch 1478.63245 | train_loss  0.88 | eval_loss  0.10\n",
      "| epoch 169/500 | lr 1.00000 | ms/epoch 1878.30830 | train_loss  0.85 | eval_loss  0.10\n",
      "| epoch 170/500 | lr 1.00000 | ms/epoch 2353.76167 | train_loss  0.85 | eval_loss  0.10\n",
      "| epoch 171/500 | lr 1.00000 | ms/epoch 1866.21642 | train_loss  0.81 | eval_loss  0.09\n",
      "| epoch 172/500 | lr 1.00000 | ms/epoch 2062.38270 | train_loss  0.85 | eval_loss  0.08\n",
      "| epoch 173/500 | lr 1.00000 | ms/epoch 1452.14987 | train_loss  0.82 | eval_loss  0.09\n",
      "| epoch 174/500 | lr 1.00000 | ms/epoch 1471.64178 | train_loss  0.78 | eval_loss  0.08\n",
      "| epoch 175/500 | lr 1.00000 | ms/epoch 1802.51145 | train_loss  0.78 | eval_loss  0.08\n",
      "| epoch 176/500 | lr 1.00000 | ms/epoch 1946.76995 | train_loss  0.82 | eval_loss  0.08\n",
      "| epoch 177/500 | lr 1.00000 | ms/epoch 1681.03981 | train_loss  0.79 | eval_loss  0.08\n",
      "| epoch 178/500 | lr 1.00000 | ms/epoch 1709.15723 | train_loss  0.82 | eval_loss  0.07\n",
      "| epoch 179/500 | lr 1.00000 | ms/epoch 2997.55549 | train_loss  0.74 | eval_loss  0.07\n",
      "| epoch 180/500 | lr 1.00000 | ms/epoch 2485.12387 | train_loss  0.77 | eval_loss  0.07\n",
      "| epoch 181/500 | lr 1.00000 | ms/epoch 1982.11884 | train_loss  0.76 | eval_loss  0.08\n",
      "| epoch 182/500 | lr 1.00000 | ms/epoch 1574.60403 | train_loss  0.79 | eval_loss  0.07\n",
      "| epoch 183/500 | lr 1.00000 | ms/epoch 1414.37602 | train_loss  0.76 | eval_loss  0.06\n",
      "| epoch 184/500 | lr 1.00000 | ms/epoch 1111.73534 | train_loss  0.78 | eval_loss  0.06\n",
      "| epoch 185/500 | lr 1.00000 | ms/epoch 1207.16166 | train_loss  0.79 | eval_loss  0.07\n",
      "| epoch 186/500 | lr 1.00000 | ms/epoch 1296.37313 | train_loss  0.73 | eval_loss  0.06\n",
      "| epoch 187/500 | lr 1.00000 | ms/epoch 1150.66814 | train_loss  0.72 | eval_loss  0.05\n",
      "| epoch 188/500 | lr 1.00000 | ms/epoch 1021.08073 | train_loss  0.74 | eval_loss  0.06\n",
      "| epoch 189/500 | lr 1.00000 | ms/epoch 960.26373 | train_loss  0.76 | eval_loss  0.06\n",
      "| epoch 190/500 | lr 1.00000 | ms/epoch 966.44807 | train_loss  0.75 | eval_loss  0.06\n",
      "| epoch 191/500 | lr 1.00000 | ms/epoch 995.74852 | train_loss  0.75 | eval_loss  0.05\n",
      "| epoch 192/500 | lr 1.00000 | ms/epoch 1042.72366 | train_loss  0.72 | eval_loss  0.05\n",
      "| epoch 193/500 | lr 0.50000 | ms/epoch 989.12168 | train_loss  0.70 | eval_loss  0.06\n",
      "| epoch 194/500 | lr 0.50000 | ms/epoch 1048.43211 | train_loss  0.70 | eval_loss  0.04\n",
      "| epoch 195/500 | lr 0.50000 | ms/epoch 1021.60597 | train_loss  0.64 | eval_loss  0.05\n",
      "| epoch 196/500 | lr 0.50000 | ms/epoch 1045.24684 | train_loss  0.69 | eval_loss  0.04\n",
      "| epoch 197/500 | lr 0.50000 | ms/epoch 1012.99477 | train_loss  0.68 | eval_loss  0.04\n",
      "| epoch 198/500 | lr 0.50000 | ms/epoch 971.64965 | train_loss  0.65 | eval_loss  0.04\n",
      "| epoch 199/500 | lr 0.50000 | ms/epoch 927.36912 | train_loss  0.62 | eval_loss  0.04\n",
      "| epoch 200/500 | lr 0.50000 | ms/epoch 957.14355 | train_loss  0.63 | eval_loss  0.04\n",
      "| epoch 201/500 | lr 0.50000 | ms/epoch 924.00002 | train_loss  0.63 | eval_loss  0.04\n",
      "| epoch 202/500 | lr 0.50000 | ms/epoch 929.49033 | train_loss  0.63 | eval_loss  0.04\n",
      "| epoch 203/500 | lr 0.50000 | ms/epoch 963.12809 | train_loss  0.64 | eval_loss  0.04\n",
      "| epoch 204/500 | lr 0.50000 | ms/epoch 971.66371 | train_loss  0.66 | eval_loss  0.04\n",
      "| epoch 205/500 | lr 0.50000 | ms/epoch 1067.38877 | train_loss  0.67 | eval_loss  0.04\n",
      "| epoch 206/500 | lr 0.50000 | ms/epoch 1200.15192 | train_loss  0.62 | eval_loss  0.04\n",
      "| epoch 207/500 | lr 0.50000 | ms/epoch 1436.79500 | train_loss  0.65 | eval_loss  0.04\n",
      "| epoch 208/500 | lr 0.50000 | ms/epoch 1199.48745 | train_loss  0.63 | eval_loss  0.04\n",
      "| epoch 209/500 | lr 0.50000 | ms/epoch 1176.44238 | train_loss  0.63 | eval_loss  0.04\n",
      "| epoch 210/500 | lr 0.50000 | ms/epoch 1157.69911 | train_loss  0.65 | eval_loss  0.04\n",
      "| epoch 211/500 | lr 0.50000 | ms/epoch 1346.55142 | train_loss  0.57 | eval_loss  0.04\n",
      "| epoch 212/500 | lr 0.50000 | ms/epoch 1470.89648 | train_loss  0.60 | eval_loss  0.04\n",
      "| epoch 213/500 | lr 0.50000 | ms/epoch 1641.77513 | train_loss  0.61 | eval_loss  0.04\n",
      "| epoch 214/500 | lr 0.50000 | ms/epoch 3192.53850 | train_loss  0.60 | eval_loss  0.04\n",
      "| epoch 215/500 | lr 0.50000 | ms/epoch 4075.45781 | train_loss  0.57 | eval_loss  0.04\n",
      "| epoch 216/500 | lr 0.50000 | ms/epoch 1855.13949 | train_loss  0.62 | eval_loss  0.04\n",
      "| epoch 217/500 | lr 0.50000 | ms/epoch 1798.23256 | train_loss  0.61 | eval_loss  0.04\n",
      "| epoch 218/500 | lr 0.50000 | ms/epoch 1475.14510 | train_loss  0.60 | eval_loss  0.04\n",
      "| epoch 219/500 | lr 0.50000 | ms/epoch 1699.73660 | train_loss  0.62 | eval_loss  0.04\n",
      "| epoch 220/500 | lr 0.50000 | ms/epoch 1449.83745 | train_loss  0.62 | eval_loss  0.04\n",
      "| epoch 221/500 | lr 0.50000 | ms/epoch 1556.45895 | train_loss  0.61 | eval_loss  0.04\n",
      "| epoch 222/500 | lr 0.50000 | ms/epoch 1540.51971 | train_loss  0.62 | eval_loss  0.04\n",
      "| epoch 223/500 | lr 0.50000 | ms/epoch 1501.60956 | train_loss  0.53 | eval_loss  0.04\n",
      "| epoch 224/500 | lr 0.50000 | ms/epoch 1509.13978 | train_loss  0.54 | eval_loss  0.04\n",
      "| epoch 225/500 | lr 0.50000 | ms/epoch 1486.50861 | train_loss  0.58 | eval_loss  0.04\n",
      "| epoch 226/500 | lr 0.50000 | ms/epoch 1689.47673 | train_loss  0.57 | eval_loss  0.04\n",
      "| epoch 227/500 | lr 0.50000 | ms/epoch 1391.39533 | train_loss  0.59 | eval_loss  0.04\n",
      "| epoch 228/500 | lr 0.50000 | ms/epoch 1496.89627 | train_loss  0.59 | eval_loss  0.04\n",
      "| epoch 229/500 | lr 0.50000 | ms/epoch 1466.57372 | train_loss  0.55 | eval_loss  0.03\n",
      "| epoch 230/500 | lr 0.50000 | ms/epoch 1321.32530 | train_loss  0.60 | eval_loss  0.03\n",
      "| epoch 231/500 | lr 0.50000 | ms/epoch 1545.46213 | train_loss  0.60 | eval_loss  0.04\n",
      "| epoch 232/500 | lr 0.50000 | ms/epoch 1389.84919 | train_loss  0.55 | eval_loss  0.04\n",
      "| epoch 233/500 | lr 0.50000 | ms/epoch 1370.11123 | train_loss  0.58 | eval_loss  0.04\n",
      "| epoch 234/500 | lr 0.50000 | ms/epoch 1514.03975 | train_loss  0.58 | eval_loss  0.04\n",
      "| epoch 235/500 | lr 0.50000 | ms/epoch 1365.56697 | train_loss  0.57 | eval_loss  0.04\n",
      "| epoch 236/500 | lr 0.50000 | ms/epoch 1460.04820 | train_loss  0.62 | eval_loss  0.03\n",
      "| epoch 237/500 | lr 0.50000 | ms/epoch 1517.98987 | train_loss  0.56 | eval_loss  0.03\n",
      "| epoch 238/500 | lr 0.50000 | ms/epoch 1545.42661 | train_loss  0.54 | eval_loss  0.03\n",
      "| epoch 239/500 | lr 0.50000 | ms/epoch 1420.10212 | train_loss  0.54 | eval_loss  0.03\n",
      "| epoch 240/500 | lr 0.50000 | ms/epoch 1353.55377 | train_loss  0.55 | eval_loss  0.03\n",
      "| epoch 241/500 | lr 0.50000 | ms/epoch 1520.17736 | train_loss  0.59 | eval_loss  0.03\n",
      "| epoch 242/500 | lr 0.50000 | ms/epoch 1519.25206 | train_loss  0.52 | eval_loss  0.03\n",
      "| epoch 243/500 | lr 0.50000 | ms/epoch 1379.69518 | train_loss  0.53 | eval_loss  0.03\n",
      "| epoch 244/500 | lr 0.50000 | ms/epoch 1503.85475 | train_loss  0.57 | eval_loss  0.03\n",
      "| epoch 245/500 | lr 0.50000 | ms/epoch 1750.93460 | train_loss  0.55 | eval_loss  0.03\n",
      "| epoch 246/500 | lr 0.50000 | ms/epoch 1515.62524 | train_loss  0.53 | eval_loss  0.03\n",
      "| epoch 247/500 | lr 0.25000 | ms/epoch 1436.41186 | train_loss  0.52 | eval_loss  0.03\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File model/generated_model.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Save the model if the validation loss is the best we've seen so far.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_loss \u001b[38;5;241m<\u001b[39m best_loss:\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m eval_loss\n\u001b[0;32m     24\u001b[0m     best_model_epoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:309\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:287\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_writer_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File model/generated_model.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "best_loss = 1e8\n",
    "best_model_epoch = 0\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "\n",
    "\n",
    "        if SPLIT_DATASET:\n",
    "            eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if eval_loss < best_loss:\n",
    "                torch.save(model, MODEL_PATH)\n",
    "                best_loss = eval_loss\n",
    "                best_model_epoch = epoch + 1\n",
    "\n",
    "            # Anneal the learning rate if the validation loss plateaus\n",
    "            if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n",
    "                lr = lr / 2.\n",
    "                if lr < 0.1:\n",
    "                    lr = 2\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "\n",
    "\n",
    "        else:\n",
    "            eval_loss = 0\n",
    "\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if train_loss < best_loss:\n",
    "                torch.save(model, MODEL_PATH)\n",
    "                best_loss = train_loss\n",
    "                best_model_epoch = epoch + 1\n",
    "\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n",
    "        \n",
    "    \n",
    "    if TRAIN_MODEL:\n",
    "        print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d}' \\\n",
    "              .format(best_loss, best_model_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses over the epochs \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(eval_losses, label='eval')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# save images\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['43S', '43S', '48', '43', '43', '43', '43', 'O', 'O', '43S', 'O', 'O', '47S', '47', '47', '47', '47', '47', '47', 'O', '47S', '47', '47', '47', '50S', '50', '50', '50', '50', '50', 'O', '50S', '50', '50', '50', 'O', '53S', '53', '53', 'O', '48S', '50', 'O', 'O', '49S', '49', '49', 'O', '50S', '50', '50', '50', '50', '50', 'O', '50S', '50', '50', '50', 'O', '54S', '54', '54', '54', '54', 'O', 'O', '54S', '54', '54', 'O', '57S', '57', '57', '57', '60S', '60', '60', 'O', 'O', '59S', 'O', 'O', '57S', '57', '57', '54S', '54', '54', '54', 'O', '50S', '50', 'O', 'O', '48S', '48S', '48', '48', '48', '48', '48', 'O', 'O', 'O', '48S', '48', 'O', '52S', '52', '52', '52', '52', '52', 'O', '52S', '52', '52', '52', 'O', '55S', '55', '55', 'O', '58S', '58', 'O', 'O', '57S', 'O', 'O', 'O', '55S', 'O', 'O', '52S', '52', '52', 'O', '50S', '50', '50', 'O', '43S', '43S', '43', '43', '43', '43', '43', 'O', '43S', '43', '43', 'O', 'O', '47S', '47', '47', '47', '47', '47', 'O', '47S', '47', '47', 'O', 'O', '50S', '50', '50', '50', '50', 'O', 'O', '50S', '50', '53', 'O', 'O', '53S', '53', 'O', '52S', '52', '52', 'O', '48S', '48', '48', '48', 'O']\n",
      "[[43, 1], [43, 1], [48, 1], [43, 4], [0, 2], [43, 1], [0, 2], [47, 7], [0, 1], [47, 4], [50, 6], [0, 1], [50, 4], [0, 1], [53, 3], [0, 1], [48, 1], [50, 1], [0, 2], [49, 3], [0, 1], [50, 6], [0, 1], [50, 4], [0, 1], [54, 5], [0, 2], [54, 3], [0, 1], [57, 4], [60, 3], [0, 2], [59, 1], [0, 2], [57, 3], [54, 4], [0, 1], [50, 2], [0, 2], [48, 1], [48, 6], [0, 3], [48, 2], [0, 1], [52, 6], [0, 1], [52, 4], [0, 1], [55, 3], [0, 1], [58, 2], [0, 2], [57, 1], [0, 3], [55, 1], [0, 2], [52, 3], [0, 1], [50, 3], [0, 1], [43, 1], [43, 6], [0, 1], [43, 3], [0, 2], [47, 6], [0, 1], [47, 3], [0, 2], [50, 5], [0, 2], [50, 2], [53, 1], [0, 2], [53, 2], [0, 1], [52, 3], [0, 1], [48, 4], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(MODEL_PATH, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# get a sample from the test set\n",
    "if SPLIT_DATASET:\n",
    "    samples, targets = next(iter(test_dataloader))\n",
    "else:\n",
    "    samples, targets = next(iter(train_dataloader))\n",
    "\n",
    "# Get the last sequence from the batch and unsqueeze it to add a batch dimension.\n",
    "sample = samples[-1].unsqueeze(0)\n",
    "\n",
    "# Mask the last bar of the input data.\n",
    "sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "# Make the prediction.\n",
    "prediction = model(sample)\n",
    "prediction = prediction.contiguous().view(-1, OUTPUT_VOCAB_SIZE)\n",
    "\n",
    "# Get the predicted tokens.\n",
    "predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "# Get the predicted sequence.\n",
    "predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "# Convert the predicted sequence to MIDI.\n",
    "out_file_path = os.path.join(RESULTS_PATH, 'predicted.mid')\n",
    "midi, pitch_ticks_list =  OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path) # midi is a pretty midi object\n",
    "\n",
    "# check \n",
    "predicted_sequence_string = []\n",
    "for id in predicted_sequence:\n",
    "    predicted_sequence_string.append(OUTPUT_TOK.VOCAB.idx2word[id])\n",
    "print(predicted_sequence_string)\n",
    "print(pitch_ticks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
