{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from MIDI.PRETTY_MIDI.pretty_midi_tokenization import PrettyMidiTokenizer, BCI_TOKENS\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 2 # 4\n",
    "BATCH_SIZE = 16 # 16\n",
    "TRAIN_MODEL = True\n",
    "FEEDBACK = False\n",
    "EMPHAZISE_EEG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 6\n",
      "Number of output files: 6 \n",
      "\n",
      "1: 0_Drum_HardRock_EXCITED.mid -> 0_Bass_HardRock_EXCITED.mid\n",
      "Input sequence length: 26\n",
      "Emotion token: C\n",
      "\n",
      "2: 1_Drum_HardRock_RELAX.mid -> 1_Bass_HardRock_RELAX.mid\n",
      "Input sequence length: 32\n",
      "Emotion token: R\n",
      "\n",
      "3: 2_Drum_Blues_EXCITED.mid -> 2_Bass_Blues_EXCITED.mid\n",
      "Input sequence length: 20\n",
      "Emotion token: C\n",
      "\n",
      "4: 3_Drum_Blues_RELAX.mid -> 3_Bass_Blues_RELAX.mid\n",
      "Input sequence length: 20\n",
      "Emotion token: R\n",
      "\n",
      "5: 4_Drum_PopRock_EXCITED.mid -> 4_Bass_PopRock_EXCITED.mid\n",
      "Input sequence length: 35\n",
      "Emotion token: C\n",
      "\n",
      "6: 5_Drum_PopRock_RELAX.mid -> 5_Bass_PopRock_RELAX.mid\n",
      "Input sequence length: 23\n",
      "Emotion token: R\n",
      "\n",
      "\n",
      "Number of input sequences: 171\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 52\n",
      "\n",
      "Number of output sequences: 171\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 47\n",
      "\n",
      "Input vocab: {'O': 0, '36S': 1, '36': 2, '36_42S': 3, '38S': 4, '38': 5, '38_42S': 6, '42S_36S': 7, '42S_38S': 8, '38_42': 9, '36_42S_36S': 10, '36S_42S': 11, '36_42': 12, '42_36': 13, '38S_42S': 14, '42S': 15, '42': 16, '42_38': 17, '42_36S': 18, '42_38S': 19, '42_38_42S': 20, '42_36_42S': 21, '42_36_42': 22, '38_42S_36S': 23, '36_42S_38S': 24, '38_36S': 25, '38_36_42S': 26, '36_36S_42S': 27, '38_42_36S': 28, '38_42_36_42S': 29, 'C': 30, '36_42_36S': 31, '42_42S': 32, '36_42_38S': 33, '36_38S': 34, '36_38_42S': 35, '36_42_36S_42S': 36, '42_42': 37, 'R': 38, '42_38_42': 39, '42_36_42S_38S': 40, '42_36_42_38': 41, '42_36_38S': 42, '42_36_38_42S': 43, '42_36_38_42': 44, '36_42_38_42S': 45, '36_38': 46, '36_42_38': 47, '36_38_42': 48, '38_42_38S': 49, '38_42_38': 50, '38_42_38_42S': 51}\n",
      "Output vocab: {'O': 0, '52S': 1, '52': 2, '55S': 3, '55': 4, '45S': 5, '45': 6, '48S': 7, '48': 8, '47S': 9, '47': 10, '43S': 11, '43': 12, '57S': 13, '57': 14, '50S': 15, '50': 16, '40S': 17, '40': 18, '67S': 19, '67': 20, '59S': 21, '59': 22, '62S': 23, '62': 24, '60S': 25, '60': 26, '53S': 27, '53': 28, '64S': 29, '64': 30, '56S': 31, '56': 32, '49S': 33, '49': 34, '51S': 35, '51': 36, '54S': 37, '54': 38, '58S': 39, '58': 40, '63S': 41, '63': 42, '66S': 43, '66': 44, '61S': 45, '61': 46}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as \n",
    "    input:  drum_genere_emotion.mid \n",
    "    output: bass_genere_emotion.mid \n",
    "in the corresponding folders\n",
    "'''\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "\n",
    "input_filenames = glob.glob(os.path.join(DATASET_PATH, 'input/*.MID'))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = glob.glob(os.path.join(DATASET_PATH, 'output/*.MID'))\n",
    "print('Number of output files:', len(output_filenames), '\\n')\n",
    "\n",
    "INPUT_TOK = PrettyMidiTokenizer()\n",
    "OUTPUT_TOK = PrettyMidiTokenizer()\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    if 'RELAX' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['relax']\n",
    "    elif 'EXCITED' in in_file_name:\n",
    "        emotion_token = BCI_TOKENS['concentrate']\n",
    "    else:\n",
    "        raise Exception('Emotion not found in file name. Please add the emotion to the file name.')\n",
    "\n",
    "    in_seq, in_df = INPUT_TOK.midi_to_tokens(in_file, update_vocab=True, update_sequences=True, emotion_token = emotion_token)\n",
    "    out_seq, out_df = OUTPUT_TOK.midi_to_tokens(out_file, update_vocab=True, update_sequences=True)\n",
    "\n",
    "    print(f'Input sequence length: {len(in_seq)}')\n",
    "    print(f'Emotion token: {emotion_token}\\n')\n",
    "\n",
    "print(f'\\nNumber of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "print(f'\\nNumber of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "print('\\nInput vocab:', INPUT_TOK.VOCAB.word2idx)\n",
    "print('Output vocab:', OUTPUT_TOK.VOCAB.word2idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of input sequences after data augmentation: 1197\n",
      "Number of output sequences after data augmentation: 1197\n"
     ]
    }
   ],
   "source": [
    "# Perform data augmentation\n",
    "input_shifts = [-3, -2, -1, 1, 2, 3]\n",
    "output_shifts = list(np.zeros(len(input_shifts)))\n",
    "\n",
    "INPUT_TOK.data_augmentation_shift(input_shifts)\n",
    "OUTPUT_TOK.data_augmentation_shift(output_shifts)\n",
    "\n",
    "print(f'\\nNumber of input sequences after data augmentation: {len(INPUT_TOK.sequences)}')\n",
    "print(f'Number of output sequences after data augmentation: {len(OUTPUT_TOK.sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 719\n",
      "Evaluation set size: 239\n",
      "Test set size: 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gianni\\AppData\\Local\\Temp\\ipykernel_24504\\3823352634.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "# Split the dataset into training, evaluation and test sets\n",
    "train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_sampler = RandomSampler(train_set)          \n",
    "train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "eval_sampler = RandomSampler(eval_set)\n",
    "eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = RandomSampler(test_set)\n",
    "test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'Train set size: {len(train_set)}')\n",
    "print(f'Evaluation set size: {len(eval_set)}')\n",
    "print(f'Test set size: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created: TCN(\n",
      "  (encoder): Embedding(52, 20, padding_idx=0)\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(20, 192, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (5): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (6): TemporalBlock(\n",
      "        (conv1): Conv1d(192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(192, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.45, inplace=False)\n",
      "          (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.45, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(192, 20, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=20, out_features=47, bias=True)\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111\n",
    "OUTPUT_SIZE = len(OUTPUT_TOK.VOCAB) \n",
    "\n",
    "\n",
    "'''\n",
    "IMPORTANT:\n",
    "to cover all the sequence of tokens k * d must be >= hidden units (see the paper)\n",
    "k = kernel_size\n",
    "d = dilation = 2 ^ (n_levels - 1) \n",
    "'''\n",
    "if FEEDBACK:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) + OUTPUT_SIZE\n",
    "    LEVELS = 8\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH * 2 # 192 * 2 = 384\n",
    "else:\n",
    "    INPUT_SIZE = len(INPUT_TOK.VOCAB) \n",
    "    LEVELS = 7\n",
    "    HIDDEN_UNITS = INPUT_TOK.SEQ_LENGTH # 192 \n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] real value vectors (see model.py)\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE] # [192, 192, 192, 192, 192, 192, 20]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# balance the loss function by assigning a weight to each token related to its frequency\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_SIZE], dtype=torch.float, device=device)\n",
    "OUTPUT_TOK.VOCAB.compute_weights()\n",
    "for i, weigth in enumerate(OUTPUT_TOK.VOCAB.weights):\n",
    "    LOSS_WEIGTHS[i] = 1 - weigth\n",
    "    # print(f'{OUTPUT_TOK.VOCAB.idx2word[i]}: {LOSS_WEIGTHS[i]}')\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = INPUT_SIZE,\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            emphasize_eeg = EMPHAZISE_EEG,\n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f'\\nModel created: {model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters():\n",
    "    # plot the losses over the epochs \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(eval_losses, label='eval')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'losses.png'))\n",
    "\n",
    "\n",
    "    # save the vocabularies\n",
    "    INPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'input_vocab.txt'))\n",
    "    OUTPUT_TOK.VOCAB.save(os.path.join(RESULTS_PATH, 'output_vocab.txt'))\n",
    "\n",
    "     # save the model hyperparameters in a file txt\n",
    "    with open(os.path.join(RESULTS_PATH, 'model_hyperparameters.txt'), 'w') as f:\n",
    "\n",
    "        f.write(f'----------OPTIMIZATION PARAMETERS----------\\n')\n",
    "        f.write(f'DATE: {time.strftime(\"%Y%m%d-%H%M%S\")}\\n')\n",
    "        f.write(f'DATASET_PATH: {DATASET_PATH}\\n')\n",
    "        f.write(f'FEEDBACK: {FEEDBACK}\\n')\n",
    "        f.write(f'SEED: {SEED}\\n')\n",
    "        f.write(f'INPUT_SIZE: {INPUT_SIZE}\\n')\n",
    "        f.write(f'EMBEDDING_SIZE: {EMBEDDING_SIZE}\\n')\n",
    "        f.write(f'LEVELS: {LEVELS}\\n')\n",
    "        f.write(f'HIDDEN_UNITS: {HIDDEN_UNITS}\\n')\n",
    "        f.write(f'NUM_CHANNELS: {NUM_CHANNELS}\\n')\n",
    "        f.write(f'OUTPUT_SIZE: {OUTPUT_SIZE}\\n')\n",
    "        f.write(f'LOSS_WEIGTHS: {LOSS_WEIGTHS}\\n')\n",
    "        f.write(f'LEARNING_RATE: {LEARNING_RATE}\\n')\n",
    "        f.write(f'BATCH_SIZE: {BATCH_SIZE}\\n')\n",
    "        f.write(f'EPOCHS: {EPOCHS}\\n')\n",
    "        f.write(f'GRADIENT_CLIP: {GRADIENT_CLIP}\\n')\n",
    "        f.write(f'------------------------------------------\\n')\n",
    "        f.write(f'----------RESULTS----------\\n')\n",
    "        f.write(f'BEST_TRAIN_LOSSES: {best_train_loss}\\n')\n",
    "        f.write(f'BEST_EVAL_LOSS: {best_eval_loss}\\n')\n",
    "        f.write(f'TEST_LOSS: {test_loss}\\n')\n",
    "        f.write(f'BEST_MODEL_EPOCH: {best_model_epoch}\\n')\n",
    "        f.write(f'------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if FEEDBACK:\n",
    "        prev_output = torch.zeros([BATCH_SIZE, INPUT_TOK.SEQ_LENGTH], dtype=torch.long, device=device)\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data\n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long, device=device)), dim = 1) \n",
    "\n",
    "        if FEEDBACK:\n",
    "            input = torch.cat((data_masked, prev_output[:batch_size, :]), dim = 1)\n",
    "        else:\n",
    "            input = data_masked\n",
    "           \n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(input)[:, :INPUT_TOK.SEQ_LENGTH] \n",
    "        prev_output = torch.argmax(output, 2)# batch, seq_len (hidden units), vocab_size\n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "    \n",
    "        final_target = targets.contiguous().view(-1)    \n",
    "        final_output = output.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        if mode == 'train':\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # clip the gradients to avoid exploding gradients\n",
    "            if GRADIENT_CLIP > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "        [-4.7813e-02,  1.2285e-02, -9.5078e-03,  2.5429e-03,  3.4518e-02,\n",
      "         -2.9169e-02,  8.7573e-03, -2.8767e-02, -7.6869e-03, -4.4830e-02,\n",
      "         -1.9061e-02, -1.5287e-02, -6.8928e-03, -6.3727e-03, -8.2832e-03,\n",
      "          1.3062e-02,  2.7370e-02, -1.2486e-02, -1.7859e-02, -1.1416e-02],\n",
      "        [-1.7069e-02,  4.7620e-03,  4.7714e-03,  1.1454e-02, -1.9045e-02,\n",
      "          2.5807e-02, -1.1608e-02,  6.1198e-03, -6.7520e-03, -2.4383e-02,\n",
      "         -9.1061e-03, -1.4523e-02, -1.1824e-02, -1.7942e-02, -3.6100e-02,\n",
      "         -3.3100e-03, -2.9734e-02,  4.9426e-05, -8.5654e-03, -2.4802e-02],\n",
      "        [ 1.8321e-02, -1.6269e-02, -4.7567e-03, -1.6906e-02, -1.4101e-02,\n",
      "          8.7865e-03, -1.6398e-02, -2.7112e-02,  2.2057e-03, -7.5846e-03,\n",
      "         -5.7820e-03, -5.7773e-03,  5.1939e-03,  1.0758e-02,  2.7410e-03,\n",
      "         -4.6107e-03, -1.2152e-02,  8.7527e-03,  5.0709e-04, -6.2765e-03],\n",
      "        [-2.9998e-03, -7.3373e-03,  4.3289e-03, -6.0991e-03,  8.5307e-03,\n",
      "         -5.7616e-03, -1.3353e-02, -8.8586e-03, -1.6076e-02, -4.6320e-03,\n",
      "         -1.5011e-02,  2.0595e-02, -5.6592e-04,  7.1550e-03,  1.7619e-03,\n",
      "          4.1970e-03,  2.8548e-03,  1.9364e-02,  5.7174e-04,  1.2170e-02]])\n",
      "tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "        [-5.1781e-02,  8.8877e-03, -1.4333e-02,  7.0870e-03,  4.6033e-02,\n",
      "         -3.5292e-02,  5.3446e-03, -3.7536e-02, -5.4965e-03, -5.2656e-02,\n",
      "         -2.5923e-02, -1.7958e-02, -1.0241e-02, -1.1330e-02, -6.2928e-03,\n",
      "          1.1487e-02,  3.1120e-02, -1.6443e-02, -2.7550e-02, -1.9400e-02],\n",
      "        [-3.5103e-02, -9.5015e-03, -3.5832e-03, -1.0234e-02, -3.8882e-02,\n",
      "          1.5395e-02, -2.6621e-02, -1.3055e-02, -2.6812e-02, -4.1735e-02,\n",
      "         -2.2545e-02, -2.6921e-02, -2.7107e-02, -3.1796e-02, -5.3186e-02,\n",
      "         -1.2526e-02, -5.0917e-02, -1.2957e-02, -2.2337e-02, -4.1439e-02],\n",
      "        [ 1.7488e-02, -1.6776e-02, -5.0119e-03, -1.8026e-02, -1.5159e-02,\n",
      "          8.9766e-03, -1.7655e-02, -2.8225e-02,  1.4423e-03, -7.9137e-03,\n",
      "         -6.2471e-03, -6.3262e-03,  3.7095e-03,  9.5502e-03,  1.7401e-03,\n",
      "         -4.8236e-03, -1.2675e-02,  8.0158e-03, -7.2949e-04, -7.9937e-03],\n",
      "        [-3.6800e-03, -7.8241e-03,  4.2065e-03, -6.5445e-03,  7.9902e-03,\n",
      "         -6.0578e-03, -1.4191e-02, -9.6728e-03, -1.6430e-02, -5.0629e-03,\n",
      "         -1.5406e-02,  2.0488e-02, -1.2402e-03,  6.3988e-03,  7.9289e-04,\n",
      "          3.4867e-03,  2.4186e-03,  1.8600e-02, -5.5645e-04,  1.1330e-02]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     21\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 23\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mepoch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[:\u001b[38;5;241m5\u001b[39m][:\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m     27\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m epoch_step(eval_dataloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [7], line 48\u001b[0m, in \u001b[0;36mepoch_step\u001b[1;34m(dataloader, mode)\u001b[0m\n\u001b[0;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(final_output, final_target)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# calculate the gradients\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# clip the gradients to avoid exploding gradients\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m GRADIENT_CLIP \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_eval_loss = 1e8\n",
    "best_train_loss = 1e8\n",
    "best_model_epoch = 0\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "early_stop = True\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "\n",
    "    RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results', time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    if not os.path.exists(RESULTS_PATH):\n",
    "        os.makedirs(RESULTS_PATH)\n",
    "        \n",
    "    MODEL_PATH = os.path.join(RESULTS_PATH, 'model_state_dict.pth')\n",
    "\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "        \n",
    "        eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if eval_loss < best_eval_loss:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_eval_loss = eval_loss\n",
    "            best_model_epoch = epoch \n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "\n",
    "        # # Anneal the learning rate if the validation loss plateaus\n",
    "        # if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n",
    "        #     lr = lr / 2.\n",
    "        #     if lr < 0.1:\n",
    "        #         lr = 2\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = lr\n",
    "\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop:\n",
    "            if epoch > 15:\n",
    "                if min(eval_losses[-15:]) > best_eval_loss:\n",
    "                    break\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n",
    "\n",
    "    print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d} \\n\\n' \\\n",
    "            .format(best_eval_loss, best_model_epoch))\n",
    "    \n",
    "    # test the model\n",
    "    test_loss = epoch_step(test_dataloader, 'eval')\n",
    "    print(f'\\n\\nTEST LOSS: {test_loss}')\n",
    "    save_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 2, 2, 0, 15, 16, 16, 16, 19, 20, 5, 5, 5, 5, 5, 0, 15, 16,\n",
      "       16, 16, 18, 21, 22, 2, 2, 2, 2, 0, 15, 16, 16, 16, 19, 20, 39, 5,\n",
      "       5, 5, 5, 0, 0, 15, 16, 16, 16, 7, 13, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0,\n",
      "       8, 17, 5, 5, 5, 5, 5, 0, 15, 16, 16, 16, 7, 13, 2, 2, 2, 2, 2, 0,\n",
      "       15, 16, 16, 16, 8, 17, 5, 5, 5, 5, 5, 15, 16, 16, 16, 16, 7, 13, 1,\n",
      "       2, 2, 2, 0, 15, 16, 16, 16, 16, 8, 17, 5, 5, 5, 5, 5, 0, 15, 16,\n",
      "       16, 18, 21, 22, 2, 2, 2, 2, 0, 15, 16, 16, 16, 8, 17, 17, 5, 5, 5,\n",
      "       5, 0, 0, 15, 16, 16, 16, 7, 13, 1, 2, 2, 2, 2, 15, 16, 16, 16, 19,\n",
      "       20, 39, 5, 5, 5, 5, 0, 15, 16, 16, 16, 18, 21, 22, 2, 2, 2, 0, 0,\n",
      "       7, 13, 13, 13, 40, 41, 41, 5, 5, 5, 5, 0, 15, 16, 16, 16, 16, 7,\n",
      "       13], dtype=object)]\n",
      "[2, 2, 6, 0, 14, 2, 6, 0, 0, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 0, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0, 26, 26, 26, 26, 26, 26, 0, 0, 0, 0, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 0, 0, 15, 16, 16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 37, 38, 38, 38, 38, 38, 38, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 35, 36, 36, 36, 36, 36, 36, 36, 36, 0, 0, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0, 25, 26, 26, 26, 26, 26, 26, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 0, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0, 26, 26, 26, 26, 26, 26, 0, 0, 0, 0, 1, 2, 2, 2]\n",
      "MIDI file saved at results/model_short\\predicted_blues.mid\n",
      "[array([11, 12, 12, 16, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16, 0, 0, 0, 0,\n",
      "       0, 0, 14, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16,\n",
      "       0, 0, 0, 0, 0, 7, 13, 13, 13, 7, 13, 13, 0, 0, 0, 0, 0, 15, 16, 16,\n",
      "       16, 16, 16, 16, 0, 0, 0, 0, 0, 14, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0,\n",
      "       0, 15, 16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 1, 3, 12, 12, 11, 12, 12,\n",
      "       0, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 4, 6, 9,\n",
      "       9, 9, 9, 9, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16, 16, 0, 0, 0, 0,\n",
      "       0, 7, 13, 13, 13, 7, 13, 13, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16,\n",
      "       16, 0, 0, 0, 0, 0, 0, 14, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 15, 16,\n",
      "       16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 11, 12, 12], dtype=object)]\n",
      "[9, 10, 10, 12, 10, 10, 10, 12, 10, 10, 10, 12, 10, 12, 12, 12, 10, 12, 12, 16, 10, 0, 0, 0, 10, 0, 10, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 16, 16, 12, 16, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 16, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 12, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 20, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 11, 12, 12, 12]\n",
      "MIDI file saved at results/model_short\\predicted_rock_relax.mid\n",
      "[array([1, 2, 2, 2, 0, 0, 0, 0, 0, 8, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 15,\n",
      "       1, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 4, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0,\n",
      "       0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 14, 9, 9, 9, 9, 9, 9, 0,\n",
      "       0, 0, 0, 0, 7, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 4, 6, 9, 9, 9, 9, 9,\n",
      "       16, 0, 0, 0, 0, 7, 2, 2, 1, 2, 2, 2, 0, 0, 0, 0, 0, 4, 6, 9, 9, 9,\n",
      "       9, 9, 0, 0, 0, 0, 0, 7, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 8, 5, 5,\n",
      "       5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 7, 13, 7, 13, 13, 13, 13, 0, 0, 0, 0,\n",
      "       0, 14, 9, 9, 9, 9, 9, 18, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 1, 2, 2,\n",
      "       2, 2, 2, 24, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 7, 2], dtype=object)]\n",
      "[2, 2, 6, 0, 14, 14, 14, 0, 0, 18, 2, 2, 2, 0, 13, 1, 2, 2, 14, 0, 0, 2, 10, 10, 10, 0, 0, 10, 10, 2, 10, 10, 0, 9, 10, 10, 2, 0, 0, 9, 10, 20, 20, 20, 19, 20, 20, 20, 20, 0, 19, 19, 20, 20, 20, 20, 0, 9, 20, 8, 8, 0, 0, 7, 7, 8, 8, 8, 0, 7, 8, 8, 8, 8, 0, 7, 7, 8, 8, 0, 0, 0, 7, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 12, 0, 0, 0, 11, 12, 12, 12, 0, 0, 11, 12, 12, 12, 0, 11, 12, 12, 12, 12, 0, 0, 11, 12, 12, 12, 0, 0, 11, 12, 12, 12, 0, 0, 11, 12, 12, 12, 0, 0, 11, 12, 12, 12, 12, 0, 9, 10, 10, 10, 2, 2, 10, 2, 2, 2, 0, 0, 1, 2, 2, 2, 0, 0, 0, 9, 10, 0, 0, 0, 7, 8, 8, 8, 0, 0, 0, 10, 8, 10, 10, 0, 0, 2, 10, 10, 0, 0, 0, 0, 0, 0, 0, 7, 7, 2, 2]\n",
      "MIDI file saved at results/model_short\\predicted_rock_excited.mid\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "RESULTS_PATH = 'results/model_short'\n",
    "MODEL_PATH = f'{RESULTS_PATH}/model_state_dict.pth'\n",
    "\n",
    "INPUT_TOK.load_vocab(f'{RESULTS_PATH}/input_vocab.txt')\n",
    "OUTPUT_TOK.load_vocab(f'{RESULTS_PATH}/output_vocab.txt')\n",
    "\n",
    "model = TCN(input_size = len(INPUT_TOK.VOCAB),\n",
    "            embedding_size = EMBEDDING_SIZE, \n",
    "            output_size = len(OUTPUT_TOK.VOCAB), \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# select a genere to be predicted\n",
    "generes = ['blues', 'rock_relax', 'rock_excited']\n",
    "\n",
    "for genere in generes:\n",
    "    # get a sample to be predicted\n",
    "    sample_path = os.path.join(DATASET_PATH, f'test/drum_{genere}_2.mid')\n",
    "    sample = INPUT_TOK.midi_to_tokens(sample_path, update_vocab=False) [0]\n",
    "\n",
    "    print(sample)\n",
    "    sample = torch.LongTensor(sample)\n",
    "\n",
    "    # Get the last sequence from the batch and unsqueeze it to add a batch dimension.\n",
    "    sample = sample[-1].unsqueeze(0)\n",
    "\n",
    "    # Mask the last bar of the input data.\n",
    "    sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "    # Make the prediction.\n",
    "    prediction = model(sample.to(device))\n",
    "    prediction = prediction.contiguous().view(-1, OUTPUT_SIZE)\n",
    "\n",
    "    # Get the predicted tokens.\n",
    "    predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "    # Get the predicted sequence.\n",
    "    predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "    # Convert the predicted sequence to MIDI.\n",
    "    out_file_path = os.path.join(RESULTS_PATH, f'predicted_{genere}.mid')\n",
    "    pitch_ticks_velocity =  OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path = out_file_path, ticks_filter = 3, instrument_name = 'Electric Bass (finger)') \n",
    "\n",
    "\n",
    "# # check \n",
    "# predicted_sequence_string = []\n",
    "# for id in predicted_sequence:\n",
    "#     predicted_sequence_string.append(OUTPUT_TOK.VOCAB.idx2word[id])\n",
    "# print(predicted_sequence_string)\n",
    "# print(pitch_ticks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
