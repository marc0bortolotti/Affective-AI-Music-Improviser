{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gianni\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from word_cnn.model import TCN\n",
    "from MIDI.PRETTY_MIDI.pretty_midi_tokenization import PrettyMidiTokenizer, SILENCE_TOKEN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000)\n",
    "\n",
    "\n",
    "DIRECTORY_PATH = './'\n",
    "DATASET_PATH = os.path.join(DIRECTORY_PATH, 'dataset')\n",
    "MODEL_PATH = os.path.join(DIRECTORY_PATH, 'model/generated_model.pth')\n",
    "RESULTS_PATH = os.path.join(DIRECTORY_PATH, 'results')\n",
    "\n",
    "\n",
    "EPOCHS = 500 # 500\n",
    "LEARNING_RATE = 4\n",
    "BATCH_SIZE = 2\n",
    "TRAIN_MODEL = True\n",
    "SPLIT_DATASET = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 1\n",
      "Number of output files: 1\n",
      "\n",
      "\n",
      "1: drum_excited.MID -> bass_example.MID\n",
      "\n",
      "Number of input bars: 24\n",
      "Number of input sequences: 20\n",
      "Input sequence length: 192\n",
      "Input vocabulars size: 13\n",
      "\n",
      "Number of output bars: 33\n",
      "Number of output sequences: 29\n",
      "Output sequence length: 192\n",
      "Output vocabulars size: 30\n",
      "\n",
      "Number of sequences after truncation: 20, 20\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assumptions:\n",
    "Sequences described as input_#.mid and output_#.mid in the corresponding folders\n",
    "'''\n",
    "\n",
    "input_filenames = glob.glob(os.path.join(DATASET_PATH, 'input/*.MID'))\n",
    "print('Number of input files:', len(input_filenames))\n",
    "\n",
    "output_filenames = glob.glob(os.path.join(DATASET_PATH, 'output/*.MID'))\n",
    "print('Number of output files:', len(output_filenames))\n",
    "\n",
    "\n",
    "for i, (in_file, out_file) in enumerate(zip(input_filenames, output_filenames)):\n",
    "\n",
    "    in_file_name = os.path.basename(in_file)\n",
    "    out_file_name = os.path.basename(out_file)\n",
    "    print(f'\\n\\n{i + 1}: {in_file_name} -> {out_file_name}')\n",
    "\n",
    "    INPUT_TOK = PrettyMidiTokenizer(in_file)\n",
    "    print(f'\\nNumber of input bars: {INPUT_TOK.num_bars}')\n",
    "    print(f'Number of input sequences: {len(INPUT_TOK.sequences)}')\n",
    "    print(f'Input sequence length: {len(INPUT_TOK.sequences[0])}')\n",
    "    print(f'Input vocabulars size: {len(INPUT_TOK.VOCAB)}')\n",
    "\n",
    "    OUTPUT_TOK = PrettyMidiTokenizer(out_file)\n",
    "    print(f'\\nNumber of output bars: {OUTPUT_TOK.num_bars}')\n",
    "    print(f'Number of output sequences: {len(OUTPUT_TOK.sequences)}')\n",
    "    print(f'Output sequence length: {len(OUTPUT_TOK.sequences[0])}')\n",
    "    print(f'Output vocabulars size: {len(OUTPUT_TOK.VOCAB)}')\n",
    "\n",
    "    # force the sequences to be the same length\n",
    "    min_length = min(len(INPUT_TOK.sequences), len(OUTPUT_TOK.sequences))\n",
    "    INPUT_TOK.sequences = INPUT_TOK.sequences[:min_length]\n",
    "    OUTPUT_TOK.sequences = OUTPUT_TOK.sequences[:min_length]\n",
    "    print(f'\\nNumber of sequences after truncation: {len(INPUT_TOK.sequences)}, {len(OUTPUT_TOK.sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gianni\\AppData\\Local\\Temp\\ipykernel_6756\\406044318.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = TensorDataset(torch.LongTensor(INPUT_TOK.sequences).to(device),\n",
    "                        torch.LongTensor(OUTPUT_TOK.sequences).to(device))\n",
    "\n",
    "if SPLIT_DATASET:\n",
    "    # Split the dataset into training, evaluation and test sets\n",
    "    train_set, eval_set, test_set = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "    # Create the dataloaders\n",
    "    train_sampler = RandomSampler(train_set)          \n",
    "    train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    eval_sampler = RandomSampler(eval_set)\n",
    "    eval_dataloader = DataLoader(eval_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    test_sampler = RandomSampler(test_set)\n",
    "    test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "else:\n",
    "    train_set = dataset\n",
    "    train_sampler = RandomSampler(train_set)          \n",
    "    train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    eval_dataloader = None\n",
    "    test_dataloader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "SEED = 1111 \n",
    "OUTPUT_VOCAB_SIZE = len(OUTPUT_TOK.VOCAB)\n",
    "EMBEDDING_SIZE = 20 # size of word embeddings -> Embedding() is used to encode input token into [192, 20] vectors (see model.py)\n",
    "LEVELS = 7\n",
    "HIDDEN_UNITS = 192\n",
    "NUM_CHANNELS = [HIDDEN_UNITS] * (LEVELS - 1) + [EMBEDDING_SIZE]\n",
    "GRADIENT_CLIP = 0.35\n",
    "\n",
    "\n",
    "# reduce the weights of the silence token since it is overrepresented in the dataset\n",
    "silence_id = OUTPUT_TOK.VOCAB.word2idx[SILENCE_TOKEN]\n",
    "LOSS_WEIGTHS = torch.ones([OUTPUT_VOCAB_SIZE], dtype=torch.float)\n",
    "LOSS_WEIGTHS[silence_id] = 0.3\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = TCN(input_size = EMBEDDING_SIZE, \n",
    "            output_size = OUTPUT_VOCAB_SIZE, \n",
    "            num_channels = NUM_CHANNELS, \n",
    "            dropout = 0.45, \n",
    "            emb_dropout = 0.25, \n",
    "            kernel_size = 3, \n",
    "            tied_weights = False) # tie encoder and decoder weights (legare)\n",
    "\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "torch.manual_seed(SEED)\n",
    "criterion = nn.CrossEntropyLoss(weight = LOSS_WEIGTHS)\n",
    "optimizer = getattr(optim, 'SGD')(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAR_LENGTH = INPUT_TOK.BAR_LENGTH\n",
    "\n",
    "def epoch_step(dataloader, mode):\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval() # disable dropout\n",
    "        \n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        # mask the last bar of the input data \n",
    "        batch_size = data.size(0)\n",
    "        data_masked = torch.cat((data[:, :BAR_LENGTH*3], torch.ones([batch_size, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "        # reset model gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make the prediction\n",
    "        output = model(data_masked)\n",
    "\n",
    "        # flatten the output sequence\n",
    "        # NB: the size -1 is inferred from other dimensions\n",
    "        # NB: contiguous() is used to make sure the tensor is stored in a contiguous chunk of memory, necessary for view() to work\n",
    "        final_target = targets.contiguous().view(-1)\n",
    "        final_output = output.contiguous().view(-1, OUTPUT_VOCAB_SIZE)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(final_output, final_target)\n",
    "\n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        if GRADIENT_CLIP > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1/500 | lr 4.00000 | ms/epoch 1228.55568 | train_loss  3.13 | eval_loss  0.00\n",
      "| epoch   2/500 | lr 4.00000 | ms/epoch 1059.20482 | train_loss  2.91 | eval_loss  0.00\n",
      "| epoch   3/500 | lr 4.00000 | ms/epoch 1084.24735 | train_loss  2.87 | eval_loss  0.00\n",
      "| epoch   4/500 | lr 4.00000 | ms/epoch 1087.37683 | train_loss  2.88 | eval_loss  0.00\n",
      "| epoch   5/500 | lr 4.00000 | ms/epoch 1149.17779 | train_loss  2.87 | eval_loss  0.00\n",
      "| epoch   6/500 | lr 4.00000 | ms/epoch 1168.89882 | train_loss  2.86 | eval_loss  0.00\n",
      "| epoch   7/500 | lr 4.00000 | ms/epoch 1022.55273 | train_loss  2.86 | eval_loss  0.00\n",
      "| epoch   8/500 | lr 4.00000 | ms/epoch 949.40281 | train_loss  2.86 | eval_loss  0.00\n",
      "| epoch   9/500 | lr 4.00000 | ms/epoch 980.65543 | train_loss  2.87 | eval_loss  0.00\n",
      "| epoch  10/500 | lr 4.00000 | ms/epoch 1029.45995 | train_loss  2.86 | eval_loss  0.00\n",
      "| epoch  11/500 | lr 4.00000 | ms/epoch 1280.31993 | train_loss  2.85 | eval_loss  0.00\n",
      "| epoch  12/500 | lr 4.00000 | ms/epoch 1222.30172 | train_loss  2.85 | eval_loss  0.00\n",
      "| epoch  13/500 | lr 4.00000 | ms/epoch 1025.26283 | train_loss  2.72 | eval_loss  0.00\n",
      "| epoch  14/500 | lr 4.00000 | ms/epoch 1201.17974 | train_loss  2.62 | eval_loss  0.00\n",
      "| epoch  15/500 | lr 4.00000 | ms/epoch 1215.17634 | train_loss  2.59 | eval_loss  0.00\n",
      "| epoch  16/500 | lr 4.00000 | ms/epoch 1340.90137 | train_loss  2.63 | eval_loss  0.00\n",
      "| epoch  17/500 | lr 4.00000 | ms/epoch 1447.15810 | train_loss  2.56 | eval_loss  0.00\n",
      "| epoch  18/500 | lr 4.00000 | ms/epoch 1578.63688 | train_loss  2.45 | eval_loss  0.00\n",
      "| epoch  19/500 | lr 4.00000 | ms/epoch 1394.34052 | train_loss  2.37 | eval_loss  0.00\n",
      "| epoch  20/500 | lr 4.00000 | ms/epoch 1410.70533 | train_loss  2.36 | eval_loss  0.00\n",
      "| epoch  21/500 | lr 4.00000 | ms/epoch 1394.79470 | train_loss  2.33 | eval_loss  0.00\n",
      "| epoch  22/500 | lr 4.00000 | ms/epoch 1375.48614 | train_loss  2.27 | eval_loss  0.00\n",
      "| epoch  23/500 | lr 4.00000 | ms/epoch 1347.94617 | train_loss  2.22 | eval_loss  0.00\n",
      "| epoch  24/500 | lr 4.00000 | ms/epoch 1441.18047 | train_loss  2.16 | eval_loss  0.00\n",
      "| epoch  25/500 | lr 4.00000 | ms/epoch 1283.96082 | train_loss  2.12 | eval_loss  0.00\n",
      "| epoch  26/500 | lr 4.00000 | ms/epoch 1339.86163 | train_loss  2.09 | eval_loss  0.00\n",
      "| epoch  27/500 | lr 4.00000 | ms/epoch 1388.69119 | train_loss  2.11 | eval_loss  0.00\n",
      "| epoch  28/500 | lr 4.00000 | ms/epoch 1408.74624 | train_loss  2.04 | eval_loss  0.00\n",
      "| epoch  29/500 | lr 4.00000 | ms/epoch 1444.80371 | train_loss  2.06 | eval_loss  0.00\n",
      "| epoch  30/500 | lr 4.00000 | ms/epoch 1413.31601 | train_loss  2.03 | eval_loss  0.00\n",
      "| epoch  31/500 | lr 4.00000 | ms/epoch 1493.75463 | train_loss  2.03 | eval_loss  0.00\n",
      "| epoch  32/500 | lr 4.00000 | ms/epoch 1673.32697 | train_loss  1.96 | eval_loss  0.00\n",
      "| epoch  33/500 | lr 4.00000 | ms/epoch 1687.30211 | train_loss  1.96 | eval_loss  0.00\n",
      "| epoch  34/500 | lr 4.00000 | ms/epoch 1796.83638 | train_loss  1.96 | eval_loss  0.00\n",
      "| epoch  35/500 | lr 4.00000 | ms/epoch 2047.55497 | train_loss  1.95 | eval_loss  0.00\n",
      "| epoch  36/500 | lr 4.00000 | ms/epoch 2436.87081 | train_loss  1.92 | eval_loss  0.00\n",
      "| epoch  37/500 | lr 4.00000 | ms/epoch 2560.68850 | train_loss  1.93 | eval_loss  0.00\n",
      "| epoch  38/500 | lr 4.00000 | ms/epoch 2746.96898 | train_loss  1.91 | eval_loss  0.00\n",
      "| epoch  39/500 | lr 4.00000 | ms/epoch 2137.51388 | train_loss  1.90 | eval_loss  0.00\n",
      "| epoch  40/500 | lr 4.00000 | ms/epoch 2236.37891 | train_loss  1.89 | eval_loss  0.00\n",
      "| epoch  41/500 | lr 4.00000 | ms/epoch 2125.47207 | train_loss  1.87 | eval_loss  0.00\n",
      "| epoch  42/500 | lr 4.00000 | ms/epoch 2657.62520 | train_loss  1.87 | eval_loss  0.00\n",
      "| epoch  43/500 | lr 4.00000 | ms/epoch 2609.28345 | train_loss  1.88 | eval_loss  0.00\n",
      "| epoch  44/500 | lr 4.00000 | ms/epoch 2452.72064 | train_loss  1.87 | eval_loss  0.00\n",
      "| epoch  45/500 | lr 4.00000 | ms/epoch 2890.86699 | train_loss  1.83 | eval_loss  0.00\n",
      "| epoch  46/500 | lr 4.00000 | ms/epoch 2010.01310 | train_loss  1.83 | eval_loss  0.00\n",
      "| epoch  47/500 | lr 4.00000 | ms/epoch 2455.78122 | train_loss  1.83 | eval_loss  0.00\n",
      "| epoch  48/500 | lr 4.00000 | ms/epoch 2267.77864 | train_loss  1.77 | eval_loss  0.00\n",
      "| epoch  49/500 | lr 4.00000 | ms/epoch 1973.74320 | train_loss  1.82 | eval_loss  0.00\n",
      "| epoch  50/500 | lr 4.00000 | ms/epoch 1978.49059 | train_loss  1.79 | eval_loss  0.00\n",
      "| epoch  51/500 | lr 4.00000 | ms/epoch 1912.29844 | train_loss  1.76 | eval_loss  0.00\n",
      "| epoch  52/500 | lr 4.00000 | ms/epoch 1908.75220 | train_loss  1.80 | eval_loss  0.00\n",
      "| epoch  53/500 | lr 4.00000 | ms/epoch 2047.54686 | train_loss  1.76 | eval_loss  0.00\n",
      "| epoch  54/500 | lr 4.00000 | ms/epoch 2463.39512 | train_loss  1.75 | eval_loss  0.00\n",
      "| epoch  55/500 | lr 4.00000 | ms/epoch 2541.18204 | train_loss  1.75 | eval_loss  0.00\n",
      "| epoch  56/500 | lr 4.00000 | ms/epoch 1714.97726 | train_loss  1.70 | eval_loss  0.00\n",
      "| epoch  57/500 | lr 4.00000 | ms/epoch 1815.03201 | train_loss  1.67 | eval_loss  0.00\n",
      "| epoch  58/500 | lr 4.00000 | ms/epoch 1826.37405 | train_loss  1.70 | eval_loss  0.00\n",
      "| epoch  59/500 | lr 4.00000 | ms/epoch 1440.09233 | train_loss  1.66 | eval_loss  0.00\n",
      "| epoch  60/500 | lr 4.00000 | ms/epoch 1346.05932 | train_loss  1.65 | eval_loss  0.00\n",
      "| epoch  61/500 | lr 4.00000 | ms/epoch 1253.10636 | train_loss  1.71 | eval_loss  0.00\n",
      "| epoch  62/500 | lr 4.00000 | ms/epoch 2193.88437 | train_loss  1.63 | eval_loss  0.00\n",
      "| epoch  63/500 | lr 4.00000 | ms/epoch 1617.29383 | train_loss  1.60 | eval_loss  0.00\n",
      "| epoch  64/500 | lr 4.00000 | ms/epoch 1223.48475 | train_loss  1.64 | eval_loss  0.00\n",
      "| epoch  65/500 | lr 4.00000 | ms/epoch 1338.49978 | train_loss  1.61 | eval_loss  0.00\n",
      "| epoch  66/500 | lr 4.00000 | ms/epoch 1210.96420 | train_loss  1.61 | eval_loss  0.00\n",
      "| epoch  67/500 | lr 4.00000 | ms/epoch 1595.96920 | train_loss  1.56 | eval_loss  0.00\n",
      "| epoch  68/500 | lr 4.00000 | ms/epoch 1872.62893 | train_loss  1.59 | eval_loss  0.00\n",
      "| epoch  69/500 | lr 4.00000 | ms/epoch 2294.00039 | train_loss  1.58 | eval_loss  0.00\n",
      "| epoch  70/500 | lr 4.00000 | ms/epoch 1169.86704 | train_loss  1.52 | eval_loss  0.00\n",
      "| epoch  71/500 | lr 4.00000 | ms/epoch 999.05777 | train_loss  1.55 | eval_loss  0.00\n",
      "| epoch  72/500 | lr 4.00000 | ms/epoch 1150.07687 | train_loss  1.52 | eval_loss  0.00\n",
      "| epoch  73/500 | lr 4.00000 | ms/epoch 1694.45539 | train_loss  1.50 | eval_loss  0.00\n",
      "| epoch  74/500 | lr 4.00000 | ms/epoch 1734.07364 | train_loss  1.52 | eval_loss  0.00\n",
      "| epoch  75/500 | lr 4.00000 | ms/epoch 1370.01920 | train_loss  1.50 | eval_loss  0.00\n",
      "| epoch  76/500 | lr 4.00000 | ms/epoch 1246.26899 | train_loss  1.48 | eval_loss  0.00\n",
      "| epoch  77/500 | lr 4.00000 | ms/epoch 1494.25435 | train_loss  1.47 | eval_loss  0.00\n",
      "| epoch  78/500 | lr 4.00000 | ms/epoch 1220.68906 | train_loss  1.48 | eval_loss  0.00\n",
      "| epoch  79/500 | lr 4.00000 | ms/epoch 1404.62995 | train_loss  1.41 | eval_loss  0.00\n",
      "| epoch  80/500 | lr 4.00000 | ms/epoch 1361.07612 | train_loss  1.44 | eval_loss  0.00\n",
      "| epoch  81/500 | lr 4.00000 | ms/epoch 1771.44694 | train_loss  1.40 | eval_loss  0.00\n",
      "| epoch  82/500 | lr 4.00000 | ms/epoch 1681.50830 | train_loss  1.49 | eval_loss  0.00\n",
      "| epoch  83/500 | lr 4.00000 | ms/epoch 1704.68879 | train_loss  1.41 | eval_loss  0.00\n",
      "| epoch  84/500 | lr 4.00000 | ms/epoch 1961.43293 | train_loss  1.41 | eval_loss  0.00\n",
      "| epoch  85/500 | lr 4.00000 | ms/epoch 2457.56221 | train_loss  1.39 | eval_loss  0.00\n",
      "| epoch  86/500 | lr 4.00000 | ms/epoch 3191.58196 | train_loss  1.39 | eval_loss  0.00\n",
      "| epoch  87/500 | lr 4.00000 | ms/epoch 1745.28980 | train_loss  1.34 | eval_loss  0.00\n",
      "| epoch  88/500 | lr 4.00000 | ms/epoch 1870.04638 | train_loss  1.34 | eval_loss  0.00\n",
      "| epoch  89/500 | lr 4.00000 | ms/epoch 1859.64370 | train_loss  1.34 | eval_loss  0.00\n",
      "| epoch  90/500 | lr 4.00000 | ms/epoch 1839.98513 | train_loss  1.33 | eval_loss  0.00\n",
      "| epoch  91/500 | lr 4.00000 | ms/epoch 2052.55032 | train_loss  1.29 | eval_loss  0.00\n",
      "| epoch  92/500 | lr 4.00000 | ms/epoch 2321.99025 | train_loss  1.33 | eval_loss  0.00\n",
      "| epoch  93/500 | lr 4.00000 | ms/epoch 1913.67888 | train_loss  1.29 | eval_loss  0.00\n",
      "| epoch  94/500 | lr 4.00000 | ms/epoch 2154.94394 | train_loss  1.29 | eval_loss  0.00\n",
      "| epoch  95/500 | lr 4.00000 | ms/epoch 1983.75559 | train_loss  1.28 | eval_loss  0.00\n",
      "| epoch  96/500 | lr 4.00000 | ms/epoch 2591.21037 | train_loss  1.28 | eval_loss  0.00\n",
      "| epoch  97/500 | lr 4.00000 | ms/epoch 2997.62988 | train_loss  1.26 | eval_loss  0.00\n",
      "| epoch  98/500 | lr 4.00000 | ms/epoch 2224.91550 | train_loss  1.22 | eval_loss  0.00\n",
      "| epoch  99/500 | lr 4.00000 | ms/epoch 1962.70490 | train_loss  1.21 | eval_loss  0.00\n",
      "| epoch 100/500 | lr 4.00000 | ms/epoch 1830.46794 | train_loss  1.24 | eval_loss  0.00\n",
      "| epoch 101/500 | lr 4.00000 | ms/epoch 1940.10401 | train_loss  1.18 | eval_loss  0.00\n",
      "| epoch 102/500 | lr 4.00000 | ms/epoch 1821.28239 | train_loss  1.18 | eval_loss  0.00\n",
      "| epoch 103/500 | lr 4.00000 | ms/epoch 2556.13637 | train_loss  1.15 | eval_loss  0.00\n",
      "| epoch 104/500 | lr 4.00000 | ms/epoch 2854.74896 | train_loss  1.13 | eval_loss  0.00\n",
      "| epoch 105/500 | lr 4.00000 | ms/epoch 3488.74402 | train_loss  1.13 | eval_loss  0.00\n",
      "| epoch 106/500 | lr 4.00000 | ms/epoch 4944.39816 | train_loss  1.15 | eval_loss  0.00\n",
      "| epoch 107/500 | lr 4.00000 | ms/epoch 3518.92734 | train_loss  1.12 | eval_loss  0.00\n",
      "| epoch 108/500 | lr 4.00000 | ms/epoch 2415.29489 | train_loss  1.12 | eval_loss  0.00\n",
      "| epoch 109/500 | lr 4.00000 | ms/epoch 2607.53798 | train_loss  1.10 | eval_loss  0.00\n",
      "| epoch 110/500 | lr 4.00000 | ms/epoch 2810.55021 | train_loss  1.03 | eval_loss  0.00\n",
      "| epoch 111/500 | lr 4.00000 | ms/epoch 4004.34947 | train_loss  1.06 | eval_loss  0.00\n",
      "| epoch 112/500 | lr 4.00000 | ms/epoch 2472.22853 | train_loss  1.04 | eval_loss  0.00\n",
      "| epoch 113/500 | lr 4.00000 | ms/epoch 1996.80305 | train_loss  1.03 | eval_loss  0.00\n",
      "| epoch 114/500 | lr 4.00000 | ms/epoch 2053.86353 | train_loss  1.02 | eval_loss  0.00\n",
      "| epoch 115/500 | lr 4.00000 | ms/epoch 1641.51192 | train_loss  1.04 | eval_loss  0.00\n",
      "| epoch 116/500 | lr 4.00000 | ms/epoch 1459.99861 | train_loss  1.00 | eval_loss  0.00\n",
      "| epoch 117/500 | lr 4.00000 | ms/epoch 1349.82657 | train_loss  0.97 | eval_loss  0.00\n",
      "| epoch 118/500 | lr 4.00000 | ms/epoch 1376.44815 | train_loss  0.99 | eval_loss  0.00\n",
      "| epoch 119/500 | lr 4.00000 | ms/epoch 1306.82850 | train_loss  0.94 | eval_loss  0.00\n",
      "| epoch 120/500 | lr 4.00000 | ms/epoch 1180.27616 | train_loss  0.94 | eval_loss  0.00\n",
      "| epoch 121/500 | lr 4.00000 | ms/epoch 1415.16948 | train_loss  0.93 | eval_loss  0.00\n",
      "| epoch 122/500 | lr 4.00000 | ms/epoch 1190.32168 | train_loss  0.94 | eval_loss  0.00\n",
      "| epoch 123/500 | lr 4.00000 | ms/epoch 1119.73453 | train_loss  0.92 | eval_loss  0.00\n",
      "| epoch 124/500 | lr 4.00000 | ms/epoch 1380.74160 | train_loss  0.91 | eval_loss  0.00\n",
      "| epoch 125/500 | lr 4.00000 | ms/epoch 1241.96911 | train_loss  0.87 | eval_loss  0.00\n",
      "| epoch 126/500 | lr 4.00000 | ms/epoch 1070.52469 | train_loss  0.88 | eval_loss  0.00\n",
      "| epoch 127/500 | lr 4.00000 | ms/epoch 1065.03344 | train_loss  0.93 | eval_loss  0.00\n",
      "| epoch 128/500 | lr 4.00000 | ms/epoch 1106.41885 | train_loss  0.87 | eval_loss  0.00\n",
      "| epoch 129/500 | lr 4.00000 | ms/epoch 1194.57817 | train_loss  0.84 | eval_loss  0.00\n",
      "| epoch 130/500 | lr 4.00000 | ms/epoch 1415.51208 | train_loss  0.79 | eval_loss  0.00\n",
      "| epoch 131/500 | lr 4.00000 | ms/epoch 1427.97041 | train_loss  0.83 | eval_loss  0.00\n",
      "| epoch 132/500 | lr 4.00000 | ms/epoch 1341.30645 | train_loss  0.80 | eval_loss  0.00\n",
      "| epoch 133/500 | lr 4.00000 | ms/epoch 1448.66705 | train_loss  0.81 | eval_loss  0.00\n",
      "| epoch 134/500 | lr 4.00000 | ms/epoch 1409.88946 | train_loss  0.78 | eval_loss  0.00\n",
      "| epoch 135/500 | lr 4.00000 | ms/epoch 1411.88145 | train_loss  0.79 | eval_loss  0.00\n",
      "| epoch 136/500 | lr 4.00000 | ms/epoch 1471.29846 | train_loss  0.78 | eval_loss  0.00\n",
      "| epoch 137/500 | lr 4.00000 | ms/epoch 1568.30049 | train_loss  0.76 | eval_loss  0.00\n",
      "| epoch 138/500 | lr 4.00000 | ms/epoch 1677.08755 | train_loss  0.77 | eval_loss  0.00\n",
      "| epoch 139/500 | lr 4.00000 | ms/epoch 1907.69696 | train_loss  0.73 | eval_loss  0.00\n",
      "| epoch 140/500 | lr 4.00000 | ms/epoch 2066.54382 | train_loss  0.72 | eval_loss  0.00\n",
      "| epoch 141/500 | lr 4.00000 | ms/epoch 2073.08865 | train_loss  0.69 | eval_loss  0.00\n",
      "| epoch 142/500 | lr 4.00000 | ms/epoch 1927.31261 | train_loss  0.73 | eval_loss  0.00\n",
      "| epoch 143/500 | lr 4.00000 | ms/epoch 1740.26561 | train_loss  0.70 | eval_loss  0.00\n",
      "| epoch 144/500 | lr 4.00000 | ms/epoch 1994.62891 | train_loss  0.73 | eval_loss  0.00\n",
      "| epoch 145/500 | lr 4.00000 | ms/epoch 2300.49396 | train_loss  0.69 | eval_loss  0.00\n",
      "| epoch 146/500 | lr 4.00000 | ms/epoch 2367.90538 | train_loss  0.68 | eval_loss  0.00\n",
      "| epoch 147/500 | lr 4.00000 | ms/epoch 2093.01448 | train_loss  0.74 | eval_loss  0.00\n",
      "| epoch 148/500 | lr 4.00000 | ms/epoch 2557.05881 | train_loss  0.66 | eval_loss  0.00\n",
      "| epoch 149/500 | lr 4.00000 | ms/epoch 2635.00810 | train_loss  0.67 | eval_loss  0.00\n",
      "| epoch 150/500 | lr 4.00000 | ms/epoch 3773.78774 | train_loss  0.68 | eval_loss  0.00\n",
      "| epoch 151/500 | lr 4.00000 | ms/epoch 2607.69200 | train_loss  0.63 | eval_loss  0.00\n",
      "| epoch 152/500 | lr 4.00000 | ms/epoch 2405.74074 | train_loss  0.67 | eval_loss  0.00\n",
      "| epoch 153/500 | lr 4.00000 | ms/epoch 2389.89592 | train_loss  0.63 | eval_loss  0.00\n",
      "| epoch 154/500 | lr 4.00000 | ms/epoch 3044.78264 | train_loss  0.63 | eval_loss  0.00\n",
      "| epoch 155/500 | lr 4.00000 | ms/epoch 2762.41159 | train_loss  0.61 | eval_loss  0.00\n",
      "| epoch 156/500 | lr 4.00000 | ms/epoch 2399.78600 | train_loss  0.60 | eval_loss  0.00\n",
      "| epoch 157/500 | lr 4.00000 | ms/epoch 1910.34460 | train_loss  0.59 | eval_loss  0.00\n",
      "| epoch 158/500 | lr 4.00000 | ms/epoch 1869.55452 | train_loss  0.58 | eval_loss  0.00\n",
      "| epoch 159/500 | lr 4.00000 | ms/epoch 1710.12974 | train_loss  0.62 | eval_loss  0.00\n",
      "| epoch 160/500 | lr 4.00000 | ms/epoch 1726.84646 | train_loss  0.61 | eval_loss  0.00\n",
      "| epoch 161/500 | lr 4.00000 | ms/epoch 1759.11689 | train_loss  0.56 | eval_loss  0.00\n",
      "| epoch 162/500 | lr 4.00000 | ms/epoch 1820.11509 | train_loss  0.56 | eval_loss  0.00\n",
      "| epoch 163/500 | lr 4.00000 | ms/epoch 1759.94039 | train_loss  0.56 | eval_loss  0.00\n",
      "| epoch 164/500 | lr 4.00000 | ms/epoch 1679.60167 | train_loss  0.58 | eval_loss  0.00\n",
      "| epoch 165/500 | lr 4.00000 | ms/epoch 1783.73313 | train_loss  0.54 | eval_loss  0.00\n",
      "| epoch 166/500 | lr 4.00000 | ms/epoch 1418.39886 | train_loss  0.54 | eval_loss  0.00\n",
      "| epoch 167/500 | lr 4.00000 | ms/epoch 1310.22954 | train_loss  0.55 | eval_loss  0.00\n",
      "| epoch 168/500 | lr 4.00000 | ms/epoch 1211.78126 | train_loss  0.57 | eval_loss  0.00\n",
      "| epoch 169/500 | lr 4.00000 | ms/epoch 1187.97684 | train_loss  0.53 | eval_loss  0.00\n",
      "| epoch 170/500 | lr 4.00000 | ms/epoch 1102.32592 | train_loss  0.50 | eval_loss  0.00\n",
      "| epoch 171/500 | lr 4.00000 | ms/epoch 1107.77617 | train_loss  0.54 | eval_loss  0.00\n",
      "| epoch 172/500 | lr 4.00000 | ms/epoch 1074.94521 | train_loss  0.52 | eval_loss  0.00\n",
      "| epoch 173/500 | lr 4.00000 | ms/epoch 1060.43696 | train_loss  0.52 | eval_loss  0.00\n",
      "| epoch 174/500 | lr 4.00000 | ms/epoch 1047.94478 | train_loss  0.50 | eval_loss  0.00\n",
      "| epoch 175/500 | lr 4.00000 | ms/epoch 1106.50873 | train_loss  0.50 | eval_loss  0.00\n",
      "| epoch 176/500 | lr 4.00000 | ms/epoch 1051.57685 | train_loss  0.52 | eval_loss  0.00\n",
      "| epoch 177/500 | lr 4.00000 | ms/epoch 1070.61124 | train_loss  0.54 | eval_loss  0.00\n",
      "| epoch 178/500 | lr 4.00000 | ms/epoch 1110.56066 | train_loss  0.48 | eval_loss  0.00\n",
      "| epoch 179/500 | lr 4.00000 | ms/epoch 1062.59847 | train_loss  0.52 | eval_loss  0.00\n",
      "| epoch 180/500 | lr 4.00000 | ms/epoch 1094.92660 | train_loss  0.46 | eval_loss  0.00\n",
      "| epoch 181/500 | lr 4.00000 | ms/epoch 1075.14811 | train_loss  0.49 | eval_loss  0.00\n",
      "| epoch 182/500 | lr 4.00000 | ms/epoch 1145.59245 | train_loss  0.44 | eval_loss  0.00\n",
      "| epoch 183/500 | lr 4.00000 | ms/epoch 1121.83380 | train_loss  0.46 | eval_loss  0.00\n",
      "| epoch 184/500 | lr 4.00000 | ms/epoch 1061.78975 | train_loss  0.46 | eval_loss  0.00\n",
      "| epoch 185/500 | lr 4.00000 | ms/epoch 1084.54442 | train_loss  0.45 | eval_loss  0.00\n",
      "| epoch 186/500 | lr 4.00000 | ms/epoch 1119.95769 | train_loss  0.44 | eval_loss  0.00\n",
      "| epoch 187/500 | lr 4.00000 | ms/epoch 1149.86086 | train_loss  0.43 | eval_loss  0.00\n",
      "| epoch 188/500 | lr 4.00000 | ms/epoch 1110.33583 | train_loss  0.44 | eval_loss  0.00\n",
      "| epoch 189/500 | lr 4.00000 | ms/epoch 1169.44647 | train_loss  0.44 | eval_loss  0.00\n",
      "| epoch 190/500 | lr 4.00000 | ms/epoch 1220.15214 | train_loss  0.41 | eval_loss  0.00\n",
      "| epoch 191/500 | lr 4.00000 | ms/epoch 1219.99216 | train_loss  0.41 | eval_loss  0.00\n",
      "| epoch 192/500 | lr 4.00000 | ms/epoch 1223.89340 | train_loss  0.41 | eval_loss  0.00\n",
      "| epoch 193/500 | lr 4.00000 | ms/epoch 1267.06219 | train_loss  0.41 | eval_loss  0.00\n",
      "| epoch 194/500 | lr 4.00000 | ms/epoch 1331.16174 | train_loss  0.44 | eval_loss  0.00\n",
      "| epoch 195/500 | lr 4.00000 | ms/epoch 1354.28119 | train_loss  0.42 | eval_loss  0.00\n",
      "| epoch 196/500 | lr 4.00000 | ms/epoch 1484.00998 | train_loss  0.43 | eval_loss  0.00\n",
      "| epoch 197/500 | lr 4.00000 | ms/epoch 1562.05654 | train_loss  0.40 | eval_loss  0.00\n",
      "| epoch 198/500 | lr 4.00000 | ms/epoch 1898.89503 | train_loss  0.40 | eval_loss  0.00\n",
      "| epoch 199/500 | lr 4.00000 | ms/epoch 1788.02419 | train_loss  0.42 | eval_loss  0.00\n",
      "| epoch 200/500 | lr 4.00000 | ms/epoch 1759.52816 | train_loss  0.43 | eval_loss  0.00\n",
      "| epoch 201/500 | lr 4.00000 | ms/epoch 1840.91496 | train_loss  0.40 | eval_loss  0.00\n",
      "| epoch 202/500 | lr 4.00000 | ms/epoch 1758.01158 | train_loss  0.40 | eval_loss  0.00\n",
      "| epoch 203/500 | lr 4.00000 | ms/epoch 1808.41374 | train_loss  0.34 | eval_loss  0.00\n",
      "| epoch 204/500 | lr 4.00000 | ms/epoch 1768.71634 | train_loss  0.39 | eval_loss  0.00\n",
      "| epoch 205/500 | lr 4.00000 | ms/epoch 1789.25824 | train_loss  0.39 | eval_loss  0.00\n",
      "| epoch 206/500 | lr 4.00000 | ms/epoch 1806.08058 | train_loss  0.36 | eval_loss  0.00\n",
      "| epoch 207/500 | lr 4.00000 | ms/epoch 1729.66361 | train_loss  0.38 | eval_loss  0.00\n",
      "| epoch 208/500 | lr 4.00000 | ms/epoch 1678.50375 | train_loss  0.37 | eval_loss  0.00\n",
      "| epoch 209/500 | lr 4.00000 | ms/epoch 1769.20009 | train_loss  0.38 | eval_loss  0.00\n",
      "| epoch 210/500 | lr 4.00000 | ms/epoch 1729.86245 | train_loss  0.35 | eval_loss  0.00\n",
      "| epoch 211/500 | lr 4.00000 | ms/epoch 1720.01004 | train_loss  0.36 | eval_loss  0.00\n",
      "| epoch 212/500 | lr 4.00000 | ms/epoch 1663.33652 | train_loss  0.39 | eval_loss  0.00\n",
      "| epoch 213/500 | lr 4.00000 | ms/epoch 1701.91956 | train_loss  0.36 | eval_loss  0.00\n",
      "| epoch 214/500 | lr 4.00000 | ms/epoch 1695.78242 | train_loss  0.35 | eval_loss  0.00\n",
      "| epoch 215/500 | lr 4.00000 | ms/epoch 1760.85138 | train_loss  0.35 | eval_loss  0.00\n",
      "| epoch 216/500 | lr 4.00000 | ms/epoch 1755.35631 | train_loss  0.33 | eval_loss  0.00\n",
      "| epoch 217/500 | lr 4.00000 | ms/epoch 1713.82761 | train_loss  0.34 | eval_loss  0.00\n",
      "| epoch 218/500 | lr 4.00000 | ms/epoch 1890.87081 | train_loss  0.33 | eval_loss  0.00\n",
      "| epoch 219/500 | lr 4.00000 | ms/epoch 1794.05355 | train_loss  0.32 | eval_loss  0.00\n",
      "| epoch 220/500 | lr 4.00000 | ms/epoch 1720.47305 | train_loss  0.32 | eval_loss  0.00\n",
      "| epoch 221/500 | lr 4.00000 | ms/epoch 1611.94229 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 222/500 | lr 4.00000 | ms/epoch 1367.88726 | train_loss  0.32 | eval_loss  0.00\n",
      "| epoch 223/500 | lr 4.00000 | ms/epoch 1215.51442 | train_loss  0.35 | eval_loss  0.00\n",
      "| epoch 224/500 | lr 4.00000 | ms/epoch 1228.23048 | train_loss  0.32 | eval_loss  0.00\n",
      "| epoch 225/500 | lr 4.00000 | ms/epoch 1144.11616 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 226/500 | lr 4.00000 | ms/epoch 1161.88240 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 227/500 | lr 4.00000 | ms/epoch 1063.33542 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 228/500 | lr 4.00000 | ms/epoch 1051.82338 | train_loss  0.33 | eval_loss  0.00\n",
      "| epoch 229/500 | lr 4.00000 | ms/epoch 1065.70816 | train_loss  0.33 | eval_loss  0.00\n",
      "| epoch 230/500 | lr 4.00000 | ms/epoch 1065.26327 | train_loss  0.32 | eval_loss  0.00\n",
      "| epoch 231/500 | lr 4.00000 | ms/epoch 1053.96652 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 232/500 | lr 4.00000 | ms/epoch 1096.03167 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 233/500 | lr 4.00000 | ms/epoch 1037.20784 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 234/500 | lr 4.00000 | ms/epoch 1099.09534 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 235/500 | lr 4.00000 | ms/epoch 1037.57024 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 236/500 | lr 4.00000 | ms/epoch 1069.08464 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 237/500 | lr 4.00000 | ms/epoch 1044.33441 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 238/500 | lr 4.00000 | ms/epoch 1076.46608 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 239/500 | lr 4.00000 | ms/epoch 1045.43543 | train_loss  0.30 | eval_loss  0.00\n",
      "| epoch 240/500 | lr 4.00000 | ms/epoch 1074.78857 | train_loss  0.30 | eval_loss  0.00\n",
      "| epoch 241/500 | lr 4.00000 | ms/epoch 1092.18216 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 242/500 | lr 4.00000 | ms/epoch 1092.89598 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 243/500 | lr 4.00000 | ms/epoch 1104.93040 | train_loss  0.30 | eval_loss  0.00\n",
      "| epoch 244/500 | lr 4.00000 | ms/epoch 1113.12127 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 245/500 | lr 4.00000 | ms/epoch 1080.62053 | train_loss  0.31 | eval_loss  0.00\n",
      "| epoch 246/500 | lr 4.00000 | ms/epoch 1123.12198 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 247/500 | lr 4.00000 | ms/epoch 1204.29850 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 248/500 | lr 4.00000 | ms/epoch 1279.05393 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 249/500 | lr 4.00000 | ms/epoch 1293.74933 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 250/500 | lr 4.00000 | ms/epoch 1296.02528 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 251/500 | lr 4.00000 | ms/epoch 1333.93884 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 252/500 | lr 4.00000 | ms/epoch 1452.75545 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 253/500 | lr 4.00000 | ms/epoch 1477.42057 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 254/500 | lr 4.00000 | ms/epoch 1665.41600 | train_loss  0.28 | eval_loss  0.00\n",
      "| epoch 255/500 | lr 4.00000 | ms/epoch 1797.54996 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 256/500 | lr 4.00000 | ms/epoch 1772.91656 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 257/500 | lr 4.00000 | ms/epoch 1760.70261 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 258/500 | lr 4.00000 | ms/epoch 1821.41852 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 259/500 | lr 4.00000 | ms/epoch 2008.15821 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 260/500 | lr 4.00000 | ms/epoch 1822.29090 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 261/500 | lr 4.00000 | ms/epoch 1724.57314 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 262/500 | lr 4.00000 | ms/epoch 1721.57907 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 263/500 | lr 4.00000 | ms/epoch 1754.12989 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 264/500 | lr 4.00000 | ms/epoch 1705.87277 | train_loss  0.29 | eval_loss  0.00\n",
      "| epoch 265/500 | lr 4.00000 | ms/epoch 1739.55178 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 266/500 | lr 4.00000 | ms/epoch 1766.07251 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 267/500 | lr 4.00000 | ms/epoch 1747.45131 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 268/500 | lr 4.00000 | ms/epoch 1745.23473 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 269/500 | lr 4.00000 | ms/epoch 1716.11643 | train_loss  0.27 | eval_loss  0.00\n",
      "| epoch 270/500 | lr 4.00000 | ms/epoch 1739.90202 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 271/500 | lr 4.00000 | ms/epoch 1767.69209 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 272/500 | lr 4.00000 | ms/epoch 1796.23055 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 273/500 | lr 4.00000 | ms/epoch 1788.88226 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 274/500 | lr 4.00000 | ms/epoch 1769.25063 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 275/500 | lr 4.00000 | ms/epoch 1708.98008 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 276/500 | lr 4.00000 | ms/epoch 1685.33754 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 277/500 | lr 4.00000 | ms/epoch 1667.12618 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 278/500 | lr 4.00000 | ms/epoch 1652.21262 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 279/500 | lr 4.00000 | ms/epoch 1621.07801 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 280/500 | lr 4.00000 | ms/epoch 1551.96881 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 281/500 | lr 4.00000 | ms/epoch 1651.25823 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 282/500 | lr 4.00000 | ms/epoch 1199.23973 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 283/500 | lr 4.00000 | ms/epoch 1148.52381 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 284/500 | lr 4.00000 | ms/epoch 1091.47310 | train_loss  0.25 | eval_loss  0.00\n",
      "| epoch 285/500 | lr 4.00000 | ms/epoch 1056.59842 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 286/500 | lr 4.00000 | ms/epoch 1045.52364 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 287/500 | lr 4.00000 | ms/epoch 1064.38899 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 288/500 | lr 4.00000 | ms/epoch 1195.39809 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 289/500 | lr 4.00000 | ms/epoch 1070.81628 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 290/500 | lr 4.00000 | ms/epoch 1068.53104 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 291/500 | lr 4.00000 | ms/epoch 1125.21768 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 292/500 | lr 4.00000 | ms/epoch 1072.90912 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 293/500 | lr 4.00000 | ms/epoch 1068.48860 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 294/500 | lr 4.00000 | ms/epoch 1053.47133 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 295/500 | lr 4.00000 | ms/epoch 1070.25337 | train_loss  0.24 | eval_loss  0.00\n",
      "| epoch 296/500 | lr 4.00000 | ms/epoch 1059.71694 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 297/500 | lr 4.00000 | ms/epoch 1081.33388 | train_loss  0.26 | eval_loss  0.00\n",
      "| epoch 298/500 | lr 4.00000 | ms/epoch 1099.52736 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 299/500 | lr 4.00000 | ms/epoch 1120.53776 | train_loss  0.23 | eval_loss  0.00\n",
      "| epoch 300/500 | lr 4.00000 | ms/epoch 1138.09729 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 301/500 | lr 4.00000 | ms/epoch 1166.13269 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 302/500 | lr 4.00000 | ms/epoch 1191.59794 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 303/500 | lr 4.00000 | ms/epoch 1232.16128 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 304/500 | lr 4.00000 | ms/epoch 1233.11067 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 305/500 | lr 4.00000 | ms/epoch 1346.84658 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 306/500 | lr 4.00000 | ms/epoch 1330.21998 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 307/500 | lr 4.00000 | ms/epoch 1492.92612 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 308/500 | lr 4.00000 | ms/epoch 1572.48473 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 309/500 | lr 4.00000 | ms/epoch 1615.54766 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 310/500 | lr 4.00000 | ms/epoch 2103.01113 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 311/500 | lr 4.00000 | ms/epoch 1845.05272 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 312/500 | lr 4.00000 | ms/epoch 1792.89556 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 313/500 | lr 4.00000 | ms/epoch 1766.62898 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 314/500 | lr 4.00000 | ms/epoch 1885.98585 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 315/500 | lr 4.00000 | ms/epoch 1882.49779 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 316/500 | lr 4.00000 | ms/epoch 1829.30517 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 317/500 | lr 4.00000 | ms/epoch 1713.64570 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 318/500 | lr 4.00000 | ms/epoch 1907.64308 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 319/500 | lr 4.00000 | ms/epoch 1769.69957 | train_loss  0.22 | eval_loss  0.00\n",
      "| epoch 320/500 | lr 4.00000 | ms/epoch 1787.66251 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 321/500 | lr 4.00000 | ms/epoch 1759.90415 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 322/500 | lr 4.00000 | ms/epoch 1759.81665 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 323/500 | lr 4.00000 | ms/epoch 1832.41796 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 324/500 | lr 4.00000 | ms/epoch 1830.63364 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 325/500 | lr 4.00000 | ms/epoch 1720.10207 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 326/500 | lr 4.00000 | ms/epoch 1890.79571 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 327/500 | lr 4.00000 | ms/epoch 1795.28522 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 328/500 | lr 4.00000 | ms/epoch 1711.83252 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 329/500 | lr 4.00000 | ms/epoch 1891.00933 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 330/500 | lr 4.00000 | ms/epoch 1758.71038 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 331/500 | lr 4.00000 | ms/epoch 1725.38662 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 332/500 | lr 4.00000 | ms/epoch 1716.63427 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 333/500 | lr 4.00000 | ms/epoch 1616.82296 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 334/500 | lr 4.00000 | ms/epoch 1567.28721 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 335/500 | lr 4.00000 | ms/epoch 1502.27571 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 336/500 | lr 4.00000 | ms/epoch 1288.81001 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 337/500 | lr 4.00000 | ms/epoch 1175.16756 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 338/500 | lr 4.00000 | ms/epoch 1140.98120 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 339/500 | lr 4.00000 | ms/epoch 1083.80318 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 340/500 | lr 4.00000 | ms/epoch 1090.15298 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 341/500 | lr 4.00000 | ms/epoch 1066.97536 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 342/500 | lr 4.00000 | ms/epoch 1072.86263 | train_loss  0.20 | eval_loss  0.00\n",
      "| epoch 343/500 | lr 4.00000 | ms/epoch 1075.51265 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 344/500 | lr 4.00000 | ms/epoch 1107.04827 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 345/500 | lr 4.00000 | ms/epoch 1067.26122 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 346/500 | lr 4.00000 | ms/epoch 1078.46689 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 347/500 | lr 4.00000 | ms/epoch 1096.00973 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 348/500 | lr 4.00000 | ms/epoch 1068.89629 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 349/500 | lr 4.00000 | ms/epoch 1066.45513 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 350/500 | lr 4.00000 | ms/epoch 1081.29668 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 351/500 | lr 4.00000 | ms/epoch 1052.88839 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 352/500 | lr 4.00000 | ms/epoch 1095.79468 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 353/500 | lr 4.00000 | ms/epoch 1069.83376 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 354/500 | lr 4.00000 | ms/epoch 1117.22708 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 355/500 | lr 4.00000 | ms/epoch 1163.71274 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 356/500 | lr 4.00000 | ms/epoch 1137.78615 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 357/500 | lr 4.00000 | ms/epoch 1167.97519 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 358/500 | lr 4.00000 | ms/epoch 1205.04308 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 359/500 | lr 4.00000 | ms/epoch 1258.91781 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 360/500 | lr 4.00000 | ms/epoch 1280.20525 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 361/500 | lr 4.00000 | ms/epoch 1389.97197 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 362/500 | lr 4.00000 | ms/epoch 1425.44532 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 363/500 | lr 4.00000 | ms/epoch 1510.17690 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 364/500 | lr 4.00000 | ms/epoch 1807.28626 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 365/500 | lr 4.00000 | ms/epoch 1792.03939 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 366/500 | lr 4.00000 | ms/epoch 1778.08166 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 367/500 | lr 4.00000 | ms/epoch 1793.49232 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 368/500 | lr 4.00000 | ms/epoch 1784.23810 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 369/500 | lr 4.00000 | ms/epoch 1760.76293 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 370/500 | lr 4.00000 | ms/epoch 1720.44683 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 371/500 | lr 4.00000 | ms/epoch 1807.17540 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 372/500 | lr 4.00000 | ms/epoch 1721.87424 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 373/500 | lr 4.00000 | ms/epoch 1792.65308 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 374/500 | lr 4.00000 | ms/epoch 1697.77298 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 375/500 | lr 4.00000 | ms/epoch 1688.04336 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 376/500 | lr 4.00000 | ms/epoch 1792.72509 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 377/500 | lr 4.00000 | ms/epoch 1753.36957 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 378/500 | lr 4.00000 | ms/epoch 1708.71615 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 379/500 | lr 4.00000 | ms/epoch 1669.94238 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 380/500 | lr 4.00000 | ms/epoch 1700.14310 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 381/500 | lr 4.00000 | ms/epoch 1773.36121 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 382/500 | lr 4.00000 | ms/epoch 1728.37448 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 383/500 | lr 4.00000 | ms/epoch 1679.91352 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 384/500 | lr 4.00000 | ms/epoch 1707.84712 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 385/500 | lr 4.00000 | ms/epoch 1738.82890 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 386/500 | lr 4.00000 | ms/epoch 1779.67954 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 387/500 | lr 4.00000 | ms/epoch 1720.28065 | train_loss  0.21 | eval_loss  0.00\n",
      "| epoch 388/500 | lr 4.00000 | ms/epoch 1671.66352 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 389/500 | lr 4.00000 | ms/epoch 1527.97413 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 390/500 | lr 4.00000 | ms/epoch 1346.81988 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 391/500 | lr 4.00000 | ms/epoch 1263.82422 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 392/500 | lr 4.00000 | ms/epoch 1254.75311 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 393/500 | lr 4.00000 | ms/epoch 1154.32787 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 394/500 | lr 4.00000 | ms/epoch 1079.76389 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 395/500 | lr 4.00000 | ms/epoch 1075.35505 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 396/500 | lr 4.00000 | ms/epoch 1075.80447 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 397/500 | lr 4.00000 | ms/epoch 1060.17137 | train_loss  0.19 | eval_loss  0.00\n",
      "| epoch 398/500 | lr 4.00000 | ms/epoch 1090.36541 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 399/500 | lr 4.00000 | ms/epoch 1070.25743 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 400/500 | lr 4.00000 | ms/epoch 1096.76981 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 401/500 | lr 4.00000 | ms/epoch 1077.37184 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 402/500 | lr 4.00000 | ms/epoch 1072.39723 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 403/500 | lr 4.00000 | ms/epoch 1032.53794 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 404/500 | lr 4.00000 | ms/epoch 1087.51202 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 405/500 | lr 4.00000 | ms/epoch 1062.59894 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 406/500 | lr 4.00000 | ms/epoch 1109.68876 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 407/500 | lr 4.00000 | ms/epoch 1092.73148 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 408/500 | lr 4.00000 | ms/epoch 1125.61917 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 409/500 | lr 4.00000 | ms/epoch 1163.93924 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 410/500 | lr 4.00000 | ms/epoch 1139.85991 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 411/500 | lr 4.00000 | ms/epoch 1214.94174 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 412/500 | lr 4.00000 | ms/epoch 1215.02614 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 413/500 | lr 4.00000 | ms/epoch 1307.02519 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 414/500 | lr 4.00000 | ms/epoch 1262.69841 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 415/500 | lr 4.00000 | ms/epoch 1381.73294 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 416/500 | lr 4.00000 | ms/epoch 1403.28765 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 417/500 | lr 4.00000 | ms/epoch 1540.34662 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 418/500 | lr 4.00000 | ms/epoch 1646.49105 | train_loss  0.18 | eval_loss  0.00\n",
      "| epoch 419/500 | lr 4.00000 | ms/epoch 1844.11240 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 420/500 | lr 4.00000 | ms/epoch 1967.96298 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 421/500 | lr 4.00000 | ms/epoch 1727.60916 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 422/500 | lr 4.00000 | ms/epoch 1782.43423 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 423/500 | lr 4.00000 | ms/epoch 1837.48627 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 424/500 | lr 4.00000 | ms/epoch 1720.40129 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 425/500 | lr 4.00000 | ms/epoch 1780.02381 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 426/500 | lr 4.00000 | ms/epoch 1829.57602 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 427/500 | lr 4.00000 | ms/epoch 1745.08286 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 428/500 | lr 4.00000 | ms/epoch 1983.12259 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 429/500 | lr 4.00000 | ms/epoch 1948.93122 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 430/500 | lr 4.00000 | ms/epoch 1929.65007 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 431/500 | lr 4.00000 | ms/epoch 1779.96135 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 432/500 | lr 4.00000 | ms/epoch 1779.41275 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 433/500 | lr 4.00000 | ms/epoch 1782.67598 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 434/500 | lr 4.00000 | ms/epoch 1844.93589 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 435/500 | lr 4.00000 | ms/epoch 1796.41008 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 436/500 | lr 4.00000 | ms/epoch 1720.35384 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 437/500 | lr 4.00000 | ms/epoch 1720.28017 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 438/500 | lr 4.00000 | ms/epoch 1783.28419 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 439/500 | lr 4.00000 | ms/epoch 1717.76295 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 440/500 | lr 4.00000 | ms/epoch 1774.77264 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 441/500 | lr 4.00000 | ms/epoch 1905.72309 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 442/500 | lr 4.00000 | ms/epoch 1667.32645 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 443/500 | lr 4.00000 | ms/epoch 1408.78320 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 444/500 | lr 4.00000 | ms/epoch 1264.75739 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 445/500 | lr 4.00000 | ms/epoch 1185.17113 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 446/500 | lr 4.00000 | ms/epoch 1158.79703 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 447/500 | lr 4.00000 | ms/epoch 1120.03875 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 448/500 | lr 4.00000 | ms/epoch 1109.88688 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 449/500 | lr 4.00000 | ms/epoch 1054.39734 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 450/500 | lr 4.00000 | ms/epoch 1069.71836 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 451/500 | lr 4.00000 | ms/epoch 1068.30502 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 452/500 | lr 4.00000 | ms/epoch 1087.99291 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 453/500 | lr 4.00000 | ms/epoch 1077.89969 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 454/500 | lr 4.00000 | ms/epoch 1032.53198 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 455/500 | lr 4.00000 | ms/epoch 1067.37494 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 456/500 | lr 4.00000 | ms/epoch 1058.19440 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 457/500 | lr 4.00000 | ms/epoch 1056.79703 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 458/500 | lr 4.00000 | ms/epoch 1064.97526 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 459/500 | lr 4.00000 | ms/epoch 1077.37422 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 460/500 | lr 4.00000 | ms/epoch 1092.45348 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 461/500 | lr 4.00000 | ms/epoch 1120.13364 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 462/500 | lr 4.00000 | ms/epoch 1109.93338 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 463/500 | lr 4.00000 | ms/epoch 1219.81621 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 464/500 | lr 4.00000 | ms/epoch 1159.88398 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 465/500 | lr 4.00000 | ms/epoch 1217.58485 | train_loss  0.16 | eval_loss  0.00\n",
      "| epoch 466/500 | lr 4.00000 | ms/epoch 1213.05275 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 467/500 | lr 4.00000 | ms/epoch 1423.92755 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 468/500 | lr 4.00000 | ms/epoch 1328.47476 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 469/500 | lr 4.00000 | ms/epoch 1388.46707 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 470/500 | lr 4.00000 | ms/epoch 1461.32541 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 471/500 | lr 4.00000 | ms/epoch 1566.59651 | train_loss  0.17 | eval_loss  0.00\n",
      "| epoch 472/500 | lr 4.00000 | ms/epoch 1834.94043 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 473/500 | lr 4.00000 | ms/epoch 1829.58412 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 474/500 | lr 4.00000 | ms/epoch 1839.61225 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 475/500 | lr 4.00000 | ms/epoch 1759.89771 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 476/500 | lr 4.00000 | ms/epoch 1786.25393 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 477/500 | lr 4.00000 | ms/epoch 1736.99594 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 478/500 | lr 4.00000 | ms/epoch 1765.12241 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 479/500 | lr 4.00000 | ms/epoch 1748.00444 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 480/500 | lr 4.00000 | ms/epoch 1760.34784 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 481/500 | lr 4.00000 | ms/epoch 1813.53092 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 482/500 | lr 4.00000 | ms/epoch 1806.31113 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 483/500 | lr 4.00000 | ms/epoch 1744.52066 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 484/500 | lr 4.00000 | ms/epoch 1782.63283 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 485/500 | lr 4.00000 | ms/epoch 1747.34879 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 486/500 | lr 4.00000 | ms/epoch 1763.20243 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 487/500 | lr 4.00000 | ms/epoch 1725.77405 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 488/500 | lr 4.00000 | ms/epoch 1734.83086 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 489/500 | lr 4.00000 | ms/epoch 1700.82188 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 490/500 | lr 4.00000 | ms/epoch 1767.31539 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 491/500 | lr 4.00000 | ms/epoch 1732.19347 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 492/500 | lr 4.00000 | ms/epoch 1921.41986 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 493/500 | lr 4.00000 | ms/epoch 1768.57305 | train_loss  0.15 | eval_loss  0.00\n",
      "| epoch 494/500 | lr 4.00000 | ms/epoch 1624.09115 | train_loss  0.12 | eval_loss  0.00\n",
      "| epoch 495/500 | lr 4.00000 | ms/epoch 1699.75400 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 496/500 | lr 4.00000 | ms/epoch 1726.69935 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 497/500 | lr 4.00000 | ms/epoch 1602.67758 | train_loss  0.14 | eval_loss  0.00\n",
      "| epoch 498/500 | lr 4.00000 | ms/epoch 1419.38210 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 499/500 | lr 4.00000 | ms/epoch 1232.03731 | train_loss  0.13 | eval_loss  0.00\n",
      "| epoch 500/500 | lr 4.00000 | ms/epoch 1208.20284 | train_loss  0.14 | eval_loss  0.00\n",
      "\n",
      "\n",
      " TRAINING FINISHED:\n",
      "\n",
      "\tBest Loss:0.12076631635427475\tBest Model saved at epoch495\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1e8\n",
    "best_model_epoch = 0\n",
    "eval_losses = []\n",
    "train_losses = []\n",
    "lr = LEARNING_RATE\n",
    "\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = epoch_step(train_dataloader, 'train')\n",
    "\n",
    "\n",
    "        if SPLIT_DATASET:\n",
    "            eval_loss = epoch_step(eval_dataloader, 'eval')\n",
    "\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if eval_loss < best_loss:\n",
    "                torch.save(model, MODEL_PATH)\n",
    "                best_loss = eval_loss\n",
    "                best_model_epoch = epoch + 1\n",
    "\n",
    "            # Anneal the learning rate if the validation loss plateaus\n",
    "            if epoch > 5 and eval_loss >= max(eval_losses[-5:]):\n",
    "                lr = lr / 2.\n",
    "                if lr < 0.1:\n",
    "                    lr = 2\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "\n",
    "\n",
    "        else:\n",
    "            eval_loss = 0\n",
    "\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if train_loss < best_loss:\n",
    "                torch.save(model, MODEL_PATH)\n",
    "                best_loss = train_loss\n",
    "                best_model_epoch = epoch + 1\n",
    "\n",
    "\n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "        # print the loss and the progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d}/{:3d} | lr {:02.5f} | ms/epoch {:5.5f} | train_loss {:5.2f} | eval_loss {:5.2f}' \\\n",
    "                .format(epoch, EPOCHS, lr, elapsed * 1000, train_loss, eval_loss))\n",
    "        \n",
    "    \n",
    "    if TRAIN_MODEL:\n",
    "        print('\\n\\n TRAINING FINISHED:\\n\\n\\tBest Loss: {:5.2f}\\tBest Model saved at epoch: {:3d}' \\\n",
    "              .format(best_loss, best_model_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['43S', '43S', '48', '43', '43', '43', '43', 'O', 'O', '43S', 'O', 'O', '47S', '47', '47', '47', '47', '47', '47', 'O', '47S', '47', '47', '47', '50S', '50', '50', '50', '50', '50', 'O', '50S', '50', '50', '50', 'O', '53S', '53', '53', 'O', '48S', '50', 'O', 'O', '49S', '49', '49', 'O', '50S', '50', '50', '50', '50', '50', 'O', '50S', '50', '50', '50', 'O', '54S', '54', '54', '54', '54', 'O', 'O', '54S', '54', '54', 'O', '57S', '57', '57', '57', '60S', '60', '60', 'O', 'O', '59S', 'O', 'O', '57S', '57', '57', '54S', '54', '54', '54', 'O', '50S', '50', 'O', 'O', '48S', '48S', '48', '48', '48', '48', '48', 'O', 'O', 'O', '48S', '48', 'O', '52S', '52', '52', '52', '52', '52', 'O', '52S', '52', '52', '52', 'O', '55S', '55', '55', 'O', '58S', '58', 'O', 'O', '57S', 'O', 'O', 'O', '55S', 'O', 'O', '52S', '52', '52', 'O', '50S', '50', '50', 'O', '43S', '43S', '43', '43', '43', '43', '43', 'O', '43S', '43', '43', 'O', 'O', '47S', '47', '47', '47', '47', '47', 'O', '47S', '47', '47', 'O', 'O', '50S', '50', '50', '50', '50', 'O', 'O', '50S', '50', '53', 'O', 'O', '53S', '53', 'O', '52S', '52', '52', 'O', '48S', '48', '48', '48', 'O']\n",
      "[[43, 1], [43, 1], [48, 1], [43, 4], [0, 2], [43, 1], [0, 2], [47, 7], [0, 1], [47, 4], [50, 6], [0, 1], [50, 4], [0, 1], [53, 3], [0, 1], [48, 1], [50, 1], [0, 2], [49, 3], [0, 1], [50, 6], [0, 1], [50, 4], [0, 1], [54, 5], [0, 2], [54, 3], [0, 1], [57, 4], [60, 3], [0, 2], [59, 1], [0, 2], [57, 3], [54, 4], [0, 1], [50, 2], [0, 2], [48, 1], [48, 6], [0, 3], [48, 2], [0, 1], [52, 6], [0, 1], [52, 4], [0, 1], [55, 3], [0, 1], [58, 2], [0, 2], [57, 1], [0, 3], [55, 1], [0, 2], [52, 3], [0, 1], [50, 3], [0, 1], [43, 1], [43, 6], [0, 1], [43, 3], [0, 2], [47, 6], [0, 1], [47, 3], [0, 2], [50, 5], [0, 2], [50, 2], [53, 1], [0, 2], [53, 2], [0, 1], [52, 3], [0, 1], [48, 4], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(MODEL_PATH, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# get the first sequence from the batch keeping the batch dimension\n",
    "samples, targets = next(iter(train_dataloader))\n",
    "sample = samples[-1].unsqueeze(0)\n",
    "\n",
    "# Mask the last bar of the input data.\n",
    "sample = torch.cat((sample[:, :BAR_LENGTH*3], torch.ones([1, BAR_LENGTH], dtype=torch.long)), dim = 1)\n",
    "\n",
    "# Make the prediction.\n",
    "prediction = model(sample)\n",
    "prediction = prediction.contiguous().view(-1, OUTPUT_VOCAB_SIZE)\n",
    "\n",
    "# Get the predicted tokens.\n",
    "predicted_tokens = torch.argmax(prediction, 1)\n",
    "\n",
    "# Get the predicted sequence.\n",
    "predicted_sequence = predicted_tokens.cpu().numpy().tolist()\n",
    "\n",
    "# Convert the predicted sequence to MIDI.\n",
    "out_file_path = os.path.join(RESULTS_PATH, 'predicted.mid')\n",
    "midi, pitch_ticks_list =  OUTPUT_TOK.tokens_to_midi(predicted_sequence, out_file_path) # midi is a pretty midi object\n",
    "\n",
    "# check \n",
    "predicted_sequence_string = []\n",
    "for id in predicted_sequence:\n",
    "    predicted_sequence_string.append(OUTPUT_TOK.VOCAB.idx2word[id])\n",
    "print(predicted_sequence_string)\n",
    "print(pitch_ticks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
